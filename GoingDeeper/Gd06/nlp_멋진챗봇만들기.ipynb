{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4469df8",
   "metadata": {},
   "source": [
    "# [í”„ë¡œì íŠ¸] ë©‹ì§„ì±—ë´‡ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2984b",
   "metadata": {},
   "source": [
    "### Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "### Step 2. ë°ì´í„° ì •ì œ\n",
    "### Step 3. ë°ì´í„° í† í°í™”\n",
    "### Step 4. Augmentation\n",
    "### Step 5. ë°ì´í„° ë²¡í„°í™”\n",
    "### Step 6. í›ˆë ¨í•˜ê¸°\n",
    "### Step 7. ì„±ëŠ¥ ì¸¡ì •í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da239fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4f1a6",
   "metadata": {},
   "source": [
    "## Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "ChatbotData.csv ì‚¬ìš©, ì§ˆë¬¸(questions), ë‹µë³€(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a30ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q            A  label\n",
      "0           12ì‹œ ë•¡!   í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.      0\n",
      "1      1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´    ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.      0\n",
      "2     3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤  ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
      "3  3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤  ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
      "4          PPL ì‹¬í•˜ë„¤   ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .      0\n",
      "ì´ ì§ˆë¬¸ ìˆ˜: 11823\n",
      "ì´ ë‹µë³€ ìˆ˜: 11823\n",
      "ì²« ë²ˆì§¸ ì§ˆë¬¸: 12ì‹œ ë•¡!\n",
      "ì²« ë²ˆì§¸ ë‹µë³€: í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chatbot.csv íŒŒì¼ ê²½ë¡œ ì„¤ì • (ë¡œì»¬ ê²½ë¡œë¥¼ ì§€ì •í•´ ì£¼ì„¸ìš”)\n",
    "file_path = \"Chatbot.csv\"\n",
    "\n",
    "# CSV íŒŒì¼ ì½ê¸°\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# ë°ì´í„° í™•ì¸ (ì²« 5ì¤„)\n",
    "print(data.head())\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ê°ê° ë¶„ë¦¬í•´ì„œ ì €ì¥\n",
    "questions = data['Q'].tolist()  # ì§ˆë¬¸ ì—´(Q)\n",
    "answers = data['A'].tolist()    # ë‹µë³€ ì—´(A)\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(f\"ì´ ì§ˆë¬¸ ìˆ˜: {len(questions)}\")\n",
    "print(f\"ì´ ë‹µë³€ ìˆ˜: {len(answers)}\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶œë ¥\n",
    "print(f\"ì²« ë²ˆì§¸ ì§ˆë¬¸: {questions[0]}\")\n",
    "print(f\"ì²« ë²ˆì§¸ ë‹µë³€: {answers[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3832e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions ìƒ˜í”Œ: ['12ì‹œ ë•¡!', '1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´', '3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤', '3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤', 'PPL ì‹¬í•˜ë„¤']\n",
      "answers ìƒ˜í”Œ: ['í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.', 'ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.', 'ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .', 'ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .', 'ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .']\n"
     ]
    }
   ],
   "source": [
    "questions = data['Q'].tolist()  # ì§ˆë¬¸ ì—´ ì´ë¦„ í™•ì¸ í›„ ì‚¬ìš©\n",
    "answers = data['A'].tolist()    # ë‹µë³€ ì—´ ì´ë¦„ í™•ì¸ í›„ ì‚¬ìš©\n",
    "\n",
    "print(\"questions ìƒ˜í”Œ:\", questions[:5])\n",
    "print(\"answers ìƒ˜í”Œ:\", answers[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d1996",
   "metadata": {},
   "source": [
    "## Step 2. ë°ì´í„° ì •ì œ\n",
    "- ì˜ë¬¸ìì˜ ê²½ìš°, ëª¨ë‘ ì†Œë¬¸ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "- ì˜ë¬¸ìì™€ í•œê¸€, ìˆ«ì, ê·¸ë¦¬ê³  ì£¼ìš” íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œì™¸í•˜ê³¤ ì •ê·œì‹ì„ í™œìš©í•˜ì—¬ ëª¨ë‘ ì œê±°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571c0cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ í›„: hello! ë‚´ ì´ë¦„ì€ ê¹€ë¯¼í˜ì…ë‹ˆë‹¤, íŠ¹ìˆ˜ë¬¸ì !\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 1. ì˜ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 2. ì˜ë¬¸ì, í•œê¸€, ìˆ«ì, ì£¼ìš” íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ì œê±°\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9ê°€-í£?.!,]+\", \" \", sentence)\n",
    "    \n",
    "    # 3. ë¬¸ìì—´ ì–‘ ëì˜ ê³µë°± ì œê±°\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# ì˜ˆì‹œ ë°ì´í„°ë¡œ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
    "test_sentence = \"Hello! ë‚´ ì´ë¦„ì€ ê¹€ë¯¼í˜ì…ë‹ˆë‹¤, íŠ¹ìˆ˜ë¬¸ì !@#$%\"\n",
    "preprocessed_sentence = preprocess_sentence(test_sentence)\n",
    "print(\"ì „ì²˜ë¦¬ í›„:\", preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af6951",
   "metadata": {},
   "source": [
    "## Step 3. ë°ì´í„° í† í°í™”\n",
    "> í† í°í™”(mecab), build_corpus() í•¨ìˆ˜ êµ¬í˜„\n",
    "- ì†ŒìŠ¤ ë¬¸ì¥ ë°ì´í„°ì™€ íƒ€ê²Ÿ ë¬¸ì¥ ë°ì´í„°ë¥¼ ì…ë ¥\n",
    "- ë°ì´í„°ë¥¼ ì•ì„œ ì •ì˜í•œ preprocess_sentence() í•¨ìˆ˜ë¡œ ì •ì œí•˜ê³ , í† í°í™”\n",
    "- í† í°í™”ëŠ” ì „ë‹¬ë°›ì€ í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜ë¥¼ ì‚¬ìš©, mecab.morphs í•¨ìˆ˜ë¥¼ ì „ë‹¬\n",
    "- í† í°ì˜ ê°œìˆ˜ê°€ ì¼ì • ê¸¸ì´ ì´ìƒì¸ ë¬¸ì¥ì€ ë°ì´í„°ì—ì„œ ì œì™¸\n",
    "- ì¤‘ë³µë˜ëŠ” ë¬¸ì¥ì€ ë°ì´í„°ì—ì„œ ì œì™¸\n",
    "    - ì†ŒìŠ¤ : íƒ€ê²Ÿ ìŒì„ ë¹„êµí•˜ì§€ ì•Šê³  ì†ŒìŠ¤ëŠ” ì†ŒìŠ¤ëŒ€ë¡œ íƒ€ê²Ÿì€ íƒ€ê²ŸëŒ€ë¡œ ê²€ì‚¬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0564abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "def build_corpus(sentences, tokenizer, max_len=40):\n",
    "    corpus = []\n",
    "\n",
    "    seen_sentences = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Step 1: ë¬¸ì¥ ì •ì œ\n",
    "        sentence = preprocess_sentence(sentence)\n",
    "\n",
    "        # Step 2: í† í°í™”\n",
    "        tokens = tokenizer(sentence)\n",
    "\n",
    "        # Step 3: í† í°ì˜ ê°œìˆ˜ê°€ max_lenë³´ë‹¤ ê¸¸ë©´ ì œì™¸\n",
    "        if len(tokens) > max_len:\n",
    "            continue\n",
    "\n",
    "        # Step 4: ì¤‘ë³µ ë¬¸ì¥ ì œê±°\n",
    "        tokenized_sentence = ' '.join(tokens)\n",
    "        if tokenized_sentence in seen_sentences:\n",
    "            continue\n",
    "        seen_sentences.add(tokenized_sentence)\n",
    "\n",
    "        # ìµœì¢… í† í°í™”ëœ ë¬¸ì¥ì„ corpusì— ì¶”ê°€\n",
    "        corpus.append(tokens)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "# que_corpus ì™€ ans_corpus ìƒì„±\n",
    "que_corpus = build_corpus(questions, mecab.morphs, max_len=40)\n",
    "ans_corpus = build_corpus(answers, mecab.morphs, max_len=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c77ba",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation\n",
    "í•œêµ­ì–´ë¡œ ì‚¬ì „ í›ˆë ¨ëœ Embedding model ë‹¤ìš´ë¡œë“œ - ko.bin íŒŒì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b089ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# gensim ë‹¤ìš´ê·¸ë ˆì´ë“œ\n",
    "!pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c6ca4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "[('í•™êµì˜', 0.7560996413230896), ('ê°•ìŠµì†Œ', 0.7425637245178223), ('ì¤‘ê³ ë“±í•™êµ', 0.7386142015457153), ('ì „ë¬¸í•™êµ', 0.7356827855110168), ('ì‚¬ë¦½í•™êµ', 0.7347193956375122), ('ì†Œí•™êµ', 0.7305554747581482), ('ì—¬í•™êµ', 0.7091007232666016), ('ì‚¬ë²”í•™êµ', 0.6901223659515381), ('ëŒ€í•™', 0.6897724866867065), ('í•™ì›', 0.6869212985038757)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_249/691456926.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  print(wv.most_similar(\"í•™êµ\"))  # \"í•™êµ\"ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ ì¶œë ¥\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# ko.bin íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "model_path = 'ko.bin'\n",
    "\n",
    "# Word2Vec ëª¨ë¸ ë¡œë“œ\n",
    "wv = Word2Vec.load(model_path)\n",
    "print(\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ\n",
    "print(wv.most_similar(\"í•™êµ\"))  # \"í•™êµ\"ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5cebc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_249/2324208733.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  wv.most_similar(\"ë°”ë‚˜ë‚˜\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ì½”ì½”ë„›', 0.8097119927406311),\n",
       " ('ì‹œê¸ˆì¹˜', 0.7701147794723511),\n",
       " ('ë ˆëª¬', 0.76884925365448),\n",
       " ('ë•…ì½©', 0.7684735059738159),\n",
       " ('íŒŒì¸ì• í”Œ', 0.7639915347099304),\n",
       " ('ë…¹ì°¨', 0.7631460428237915),\n",
       " ('ë”¸ê¸°', 0.7617197036743164),\n",
       " ('ë°”ë‹ë¼', 0.7497864961624146),\n",
       " ('íŒŒìŠ¬ë¦¬', 0.7447543144226074),\n",
       " ('ì½”ì½”ì•„', 0.7408244609832764)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"ë°”ë‚˜ë‚˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e418a101",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11823 [00:00<?, ?it/s]/tmp/ipykernel_249/3065843551.py:10: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in model and random.random() < prob:\n",
      "/tmp/ipykernel_249/3065843551.py:12: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  similar_words = model.most_similar(word, topn=5)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11823/11823 [01:06<00:00, 178.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ì§ˆë¬¸ ë°ì´í„° ê°œìˆ˜: 11823\n",
      "ì¦ê°•ëœ ì§ˆë¬¸ ë°ì´í„° ê°œìˆ˜: 35469\n",
      "ì „ì²´ ì§ˆë¬¸ ë°ì´í„° ê°œìˆ˜: 47292\n",
      "ì›ë³¸ ë‹µë³€ ë°ì´í„° ê°œìˆ˜: 11823\n",
      "ì¦ê°•ëœ ë‹µë³€ ë°ì´í„° ê°œìˆ˜: 35469\n",
      "ì „ì²´ ë‹µë³€ ë°ì´í„° ê°œìˆ˜: 47292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# lexical_sub í•¨ìˆ˜ ì •ì˜: ë‹¨ì–´ë¥¼ ìœ ì‚¬ë„ê°€ ë†’ì€ ë‹¨ì–´ë¡œ ì¹˜í™˜í•˜ì—¬ ë°ì´í„° ì¦ê°•\n",
    "def lexical_sub(sentence, model, prob=0.7):\n",
    "    words = sentence.split()\n",
    "    new_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model and random.random() < prob:\n",
    "            try:\n",
    "                similar_words = model.most_similar(word, topn=5)\n",
    "                new_word = random.choice(similar_words)[0]\n",
    "                new_words.append(new_word)\n",
    "            except KeyError:\n",
    "                new_words.append(word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# ë°ì´í„° ì¦ê°• ìˆ˜í–‰ í•¨ìˆ˜\n",
    "def augment_corpus(questions, answers, model):\n",
    "    augmented_questions = []\n",
    "    augmented_answers = []\n",
    "\n",
    "    for que, ans in tqdm(zip(questions, answers), total=len(questions)):\n",
    "        # 1. ì§ˆë¬¸ì„ ì¦ê°•í•˜ê³  ë‹µë³€ì€ ì›ë³¸ ìœ ì§€\n",
    "        augmented_questions.append(lexical_sub(que, model))  # ê°œë³„ ì§ˆë¬¸\n",
    "        augmented_answers.append(ans)  # ê°œë³„ ë‹µë³€\n",
    "\n",
    "        # 2. ë‹µë³€ì„ ì¦ê°•í•˜ê³  ì§ˆë¬¸ì€ ì›ë³¸ ìœ ì§€\n",
    "        augmented_questions.append(que)\n",
    "        augmented_answers.append(lexical_sub(ans, model))\n",
    "        \n",
    "        # 3. ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ëª¨ë‘ ì¦ê°•\n",
    "        augmented_questions.append(lexical_sub(que, model))\n",
    "        augmented_answers.append(lexical_sub(ans, model))\n",
    "\n",
    "    # ìµœì¢… ë°ì´í„°ì…‹ ìƒì„±\n",
    "    final_questions = questions + augmented_questions\n",
    "    final_answers = answers + augmented_answers\n",
    "\n",
    "    print(f\"ì›ë³¸ ì§ˆë¬¸ ë°ì´í„° ê°œìˆ˜: {len(questions)}\")\n",
    "    print(f\"ì¦ê°•ëœ ì§ˆë¬¸ ë°ì´í„° ê°œìˆ˜: {len(augmented_questions)}\")\n",
    "    print(f\"ì „ì²´ ì§ˆë¬¸ ë°ì´í„° ê°œìˆ˜: {len(final_questions)}\")\n",
    "\n",
    "    print(f\"ì›ë³¸ ë‹µë³€ ë°ì´í„° ê°œìˆ˜: {len(answers)}\")\n",
    "    print(f\"ì¦ê°•ëœ ë‹µë³€ ë°ì´í„° ê°œìˆ˜: {len(augmented_answers)}\")\n",
    "    print(f\"ì „ì²´ ë‹µë³€ ë°ì´í„° ê°œìˆ˜: {len(final_answers)}\")\n",
    "\n",
    "    return final_questions, final_answers\n",
    "\n",
    "# ì‹¤ì œ ë°ì´í„°ì— ì¦ê°• ì ìš©\n",
    "final_questions, final_answers = augment_corpus(questions, answers, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca3ff8",
   "metadata": {},
   "source": [
    "### ì˜¤ë¥˜ ğŸš¨ğŸš¨ğŸš¨\n",
    "ì•ì—ì„œ questionsì™€ answersë¥¼ ì˜ ë¶ˆëŸ¬ì™”ë‹¤ê°€ ì¤‘ê°„ì— ë‚´ê°€ \n",
    "\n",
    "`questions=[]`\n",
    "`answers =[]`\n",
    "\n",
    "ë¥¼ ì„ ì–¸í•´ì„œ ì •ì œí–ˆë˜ ë°ì´í„°ë¥¼ ë‹¤ ë‚ ë ¸ê¸¸ë˜ ë°ì´í„° ìˆ˜ê°€ ë‹¤ 0ìœ¼ë¡œ ì§€ì •ë˜ì–´ì¡Œë‹¤.\n",
    "\n",
    "ê·¸ë˜ì„œ ë°ì´í„° ì¦ê°•ì´ ì‹¤í˜„ë˜ì§€ ì•Šì•˜ë˜ ê±¸ í™•ì¸í–ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509d1cf",
   "metadata": {},
   "source": [
    "## Step 5. ë°ì´í„° ë²¡í„°í™”\n",
    "- `<start>` í† í°ê³¼ `<end>` í† í°\n",
    "- `que_corpus` ì™€ ê²°í•©í•˜ì—¬ ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ë‹¨ì–´ ì‚¬ì „ì„ êµ¬ì¶•\n",
    "- ë²¡í„°í™”í•˜ì—¬ `enc_train` ê³¼ `dec_train` ì–»ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7d85239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', '12', 'ì‹œ', 'ë•¡', '!', '<end>']\n",
      "ë‹¨ì–´ ì‚¬ì „ í¬ê¸°: 25118\n",
      "ì¸ì½”ë” ë°ì´í„° í¬ê¸°: (47292, 50)\n",
      "ë””ì½”ë” ë°ì´í„° í¬ê¸°: (47292, 50)\n"
     ]
    }
   ],
   "source": [
    "# ë‹µë³€ ë°ì´í„°ì— <start>ì™€ <end> í† í° ì¶”ê°€\n",
    "ans_corpus_with_tokens = [[\"<start>\"] + ans.split() + [\"<end>\"] for ans in final_answers]\n",
    "\n",
    "# ê²°ê³¼ ì˜ˆì‹œ í™•ì¸\n",
    "sample_data = [\"12\", \"ì‹œ\", \"ë•¡\", \"!\"]\n",
    "print([\"<start>\"] + sample_data + [\"<end>\"])\n",
    "\n",
    "# 2. que_corpusì™€ ans_corpusë¥¼ ê²°í•©í•˜ì—¬ ë‹¨ì–´ ì‚¬ì „ ìƒì„±\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¨ì–´ ì‚¬ì „ì— í¬í•¨ì‹œí‚¤ê¸° ìœ„í•´ que_corpusì™€ ans_corpus_with_tokens ê²°í•©\n",
    "total_corpus = final_questions + ans_corpus_with_tokens\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ë° ë‹¨ì–´ ì‚¬ì „ ìƒì„±\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(total_corpus)\n",
    "\n",
    "# ë‹¨ì–´ ì‚¬ì „ í¬ê¸° í™•ì¸\n",
    "vocab_size = len(tokenizer.word_index) + 1  # íŒ¨ë”© í† í° í¬í•¨\n",
    "\n",
    "print(\"ë‹¨ì–´ ì‚¬ì „ í¬ê¸°:\", vocab_size)\n",
    "\n",
    "# 3. ê° ë¬¸ì¥ì„ ì‹œí€€ìŠ¤ ë²ˆí˜¸ë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„°í™”\n",
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ ê°ê°ì— ëŒ€í•´ ë²¡í„°í™” ìˆ˜í–‰\n",
    "enc_train = tokenizer.texts_to_sequences(final_questions)\n",
    "dec_train = tokenizer.texts_to_sequences(ans_corpus_with_tokens)\n",
    "\n",
    "# 4. íŒ¨ë”© ì²˜ë¦¬\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# ìµœëŒ€ ê¸¸ì´ ì„¤ì •\n",
    "max_len = 50  # í•„ìš”ì— ë”°ë¼ ìˆ˜ì • ê°€ëŠ¥\n",
    "\n",
    "enc_train = pad_sequences(enc_train, maxlen=max_len, padding='post')\n",
    "dec_train = pad_sequences(dec_train, maxlen=max_len, padding='post')\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(f\"ì¸ì½”ë” ë°ì´í„° í¬ê¸°: {enc_train.shape}\")\n",
    "print(f\"ë””ì½”ë” ë°ì´í„° í¬ê¸°: {dec_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6b83d",
   "metadata": {},
   "source": [
    "## Step 6. í›ˆë ¨í•˜ê¸°\n",
    "- Transformer ëª¨ë¸ ì„¤ê³„\n",
    "    - Positional Encoding\n",
    "    - Mask generate\n",
    "    - Multi-Head Attention\n",
    "    - Position-wise Feed-Forward Network\n",
    "    - Encoder Layer\n",
    "    - Decoder Layer\n",
    "    - Encoder\n",
    "    - Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b8263",
   "metadata": {},
   "source": [
    "Positonal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74cd9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d298d0",
   "metadata": {},
   "source": [
    "mask ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5322fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6ba6e",
   "metadata": {},
   "source": [
    "Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ab30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567dec5",
   "metadata": {},
   "source": [
    "Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b406b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5ca64",
   "metadata": {},
   "source": [
    "Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b8d708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57434943",
   "metadata": {},
   "source": [
    "Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c0e4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V ìˆœì„œì— ì£¼ì˜í•˜ì„¸ìš”!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f02e7",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d5e8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206228f8",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "496d19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6306fc",
   "metadata": {},
   "source": [
    "Transformer ì¡°ë¦½í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "299f35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab49af",
   "metadata": {},
   "source": [
    "ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d781b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\t\t\n",
    "d_model = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f33c88",
   "metadata": {},
   "source": [
    "Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e14486a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19ead4",
   "metadata": {},
   "source": [
    "Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c80fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81ae7b",
   "metadata": {},
   "source": [
    "Loss Function ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efdeaabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8becf51",
   "metadata": {},
   "source": [
    "Time Step ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72071dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoderì˜ input\n",
    "    gold = tgt[:, 1:]     # Decoderì˜ outputê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ right shiftë¥¼ í†µí•´ ìƒì„±í•œ ìµœì¢… íƒ€ê²Ÿ\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ffd03",
   "metadata": {},
   "source": [
    "ë²ˆì—­ ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fc2c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²ˆì—­ ìƒì„± í•¨ìˆ˜\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer, max_len=50):\n",
    "    sentence = preprocess_sentence(sentence)  # ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš©\n",
    "    tokens = src_tokenizer.texts_to_sequences([sentence])[0]  # í† í°í™”\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=max_len, padding='post')\n",
    "\n",
    "    output = tf.expand_dims([tgt_tokenizer.word_index['<start>']], 0)\n",
    "    ids = []\n",
    "\n",
    "    for i in range(max_len):\n",
    "        predictions, _, _, _ = model(_input, output, None, None, None)\n",
    "        predicted_id = tf.argmax(predictions[0, -1]).numpy()\n",
    "\n",
    "        if predicted_id == tgt_tokenizer.word_index['<end>']:\n",
    "            break\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    return ' '.join([tgt_tokenizer.index_word[id] for id in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f4297",
   "metadata": {},
   "source": [
    "ğŸƒğŸƒğŸƒğŸ’¨ í›ˆë ¨ ì‹œì‘!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c6a5ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [02:15<00:00,  5.44batch/s, Loss=4.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.9479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [02:19<00:00,  5.28batch/s, Loss=2.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.1867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [02:23<00:00,  5.14batch/s, Loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.2078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [02:23<00:00,  5.13batch/s, Loss=0.851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.8514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [02:23<00:00,  5.14batch/s, Loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.7018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [02:23<00:00,  5.14batch/s, Loss=0.606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.6056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [02:23<00:00,  5.14batch/s, Loss=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.5449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [02:23<00:00,  5.15batch/s, Loss=0.493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.4926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [02:23<00:00,  5.15batch/s, Loss=0.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.4604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [02:23<00:00,  5.15batch/s, Loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.4287\n",
      "\n",
      "ì§ˆë¬¸: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "ì˜ˆì¸¡ ë‹µë³€: ì¢‹ì€ ì‚¬ëŒ ë§Œë‚  ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.\n",
      "\n",
      "ì§ˆë¬¸: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "ì˜ˆì¸¡ ë‹µë³€: ì˜ëª» ì£¼ë¬´ì…¨ë‚˜ë´ìš”.\n",
      "\n",
      "ì§ˆë¬¸: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "ì˜ˆì¸¡ ë‹µë³€: ë§í•˜ê¸° í˜ë“¤ì—ˆê² ì–´ìš”.\n",
      "\n",
      "ì§ˆë¬¸: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "ì˜ˆì¸¡ ë‹µë³€: ì¢‹ì€ ê³³ìœ¼ë¡œ ë°ë ¤ë‹¤ ê¹ ê±°ì˜ˆìš”.\n",
      "ğŸ«¡ í›ˆë ¨ ë° ë²ˆì—­ ìƒì„± ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(final_questions)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # ë‹¨ì–´ ì‚¬ì „ í¬ê¸°\n",
    "\n",
    "# 1. Dataset ìƒì„±\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True\n",
    ")\n",
    "learning_rate = LearningRateScheduler(512, warmup_steps=1000)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# 2. í›ˆë ¨ ë£¨í”„\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    dataset_count = tf.data.experimental.cardinality(dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count, desc=f'Epoch {epoch + 1}', unit='batch')\n",
    "\n",
    "    for (batch, (src, tgt)) in enumerate(dataset):\n",
    "        loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += loss\n",
    "        tqdm_bar.set_postfix({'Loss': total_loss.numpy() / (batch + 1)})\n",
    "        tqdm_bar.update(1)\n",
    "    \n",
    "    tqdm_bar.close()\n",
    "    print(f'Epoch {epoch + 1} Loss: {total_loss.numpy() / dataset_count:.4f}')\n",
    "\n",
    "# ì˜ˆë¬¸ ë²ˆì—­ ìƒì„±\n",
    "test_questions = [\n",
    "    \"ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\",\n",
    "    \"ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\",\n",
    "    \"ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\",\n",
    "    \"ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\nì§ˆë¬¸: {question}\")\n",
    "    predicted_answer = evaluate(question, transformer, tokenizer, tokenizer)\n",
    "    print(f\"ì˜ˆì¸¡ ë‹µë³€: {predicted_answer}\")\n",
    "\n",
    "print(\"ğŸ«¡ í›ˆë ¨ ë° ë²ˆì—­ ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa1a78",
   "metadata": {},
   "source": [
    "## Step 7. ì„±ëŠ¥ ì¸¡ì •í•˜ê¸°\n",
    "ì˜¬ë°”ë¥¸ ëŒ€ë‹µì„ í•˜ëŠ”ì§€ ì¤‘ìš”í•œ ê²ƒì´ í‰ê°€ ì§€í‘œì´ë‹¤.\n",
    "\n",
    "ì£¼ì–´ì§„ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” 'BLUE Score'ì„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b6576",
   "metadata": {},
   "source": [
    "### ì˜¤ë¥˜ ğŸš¨ğŸš¨ğŸš¨ â“â“â“â“\n",
    "BLUE ì ìˆ˜ë¥¼ êµ¬í˜„í•˜ë ¤ëŠ”ë°, ìŠ¤ì½”ì–´ ì ìˆ˜ê°€ 0ì ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê±¸ ë³´ë©´ ì•„ì˜ˆ ì ìˆ˜ë¥¼ ë‚´ì§€ ëª»í•˜ëŠ”ë°.\n",
    "\n",
    "ê·¸ëŸ¬í•œ ì´ìœ ë¥¼ ë¶„ì„í•´ë´¤ë‹¤.\n",
    "\n",
    "`ì±—ë´‡`ì˜ ê²½ìš° ì§ˆë¬¸ì— ëŒ€í•œ ë‹¨ í•˜ë‚˜ì˜ ì •ë‹µì´ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— BLEU Scoreë§Œìœ¼ë¡œëŠ” ì™„ë²½í•˜ê²Œ ì±—ë´‡ ì‘ë‹µì˜ ì§ˆì„ í‰ê°€í•  ìˆ˜ ì—†ë‹¤. \n",
    "\n",
    "í•˜ì§€ë§Œ, ì±—ë´‡ ë‹µë³€ì´ ì°¸ì¡° ì‘ë‹µê³¼ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ìš©ë„ë¡œ BLEU Scoreë¥¼ `ì°¸ê³ `í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4dbca89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> ì ê¹ ì‰¬ ì–´ë„ ë¼ìš” . <end>\n",
      "Candidate: ì¢‹ì€ ì‚¬ëŒ ë§Œë‚  ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.\n",
      "BLEU Score: 0.0000\n",
      "Reference: <start> ë§›ë‚œ ê±° ë“œì„¸ìš” . <end>\n",
      "Candidate: ì˜ëª» ì£¼ë¬´ì…¨ë‚˜ë´ìš”.\n",
      "BLEU Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> ë–¨ë¦¬ ê²  ì£  . <end>\n",
      "Candidate: ë§í•˜ê¸° í˜ë“¤ì—ˆê² ì–´ìš”.\n",
      "BLEU Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> ì¢‹ ì•„ í•˜ ë©´ ê·¸ëŸ´ ìˆ˜ ìˆ ì–´ìš” . <end>\n",
      "Candidate: ì¢‹ì€ ê³³ìœ¼ë¡œ ë°ë ¤ë‹¤ ê¹ ê±°ì˜ˆìš”.\n",
      "BLEU Score: 0.0000\n",
      "\n",
      "Num of Samples: 4\n",
      "Average BLEU Score: 0.0000\n",
      "Average BLEU Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "# BLEU Score ê³„ì‚° í•¨ìˆ˜ ì •ì˜\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_score = sentence_bleu([reference], candidate, weights=weights, smoothing_function=smoothie)\n",
    "    return bleu_score\n",
    "\n",
    "# ê°œë³„ ë¬¸ì¥ì— ëŒ€í•œ BLEU Score ê³„ì‚° í•¨ìˆ˜\n",
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "\n",
    "    # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ë‹µë³€ ìƒì„±\n",
    "    predicted_answer = evaluate(src_sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    # ì°¸ì¡° ë¬¸ì¥ê³¼ ì˜ˆì¸¡ ë¬¸ì¥ í† í°í™”\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = predicted_answer.split()\n",
    "    \n",
    "    # BLEU Score ê³„ì‚°\n",
    "    bleu_score = calculate_bleu(reference, candidate)\n",
    "    if verbose:\n",
    "        print(f\"Reference: {' '.join(reference)}\")\n",
    "        print(f\"Candidate: {' '.join(candidate)}\")\n",
    "        print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    \n",
    "    return bleu_score\n",
    "\n",
    "# ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ BLEU Score í‰ê°€ í•¨ìˆ˜\n",
    "def eval_bleu(model, src_sentences, tgt_sentences, src_tokenizer, tgt_tokenizer, verbose=False):\n",
    "\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size), desc=\"Evaluating BLEU\"):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentences[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        total_score += score\n",
    "    \n",
    "    # í‰ê·  BLEU Score ê³„ì‚°\n",
    "    avg_bleu_score = total_score / sample_size\n",
    "    print(f\"\\nNum of Samples: {sample_size}\")\n",
    "    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "    return avg_bleu_score\n",
    "\n",
    "# ì˜ˆë¬¸ ë° ì°¸ì¡° ë‹µë³€ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "test_questions = [\n",
    "    \"ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\",\n",
    "    \"ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\",\n",
    "    \"ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\",\n",
    "    \"ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\"\n",
    "]\n",
    "\n",
    "# ì˜ˆì‹œ ë‹µë³€ (BLEU ê³„ì‚° ì‹œ ì°¸ì¡° ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©)\n",
    "reference_answers = [\n",
    "    \"<start> ì ê¹ ì‰¬ ì–´ë„ ë¼ìš” . <end>\",\n",
    "    \"<start> ë§›ë‚œ ê±° ë“œì„¸ìš” . <end>\",\n",
    "    \"<start> ë–¨ë¦¬ ê²  ì£  . <end>\",\n",
    "    \"<start> ì¢‹ ì•„ í•˜ ë©´ ê·¸ëŸ´ ìˆ˜ ìˆ ì–´ìš” . <end>\"\n",
    "]\n",
    "\n",
    "# BLEU í‰ê°€ ìˆ˜í–‰\n",
    "avg_bleu_score = eval_bleu(transformer, test_questions, reference_answers, tokenizer, tokenizer, verbose=True)\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37cb5b8",
   "metadata": {},
   "source": [
    "BLUE Score êµ¬í˜„ì„ ì˜ëª».. í–ˆë‚˜?\n",
    "\n",
    "ë‹¹ì¥ ì‹œí—˜í•´ë³´ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "208e01b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> ì¢‹ì€ ì‚¬ëŒ ë§Œë‚˜ <end>\n",
      "Candidate: ì¢‹ì€ ì‚¬ëŒ ë§Œë‚  ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.\n",
      "BLEU Score: 0.0972\n",
      "Reference: <start> ì˜ëª» ì£¼ë¬´ì…¨ë‚˜ìš” <end>\n",
      "Candidate: ì˜ëª» ì£¼ë¬´ì…¨ë‚˜ë´ìš”.\n",
      "BLEU Score: 0.0248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> ë§í•˜ê¸° í˜ë“¤ì—ˆì§€ë§Œ ê´œì°®ì•„ìš” <end>\n",
      "Candidate: ë§í•˜ê¸° í˜ë“¤ì—ˆê² ì–´ìš”.\n",
      "BLEU Score: 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> ì¢‹ì€ ê³³ìœ¼ë¡œ ë°ë ¤ê°ˆê±°ì˜ˆìš” <end>\n",
      "Candidate: ì¢‹ì€ ê³³ìœ¼ë¡œ ë°ë ¤ë‹¤ ê¹ ê±°ì˜ˆìš”.\n",
      "BLEU Score: 0.1212\n",
      "\n",
      "Num of Samples: 4\n",
      "Average BLEU Score: 0.0646\n",
      "Average BLEU Score: 0.0646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ì˜ˆë¬¸ ë° ì°¸ì¡° ë‹µë³€ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "test_questions = [\n",
    "    \"ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\",\n",
    "    \"ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\",\n",
    "    \"ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\",\n",
    "    \"ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\"\n",
    "]\n",
    "\n",
    "# ì˜ˆì‹œ ë‹µë³€ (BLEU ê³„ì‚° ì‹œ ì°¸ì¡° ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©)\n",
    "reference_answers = [\n",
    "    \"<start> ì¢‹ì€ ì‚¬ëŒ ë§Œë‚˜ <end>\",\n",
    "    \"<start> ì˜ëª» ì£¼ë¬´ì…¨ë‚˜ìš” <end>\",\n",
    "    \"<start> ë§í•˜ê¸° í˜ë“¤ì—ˆì§€ë§Œ ê´œì°®ì•„ìš” <end>\",\n",
    "    \"<start> ì¢‹ì€ ê³³ìœ¼ë¡œ ë°ë ¤ê°ˆê±°ì˜ˆìš” <end>\"\n",
    "]\n",
    "\n",
    "# BLEU í‰ê°€ ìˆ˜í–‰\n",
    "avg_bleu_score = eval_bleu(transformer, test_questions, reference_answers, tokenizer, tokenizer, verbose=True)\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188a789",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µë³€ì„ ì¸ìœ„ì ìœ¼ë¡œ ë‹µë³€ê³¼ ë¹„ìŠ·í•˜ê²Œ êµ¬í˜„í•´ë´¤ëŠ”ë° Scoreê°€ êµ¬í•´ì§€ëŠ” ê²ƒì€ ì •ìƒì´ì˜€ë‹¤ ã…\n",
    "\n",
    "ê·¸ì € ì„±ëŠ¥ì´ ì•ˆì¢‹ì•˜ì„ ë¿... ê·¸ë˜ë„ ì±—ë´‡ì¸ì§€ë¼ ì±—ë´‡ë‹µê²Œ êµ¬í˜„í•´ë³´ê² ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a95fa",
   "metadata": {},
   "source": [
    "### ë¯¼í˜ì˜ ì±—ë´‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50da8a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆë…•í•˜ì„¸ìš” ë¯¼í˜ì˜ ì±—ë´‡ì…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ'ë¼ê³  ì…ë ¥í•˜ì„¸ìš”.\n",
      "ë‹¹ì‹ : ì˜¤ëŠ˜ ì ì‹¬ì„ ì¶”ì²œí•´ì¤„ë˜?\n",
      "ì±—ë´‡: ì‰½ì§€ ì•Šì€ ê²°ì •ì´ì—ˆì„í…ë° ìŠ¬í”” ê°ˆì¦ ë§ì•˜ì–´ìš”.\n",
      "ë‹¹ì‹ : ë‚˜ ë„ˆë¬´ ë°°ê³ íŒŒ ë­˜ ë¨¹ì„ê¹Œ?\n",
      "ì±—ë´‡: ì¢€ ë” ë§ˆìŒì´ ì•„í”„ë„¤ìš”.\n",
      "ë‹¹ì‹ : ë¬´ì—‡ì„ ë¨¹ì„ì§€ ì¶”ì²œí•´ì¤˜\n",
      "ì±—ë´‡: ëì„ ì•„ëŠ” ê²ƒë„ ì¤‘ìš”í•œ ê²ƒ ê°™ì•„ìš”.\n",
      "ë‹¹ì‹ : ì–´ì œ ì €ë…ì„ ì•ˆ ë¨¹ì—ˆë”ë‹ˆ. ë„ˆë¬´ í—ˆê¸°ì ¸\n",
      "ì±—ë´‡: ìŠ¬í”ˆ ì˜ˆê°ì€ í‹€ë¦° ì ì´ ì—†ì£ .\n",
      "ë‹¹ì‹ : ì§€ê¸ˆ ê³¼ì ì˜ˆê°ì„ ë¨¹ìœ¼ë¼ê³  í•œê±°ì•¼?\n",
      "ì±—ë´‡: ì±™ê²¨ì£¼ê³  ì‹¶ë‚˜ë´ìš”.\n",
      "ë‹¹ì‹ : ë„ˆ ë¨¹ì„ ì¤„ ì•„ëŠ”êµ¬ë‚˜?\n",
      "ì±—ë´‡: ì‚¬ëŒì€ í˜ì´ ë˜ì§€ ì•Šê² ì§€ë§Œ í˜ë‚´ì„¸ìš”.\n",
      "ë‹¹ì‹ : ì´ì œ ê·¸ë§Œí•˜ì\n",
      "ì±—ë´‡: ì¢‹ì€ ì‹œê°„ì´ ë§Œë‚  ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.\n",
      "ë‹¹ì‹ : ì¢…ë£Œ\n",
      "ì±—ë´‡: ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì±—ë´‡ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "def chatbot_response(input_text, model, src_tokenizer, tgt_tokenizer):\n",
    "    # ì…ë ¥ ë¬¸ì¥ ì „ì²˜ë¦¬\n",
    "    processed_input = preprocess_sentence(input_text)\n",
    "    \n",
    "    # ëª¨ë¸ ì˜ˆì¸¡ì„ ìœ„í•œ ë²ˆì—­ í•¨ìˆ˜ í˜¸ì¶œ\n",
    "    response = evaluate(processed_input, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ì‹œì‘\n",
    "print(\"ì•ˆë…•í•˜ì„¸ìš” ë¯¼í˜ì˜ ì±—ë´‡ì…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ'ë¼ê³  ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "while True:\n",
    "    user_input = input(\"ë‹¹ì‹ : \")\n",
    "    \n",
    "    if user_input.lower() == \"ì¢…ë£Œ\":\n",
    "        print(\"ì±—ë´‡: ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!\")\n",
    "        break\n",
    "\n",
    "    # ëª¨ë¸ì„ í†µí•œ ë‹µë³€ ìƒì„±\n",
    "    bot_response = chatbot_response(user_input, transformer, tokenizer, tokenizer)\n",
    "    print(f\"ì±—ë´‡: {bot_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7918b",
   "metadata": {},
   "source": [
    "## íšŒê³ \n",
    "ì´ë²ˆ í•™ìŠµê³¼ í”„ë¡œì íŠ¸ì—ì„œ ë²ˆì—­ê³¼ ê´€ë ¨ëœ ë°ì´í„°, ëª¨ë¸, ì„±ëŠ¥ í‰ê°€ ë“± ë§ì€ ê²ƒë“¤ì„ ë°°ì›Œë³´ë©° ì‹¤ìŠµì„ ì§„í–‰í•´ë´¤ë‹¤.\n",
    "\n",
    "ì‹¤ìŠµì„ í–ˆë˜ ê³¼ì •ì„ í”„ë¡œì íŠ¸ë¡œ ì ìš©í•´ë³´ë‹ˆ ì‰½ì§€ ì•Šì•˜ì§€ë§Œ.\n",
    "\n",
    "ì¬ë°Œì—ˆê³  ë˜í•œ GPT ê°™ì€ ëª¨ë¸ì€ ì–¼ë§ˆë‚˜ ë§ì€ í•™ìŠµì„ í–ˆê³  ë§ì€ ë…¸ë ¥ìœ¼ë¡œ ë§Œë“  ì±—ë´‡ì¸ì§€ ì²´ê°ì´ ë” ë˜ì—ˆë‹¤. í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ëŠ” ë¶€ë¶„ì´ë¼ë˜ì§€ ë” ì½”ë“œì ì¸ ë©´ì—ì„œ ëª¨ë“ˆí™”í•˜ê³  í•¨ìˆ˜í™”í•´ì„œ ë” ê³µë¶€í•˜ìëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c314e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e35ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e7dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed01fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9061b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310287fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea28f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
