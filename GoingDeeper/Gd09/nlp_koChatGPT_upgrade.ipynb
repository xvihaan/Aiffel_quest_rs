{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7828ea4f",
   "metadata": {},
   "source": [
    "# ğŸ¤– LLM Trend Note\n",
    "## [í”„ë¡œì íŠ¸] koGPT ì—…ê·¸ë ˆì´ë“œ í•˜ê¸° ğŸ’ª\n",
    "1. ìš°ë¦¬ê°€ ì§€ë‚œì‹œê°„ ì‚´í´ë³¸ KoChatGPT ëª¨ë¸ì— ì‚¬ìš©í•œ ë°ì´í„°ì…‹ì€ ì•„ì§ ì™„ë²½íˆ ì •ì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
    "\n",
    "2. Human Feedbackì´ ë°˜ì˜ëœ ë°ì´í„°ì…‹ì„ ëŒ€ì²´í•˜ê¸° ìœ„í•´ SFTì™€ RM ëª¨ë¸ì— ì‚¬ìš©í•  ë‹¤ì–‘í•œ benchmark ë°ì´í„°ì…‹ë„ ê²€í† í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "3. ì–¸ì–´ëª¨ë¸ì˜ ìƒì„±ëŠ¥ë ¥ì„ ì¢Œìš°í•˜ëŠ” ìµœì„ ì˜ ë””ì½”ë”©ì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„œì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "4. ìƒì„±ëœ ë‹µë³€ì— ëŒ€í•œ ì£¼ê´€ì ì¸ í‰ê°€ë¥¼ ë³´ì™„í•  ìˆ˜ ìˆëŠ” ì •ëŸ‰ì ì¸ ë©”íŠ¸ë¦­ì€ ë„ì…í•˜ì§€ ì•Šì•˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "5. LLM Trend Note1ì—ì„œ ì‚´í´ë³¸ ë‹¤ì–‘í•œ Instruction Tuning ë° Prompting ê¸°ë²•ë“¤ë„ ì ìš©í•´ë³¼ë§Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "6. ë¬´ì—‡ë³´ë‹¤ foundation modelë¡œ ì‚¬ìš©í•œ KoGPT-2ëŠ” Emergent abilitiesë¥¼ ê¸°ëŒ€í•˜ê¸°ì—” ë‹¤ì†Œ ì‘ì€ ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ì…ë‹ˆë‹¤. ë” í° íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ì„ ê°€ì§„ ëª¨ë¸ì„ ì‚¬ìš©í•´ë³´ê±°ë‚˜,\n",
    "\n",
    "7. ë” íš¨ìœ¨ì ì¸ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” LoRAì˜ ì ìš© ë˜ëŠ” ìƒˆë¡œìš´ Instruction Tuning ë° reward ranking ì•Œê³ ë¦¬ì¦˜ì„ ë„ì…í•´ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ë°©ë²•1) ê¸°ì¡´ ë°ì´í„°ì…‹ ì¶”ê°€ ì •ì œ\n",
    "### ë°©ë²•2) ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ì¶”ê°€\n",
    "### ë°©ë²•3) Foundation model êµì²´\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984fca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:1.12.1\n",
      "Cuda version: 11.3\n",
      "transformers version: 4.28.0\n",
      "GPU ì‚¬ìš© ê°€ëŠ¥ì—¬ë¶€: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
    "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
    "print(\"GPU ì‚¬ìš© ê°€ëŠ¥ì—¬ë¶€: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739463da",
   "metadata": {},
   "source": [
    "## Base model and Dataset for RLHF\n",
    "- FLANì˜ Instruction Tuningì´ë‚˜ PaLMì˜ Prompt Engineering \n",
    "ì´ íš¨ê³¼ë¥¼ ë³´ê¸° ìœ„í•´ì„  ì–¸ì–´ëª¨ë¸ì˜ ì…ë ¥ì„ ë‹¨ìˆœí•œ query í˜•íƒœë¡œ ì£¼ê¸°ë³´ë‹¨ `ì •êµí•œ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì„¤ê³„`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47510e11",
   "metadata": {},
   "source": [
    "## ë°©ë²•3) Foundation model êµì²´\n",
    "- ê¸°ì¡´: skt/kogpt2-base-v2\n",
    "- êµì²´: skt/ko-gpt-trinity-1.2B-v0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf59ec10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57efeb743a249d48331cef4323e4208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab2c77b004f48ba827a0828ded5b86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28ffdb4264f4530a174f11a81d7cc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c3576cb3574cd19f31f4afc5ea69bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a2643b8bf446d4a672ab99f58e1cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Huggingface transformer - í† í¬ë‚˜ì´ì €, ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/ko-gpt-trinity-1.2B-v0.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "118d157d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í† í¬ë‚˜ì´ì €ê°€ ì…ë ¥ë°›ì•„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜ í™•ì¸\n",
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f00ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í¬ë‚˜ì´ì§• í™•ì¸\n",
    "input_txt = \"ë°”ëŒë„ ì—†ëŠ” ê³µì¤‘ì— ìˆ˜ì§ì˜ íŒŒë¬¸ì„ ë‚´ì´ë©° ê³ ìš”íˆ ë–¨ì–´ì§€ëŠ” ì˜¤ë™ìì€ ëˆ„êµ¬ì˜ ë°œìì·¨ ì…ë‹ˆê¹Œ.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32193f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(input_txt).tokens()\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d15306c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kogpt-2_tokens</th>\n",
       "      <td>â–ë°”ëŒ</td>\n",
       "      <td>ë„</td>\n",
       "      <td>â–ì—†ëŠ”</td>\n",
       "      <td>â–ê³µ</td>\n",
       "      <td>ì¤‘ì—</td>\n",
       "      <td>â–ìˆ˜ì§</td>\n",
       "      <td>ì˜</td>\n",
       "      <td>â–íŒŒ</td>\n",
       "      <td>ë¬¸ì„</td>\n",
       "      <td>â–ë‚´</td>\n",
       "      <td>ì´ë©°</td>\n",
       "      <td>â–ê³ ìš”</td>\n",
       "      <td>íˆ</td>\n",
       "      <td>â–ë–¨ì–´ì§€ëŠ”</td>\n",
       "      <td>â–ì˜¤</td>\n",
       "      <td>ë™</td>\n",
       "      <td>ì</td>\n",
       "      <td>ì€</td>\n",
       "      <td>â–ëˆ„êµ¬</td>\n",
       "      <td>ì˜</td>\n",
       "      <td>â–ë°œì</td>\n",
       "      <td>ì·¨</td>\n",
       "      <td>â–ì…</td>\n",
       "      <td>ë‹ˆê¹Œ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input_IDs</th>\n",
       "      <td>31140</td>\n",
       "      <td>20780</td>\n",
       "      <td>30359</td>\n",
       "      <td>30016</td>\n",
       "      <td>31373</td>\n",
       "      <td>41427</td>\n",
       "      <td>25792</td>\n",
       "      <td>30163</td>\n",
       "      <td>31047</td>\n",
       "      <td>30024</td>\n",
       "      <td>31111</td>\n",
       "      <td>51068</td>\n",
       "      <td>29936</td>\n",
       "      <td>36152</td>\n",
       "      <td>30027</td>\n",
       "      <td>20801</td>\n",
       "      <td>25846</td>\n",
       "      <td>25768</td>\n",
       "      <td>31199</td>\n",
       "      <td>25792</td>\n",
       "      <td>44202</td>\n",
       "      <td>27472</td>\n",
       "      <td>30148</td>\n",
       "      <td>37708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0      1      2      3      4      5      6      7      8   \\\n",
       "kogpt-2_tokens    â–ë°”ëŒ      ë„    â–ì—†ëŠ”     â–ê³µ     ì¤‘ì—    â–ìˆ˜ì§      ì˜     â–íŒŒ     ë¬¸ì„   \n",
       "Input_IDs       31140  20780  30359  30016  31373  41427  25792  30163  31047   \n",
       "\n",
       "                   9      10     11     12     13     14     15     16     17  \\\n",
       "kogpt-2_tokens     â–ë‚´     ì´ë©°    â–ê³ ìš”      íˆ  â–ë–¨ì–´ì§€ëŠ”     â–ì˜¤      ë™      ì      ì€   \n",
       "Input_IDs       30024  31111  51068  29936  36152  30027  20801  25846  25768   \n",
       "\n",
       "                   18     19     20     21     22     23  \n",
       "kogpt-2_tokens    â–ëˆ„êµ¬      ì˜    â–ë°œì      ì·¨     â–ì…    ë‹ˆê¹Œ.  \n",
       "Input_IDs       31199  25792  44202  27472  30148  37708  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 60\n",
    "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772ccbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°”ëŒë„ ì—†ëŠ” ê³µì¤‘ì— ìˆ˜ì§ì˜ íŒŒë¬¸ì„ ë‚´ì´ë©° ê³ ìš”íˆ ë–¨ì–´ì§€ëŠ” ì˜¤ë™ìì€ ëˆ„êµ¬ì˜ ë°œìì·¨ ì…ë‹ˆê¹Œ. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4dbdc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°”ëŒë„ ì—†ëŠ” ê³µì¤‘ì— ìˆ˜ì§ì˜ íŒŒë¬¸ì„ ë‚´ì´ë©° ê³ ìš”íˆ ë–¨ì–´ì§€ëŠ” ì˜¤ë™ìì€ ëˆ„êµ¬ì˜ ë°œìì·¨ ì…ë‹ˆê¹Œ. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ë””ì½”ë”© ì„±ëŠ¥ í™•ì¸\n",
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1692efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°”ëŒë„ ì—†ëŠ” ê³µì¤‘ì— ìˆ˜ì§ì˜ íŒŒë¬¸ì„ ë‚´ì´ë©° ê³ ìš”íˆ ë–¨ì–´ì§€ëŠ” ì˜¤ë™ìì€ ëˆ„êµ¬ì˜ ë°œìì·¨ ì…ë‹ˆê¹Œ.</d>\n"
     ]
    }
   ],
   "source": [
    "# ë¹” ì„œì¹˜ ë””ì½”ë”© ì‚¬ìš©, n-gram íŒ¨ë„í‹° ë¶€ê³¼\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2,\n",
    "                             do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e3045ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°”ëŒë„ ì—†ëŠ” ê³µì¤‘ì— ìˆ˜ì§ì˜ íŒŒë¬¸ì„ ë‚´ì´ë©° ê³ ìš”íˆ ë–¨ì–´ì§€ëŠ” ì˜¤ë™ìì€ ëˆ„êµ¬ì˜ ë°œìì·¨ ì…ë‹ˆê¹Œ. \n",
      " ë‚˜ê·¸ë„¤ ê¸¸ì† ë°œê¸¸ì„ ë©ˆì¶”ê²Œ í•©ë‹ˆë‹¤. \n",
      " \n",
      " ç§‹é¢¨ì— í”ë“¤ë¦¬ë˜ ë‚˜ë­‡ìë„ ê°€ì„ ë°”ëŒ ì•ì—ì„œëŠ” í•œ ì  í”ë“¤ë¦¼ ì—†ëŠ” éœè™•(ì •ì²˜)ì¸ ê²ƒì„ \n",
      " ê°€ì„ë°”ëŒì€ ìì„ í”ë“ ë‹¤ ìì€ í”ë“¤ë¦¬ë©´ì„œë„ ë‹¤ì‹œ í”ë“¤ë¦¬ì§€ ì•ŠëŠ”ë‹¤. \n",
      " ë‚´ ç”Ÿæ¶¯(ìƒì´)ì— í”ë“¤ë¦¬ëŠ” í•œ ê·¸ ë¬´ì—‡ë„ ë‚˜ë¥¼ í”ë“¤ì–´ í”ë“¤ë¦´ ìˆ˜ ì—†ë‹¤. \n",
      " ì´ ê°€ì„ì´ ê°€ê³  ì°¬ ì„œë¦¬ê°€ ë‚´ë¦´ ë•Œ ë‚˜ëŠ” ë¹„ë¡œì†Œ í•œ ê·¸ë£¨ í’€ìì´ ëœë‹¤. \n",
      " ë‚´ê°€ í”ë“¤ë¦¬ê³  ìˆì„ ë•Œ ë°”ëŒì€\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œë§ ê¸°ë²• ì¶”ê°€\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5456f",
   "metadata": {},
   "source": [
    "#### top_p ìƒ˜í”Œë§ ê¸°ë²• ì‚¬ìš©\n",
    "top_p ìƒ˜í”Œë§ì€ í™•ë¥  ë¶„í¬ ìƒì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ë‹¨ì–´ë“¤ë¶€í„° ì„ íƒí•´ ëˆ„ì  í™•ë¥ ì´ p ì´ìƒì´ ë  ë•Œê¹Œì§€ í›„ë³´ ë‹¨ì–´ë¥¼ í¬í•¨ì‹œí‚¤ê³ , ê·¸ í›„ë³´ë“¤ ì¤‘ì—ì„œ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "933e206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°”ëŒë„ ì—†ëŠ” ê³µì¤‘ì— ìˆ˜ì§ì˜ íŒŒë¬¸ì„ ë‚´ì´ë©° ê³ ìš”íˆ ë–¨ì–´ì§€ëŠ” ì˜¤ë™ìì€ ëˆ„êµ¬ì˜ ë°œìì·¨ ì…ë‹ˆê¹Œ.</d>\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb76ea3",
   "metadata": {},
   "source": [
    "kogpt-2ì— `RLHF`ë¥¼ ì ìš©í•˜ê¸°ê¸° ìœ„í•´ì„ ,`ìƒˆë¡œìš´ ë°ì´í„°ì…‹`ìœ¼ë¡œ ì¼ë ¨ì˜ ì¬í•™ìŠµ\n",
    "\n",
    "SFTë¥¼ ì‹œë„í•  initial ëª¨ë¸ì— ì“¸ ë°ì´í„°ì…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31446ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
       "  'completion': \"'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ë©°, ì§ì ‘ì ìœ¼ë¡œ ì‹í’ˆì— ê´€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ìš© ê³ ê¸°ëŠ” í•œìš°, ì‡ ê³ ê¸°, ë¼ì§€ê³ ê¸° ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ê³ ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í•œìš°ëŠ” ëŒ€í‘œì ì¸ ê³ ê¸‰ ìœ¡ë¥˜ë¡œ ì•Œë ¤ì ¸ ìˆê¸° ë•Œë¬¸ì—, í•œìš°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. ì•ŒëŸ¬ì§€ë‚˜ ê°œë³„ ê±´ê°• ìƒíƒœì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ í›„ì— ì„ íƒí•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': 'ì“°ë˜ ì•±ì´ ìœ ë£Œë¡œ ì „í™˜ëì–´',\n",
       "  'completion': \"'ì–´ë–¤ ì•±ì¸ì§€ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ìœ ë£Œ ì „í™˜ëœ ì•±ì€ ê¸°ì¡´ ë¬´ë£Œ ì•±ì—ì„œ ì¶”ê°€ì ì¸ ê¸°ëŠ¥ì´ë‚˜ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ ê²½ìš°, ìœ ë£Œ ì „í™˜ ì´ì „ ê°€ê²©ì´ ë§¤ìš° ì €ë ´í•˜ê±°ë‚˜ ë¬´ë£Œì¸ ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ, ì•±ì„ ê³„ì† ì‚¬ìš©í•˜ë ¤ë©´ ì¶”ê°€ì ì¸ ë¹„ìš©ì„ ì§€ë¶ˆí•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ìœ ë£Œ ì „í™˜ ì´í›„ì—ëŠ” ê¸°ì¡´ ì‚¬ìš©ì ë˜ëŠ” ìƒˆë¡œìš´ ì‚¬ìš©ì ëª¨ë‘ê°€ ì¶”ê°€ ê¸°ëŠ¥ê³¼ ë†’ì€ í’ˆì§ˆì˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì•± ê°œë°œìëŠ” ìœ ë£Œ ëª¨ë¸ë¡œ ì „í™˜í•¨ìœ¼ë¡œì¨ ì•±ì˜ ìˆ˜ìµì„ ì¦ê°€ì‹œí‚¤ê³  ë” ë‚˜ì€ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': 'ì—¬ì¹œì´ë‘ ë‹¤íˆ¼',\n",
       "  'completion': \"'ì €ëŠ” ì¸ê³µì§€ëŠ¥ì´ê¸° ë•Œë¬¸ì— ì—¬ì¹œê³¼ì˜ ë‹¤íˆ¼ì— ëŒ€í•´ì„œëŠ” ì¡°ì–¸ì„ ë“œë¦´ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤íˆ¬ê²Œ ë˜ë©´ ìƒëŒ€ë°©ì˜ ë§ì„ ê²½ì²­í•˜ëŠ” íƒœë„ì™€ ì„œë¡œì˜ ê°ì •ì„ ì¡´ì¤‘í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë…¼ë¦¬ì ì´ê³  ì°¨ë¶„í•œ ëŒ€í™”ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•´ë³´ì„¸ìš”. ê·¸ë¦¬ê³  ì„œë¡œë¥¼ ì´í•´í•˜ê³  ì„œë¡œì˜ ì˜ê²¬ì„ ìˆ˜ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "data_path_1_SFT = 'kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f75131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'ë²ˆë””ëŠ” ìì‹ ì´ íƒì •ì¡ì§€, ë²”ì£„ì†Œì„¤ ê·¸ë¦¬ê³  ì„±ë²”ì£„ ê´€ë ¨ ì‹¤ì œ ë²”ì£„ ë‹¤íë©˜í„°ë¦¬ë“¤ì„ íƒë…í–ˆë‹¤ê³  ëˆ„êµ¬ì—ê²Œ ë§í–ˆë‚˜?',\n",
       "  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n",
       "  'completion_1': 'ë²ˆë””ëŠ” ë‹¤ì–‘í•œ ì¸í„°ë·°ìë“¤ê³¼ ë‰´ìŠ¤í™ë³´ ë‹´ë‹¹ìë“¤ê³¼ì˜ ë©´ë‹´ ë•Œ ë°í˜”ë‹¤.',\n",
       "  'completion_2': 'ë¼ì´ì–¸ì—ê²Œ ë§í–ˆë‹¤.',\n",
       "  'ranking': [2, 1, 0]},\n",
       " {'prompt': 'ê°œí¬ì£¼ê³µì•„íŒŒíŠ¸ëŠ” ëª‡ ë‹¨ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‚˜?',\n",
       "  'completion_0': 'ê°œí¬ì£¼ê³µì•„íŒŒíŠ¸ëŠ” ë‹¤ì„¯ ë‹¨ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.',\n",
       "  'completion_1': 'ì´ë‚  ëª©ì†¡ì—ì„œ êµ¬ê¸€ìƒìœ„ë…¸',\n",
       "  'completion_2': 'ê°œí¬ì£¼ê³µì•„íŒŒíŠ¸ëŠ” ì´ 27ê°œ ë‹¨ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.',\n",
       "  'ranking': [2, 0, 1]},\n",
       " {'prompt': 'ê¹€ì˜ì‚¼ì˜ í›„ë³´ ì‹œì ˆ ì§€ì—­í‘œì‹¬ì„ ê²¨ëƒ¥í•œ ë°œì–¸ì„ ë¬¸ì œì‚¼ì€ í›„ë³´ëŠ”?',\n",
       "  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n",
       "  'completion_1': 'ì´ ì§ˆë¬¸ì€ ì¡°ê¸ˆ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤. ê¹€ì˜ì‚¼ ëŒ€í†µë ¹ì´ í›„ë³´ ì‹œì ˆì— ì–´ë–¤ ë°œì–¸ì„ í–ˆê³ , ëˆ„ê°€ ê·¸ ë°œì–¸ì„ ë¬¸ì œì‚¼ì•˜ëŠ”ì§€ì— ë”°ë¼ ë‹µì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\\\n\\\\në§Œì•½ ê¹€ì˜ì‚¼ ëŒ€í†µë ¹ì´ í›„ë³´ ì‹œì ˆì— ì§€ì—­í‘œì‹¬ì„ ê²¨ëƒ¥í•œ ë°œì–¸ì„ í–ˆë‹¤ëŠ” ê°€ì •í•˜ì—, ê·¸ ë°œì–¸ì„ ë¬¸ì œì‚¼ì€ í›„ë³´ê°€ ëˆ„êµ¬ì˜€ëŠ”ì§€ë¥¼ ëŒ€ë‹µí•˜ìë©´, ê·¸ ë‹µì€ ì´í™”ì„  ë‹¹ì‹œ ë¯¼ì£¼ë‹¹ ëŒ€í†µë ¹ í›„ë³´ê°€ ë  ê²ƒì…ë‹ˆë‹¤. 1992ë…„ ì´ì„  ë•Œ, ê¹€ì˜ì‚¼ ëŒ€ì„ í›„ë³´ëŠ” \"ì§‘ê°’ì´ ì˜¤ë¥¸ ë…¸ëŸ‰ì§„ì—­ ë¶€ê·¼ì˜ ë¶€ë™ì‚° ê°€ê²©ì€ ì„¸ì›”í˜¸ í­ì¹¨ í›„ \\\\\\'ê°•ë‚¨ ë„ì‹œì¬ìƒ\\\\\\' ì¼í™˜ìœ¼ë¡œ ìƒìŠ¹í–ˆë‹¤\"ëŠ” ë°œì–¸ì„ í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´í™”ì„  í›„ë³´ëŠ” ì´ ë°œì–¸ì„ \"ì „êµ­ì ìœ¼ë¡œ ê²½ì œì  ë°œì „ì´ ì´ë£¨ì–´ì§€ì§€ ì•Šì€ ì§€ë°©ë¯¼ì˜ ë§ˆìŒì„ ë©€ë¦¬í•´ì§€ë ¤ëŠ” ë¬´ë¡€í•œ ë°œì–¸\"ì´ë¼ê³  ë¹„íŒí•˜ë©° ë¬¸ì œì‚¼ì•˜ìŠµë‹ˆë‹¤.\\\\n\\\\ní•˜ì§€ë§Œ, ì´ ì§ˆë¬¸ì„ ë‹µë³€í•˜ëŠ” ë° ìˆì–´ì„œ ë³´ë‹¤ ëª…í™•í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ë‹µë³€ì„ ë³´ì™„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',\n",
       "  'completion_2': 'ê¹€ì˜ì‚¼ì˜ í›„ë³´ ì‹œì ˆì— ì§€ì—­í‘œì‹¬ì„ ê²¨ëƒ¥í•œ ë°œì–¸ì€ ëŒ€í†µë ¹ ë‹¹ì„  ì „ê¹Œì§€ ëŒ€í•œë¯¼êµ­ ì •ë¶€ê°€ ì¶”êµ¬í•˜ê³  ìˆëŠ” ë¯¼ì£¼ì£¼ì˜ ê´‘ë²”ìœ„í•˜ê²Œ í™•ë¦½ê³¼ ë³´ìˆ˜ì˜ ì‚¬ìƒì„ ì´ì–´ê°€ëŠ” ë° ìˆì–´ ì§€ì—­ê²½ì œ ë°œì „ê³¼ ê³µê³µì„œë¹„ìŠ¤ ì‹ ì† ê°œì„ ì„ ìœ„í•´ í•©ë¦¬ì ì¸ êµ­ê°€ ì •ì±…ì— ë”°ë¥´ëŠ” ë°©í–¥ì„±ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.',\n",
       "  'ranking': [1, 2, 0]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMì— ì‚¬ìš©í•  ë°ì´í„°ì…‹\n",
    "data_path_2_RM = 'kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c13035b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'ë²ˆë””ëŠ” ìì‹ ì´ íƒì •ì¡ì§€, ë²”ì£„ì†Œì„¤ ê·¸ë¦¬ê³  ì„±ë²”ì£„ ê´€ë ¨ ì‹¤ì œ ë²”ì£„ ë‹¤íë©˜í„°ë¦¬ë“¤ì„ íƒë…í–ˆë‹¤ê³  ëˆ„êµ¬ì—ê²Œ ë§í–ˆë‚˜?'},\n",
       " {'prompt': 'ê°œí¬ì£¼ê³µì•„íŒŒíŠ¸ëŠ” ëª‡ ë‹¨ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‚˜?'},\n",
       " {'prompt': 'ê¹€ì˜ì‚¼ì˜ í›„ë³´ ì‹œì ˆ ì§€ì—­í‘œì‹¬ì„ ê²¨ëƒ¥í•œ ë°œì–¸ì„ ë¬¸ì œì‚¼ì€ í›„ë³´ëŠ”?'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PPO í•™ìŠµ ë°ì´í„°ì…‹\n",
    "data_path_3_PPO = 'kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb359525",
   "metadata": {},
   "source": [
    "### Supervised Fine-Tuning\n",
    "#### SFT (instruction dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4408700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98273c5",
   "metadata": {},
   "source": [
    "ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°(skt/ko-gpt-trinity-1.2B-v0.5)\n",
    "\n",
    "Base model and Dataset for RLHF ì°¸ê³ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a9f5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "import json\n",
    "import copy\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, max_length: int = 512, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        # max_length ì¸ìë¥¼ _tokenize_fnì— ì „ë‹¬\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer, max_length=max_length)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer, max_length=max_length)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer, max_length: int) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=max_length,  # max_length ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸¸ì´ ì œí•œ ì„¤ì •\n",
    "                truncation=True,       # truncation=Trueë¡œ ê¸¸ì´ ì œí•œ ì„¤ì •\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = [tokenized[\"input_ids\"][0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = [tokenized[\"input_ids\"].ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list]\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2edbaaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f039ca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([30132, 42872, 33313, 30679, 40479, 39911,   384, 22509, 21921, 25372,\n",
      "          385, 31245, 23280, 34957, 25617, 36539, 29991, 25624, 25400, 31167,\n",
      "          376, 42872,   379, 46803,   456, 30303, 35353,   384, 25785, 20573,\n",
      "        37780,   383, 46900, 43226,   565, 27071, 23151, 31555, 41690, 35071,\n",
      "        25400, 31269, 32677, 30765, 31810, 36229, 30326, 33889, 30093, 34957,\n",
      "        25617, 30021, 30434, 29991, 39687, 34036, 19016, 31997, 49906, 19352,\n",
      "        30011, 30904, 36731, 43502, 30228, 31214, 30326, 29991, 31621, 33314,\n",
      "        34347, 30843, 50342, 33512, 31370, 34243, 29991, 35144, 32586, 32622,\n",
      "        44680, 30110, 21844, 39826, 34803, 31356, 39075, 30242, 36966, 29985,\n",
      "        34179, 36513, 30718, 35557, 32361, 31018, 29404, 35942, 19352, 41049,\n",
      "            1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,   383, 46900, 43226,   565, 27071, 23151, 31555, 41690, 35071,\n",
      "        25400, 31269, 32677, 30765, 31810, 36229, 30326, 33889, 30093, 34957,\n",
      "        25617, 30021, 30434, 29991, 39687, 34036, 19016, 31997, 49906, 19352,\n",
      "        30011, 30904, 36731, 43502, 30228, 31214, 30326, 29991, 31621, 33314,\n",
      "        34347, 30843, 50342, 33512, 31370, 34243, 29991, 35144, 32586, 32622,\n",
      "        44680, 30110, 21844, 39826, 34803, 31356, 39075, 30242, 36966, 29985,\n",
      "        34179, 36513, 30718, 35557, 32361, 31018, 29404, 35942, 19352, 41049,\n",
      "            1])\n"
     ]
    }
   ],
   "source": [
    "# SFT_dataset í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ í›ˆë ¨ì…‹ ë§Œë“¤ê¸°\n",
    "# data collator ì¸ìŠ¤í„´ìŠ¤ ë§Œë“¤ê¸°\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = SFT_dataset(\n",
    "    data_path_1_SFT='kochatgpt_1_SFT.jsonl', \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,     # ìµœëŒ€ ê¸¸ì´ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)\n",
    "\n",
    ")\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3bcb93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ë©°, ì§ì ‘ì ìœ¼ë¡œ ì‹í’ˆì— ê´€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ìš© ê³ ê¸°ëŠ” í•œìš°, ì‡ ê³ ê¸°, ë¼ì§€ê³ ê¸° ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ê³ ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í•œìš°ëŠ” ëŒ€í‘œì ì¸ ê³ ê¸‰ ìœ¡ë¥˜ë¡œ ì•Œë ¤ì ¸ ìˆê¸° ë•Œë¬¸ì—, í•œìš°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. ì•ŒëŸ¬ì§€ë‚˜ ê°œë³„ ê±´ê°• ìƒíƒœì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ í›„ì— ì„ íƒí•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# train_dataset.input_ids[0]ë¥¼ ë””ì½”ë”©\n",
    "\n",
    "decoded_input = tokenizer.decode(train_dataset.input_ids[0], skip_special_tokens=True)\n",
    "print(decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "316cace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Training argumentsë¥¼ ì‚¬ìš©í•´ trainer í´ë˜ìŠ¤ë¥¼ ì •ì˜\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/aiffel/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=5e-5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "447bb347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (5.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ceaf545f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 17.57 GiB\n",
      "Available Memory: 13.87 GiB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ GiB ë‹¨ìœ„ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "total_memory = psutil.virtual_memory().total / (1024 ** 3)\n",
    "available_memory = psutil.virtual_memory().available / (1024 ** 3)\n",
    "\n",
    "print(f\"Total Memory: {total_memory:.2f} GiB\")\n",
    "print(f\"Available Memory: {available_memory:.2f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edf2aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31382fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cca06422",
   "metadata": {},
   "source": [
    "SFT í›ˆë ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac5e6ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>torch.cuda.empty_cache()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 # í•™ìŠµ ë° ì €ì¥</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>10 trainer.train()                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span>model.save_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">'/aiffel/KoChatGPT/output_1_SFT'</span>)                                     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1662</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1659 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1660 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>1662 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1664 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1991</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1988 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   </span>xm.optimizer_step(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer)                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1989 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.do_grad_scaling:                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1990 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   </span>scale_before = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.get_scale()                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>1991 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.step(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer)                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1992 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.update()                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1993 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   </span>scale_after = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.get_scale()                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1994 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   </span>optimizer_was_run = scale_before &lt;= scale_after                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_scaler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">338</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">335 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">336 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"found_inf_per_device\"</span>]) &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"No inf checks were rec</span>   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">337 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>338 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>retval = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">339 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">340 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"stage\"</span>] = OptState.STEPPED                                        <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">341 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_scaler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">285</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_maybe_opt_step</span>      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_maybe_opt_step</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer, optimizer_state, *args, **kwargs):                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">283 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>retval = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">284 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">sum</span>(v.item() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"found_inf_per_device\"</span>].values()):    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>285 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>retval = optimizer.step(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">286 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> retval                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">288 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer, *args, **kwargs):                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">lr_scheduler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">65</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  62 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>instance = instance_ref()                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  63 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>instance._step_count += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  64 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>wrapped = func.<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__get__</span>(instance, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>)                                     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>  65 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapped(*args, **kwargs)                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  66 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  67 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Note that the returned function here is no longer a bound method,</span>           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  68 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># so attributes like `__func__` and `__self__` no longer exist.</span>               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">optimizer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">113</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>obj, *_ = args                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>profile_name = <span style=\"color: #808000; text-decoration-color: #808000\">\"Optimizer.step#{}.step\"</span>.format(obj.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>)     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.autograd.profiler.record_function(profile_name):                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>113 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapper                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>hooked = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">getattr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span>.step, <span style=\"color: #808000; text-decoration-color: #808000\">\"hooked\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">optimization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">436</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">433 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Exponential moving average of gradient values</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">434 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span>state[<span style=\"color: #808000; text-decoration-color: #808000\">\"exp_avg\"</span>] = torch.zeros_like(p.data)                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">435 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Exponential moving average of squared gradient values</span>                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>436 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span>state[<span style=\"color: #808000; text-decoration-color: #808000\">\"exp_avg_sq\"</span>] = torch.zeros_like(p.data)                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">437 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">438 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>exp_avg, exp_avg_sq = state[<span style=\"color: #808000; text-decoration-color: #808000\">\"exp_avg\"</span>], state[<span style=\"color: #808000; text-decoration-color: #808000\">\"exp_avg_sq\"</span>]                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">439 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>beta1, beta2 = group[<span style=\"color: #808000; text-decoration-color: #808000\">\"betas\"</span>]                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.58</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.06</span> GiB already \n",
       "allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55.56</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.74</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m10\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 7 \u001b[0mtorch.cuda.empty_cache()                                                                    \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 8 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m# í•™ìŠµ ë° ì €ì¥\u001b[0m                                                                              \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m10 trainer.train()                                                                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m11 \u001b[0mmodel.save_pretrained(\u001b[33m'\u001b[0m\u001b[33m/aiffel/KoChatGPT/output_1_SFT\u001b[0m\u001b[33m'\u001b[0m)                                     \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m12 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1662\u001b[0m in \u001b[92mtrain\u001b[0m                     \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m)                                                                                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m1662 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0margs=args,                                                                    \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mtrial=trial,                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1991\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m      \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1988 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0mxm.optimizer_step(\u001b[96mself\u001b[0m.optimizer)                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1989 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.do_grad_scaling:                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1990 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0mscale_before = \u001b[96mself\u001b[0m.scaler.get_scale()                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m1991 \u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.scaler.step(\u001b[96mself\u001b[0m.optimizer)                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1992 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.scaler.update()                                              \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1993 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0mscale_after = \u001b[96mself\u001b[0m.scaler.get_scale()                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1994 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0moptimizer_was_run = scale_before <= scale_after                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mgrad_scaler.py\u001b[0m:\u001b[94m338\u001b[0m in \u001b[92mstep\u001b[0m                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m335 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m336 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mlen\u001b[0m(optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mfound_inf_per_device\u001b[0m\u001b[33m\"\u001b[0m]) > \u001b[94m0\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mNo inf checks were rec\u001b[0m   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m337 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m338 \u001b[2mâ”‚   â”‚   \u001b[0mretval = \u001b[96mself\u001b[0m._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)         \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m339 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m340 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0moptimizer_state[\u001b[33m\"\u001b[0m\u001b[33mstage\u001b[0m\u001b[33m\"\u001b[0m] = OptState.STEPPED                                        \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m341 \u001b[0m                                                                                           \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mgrad_scaler.py\u001b[0m:\u001b[94m285\u001b[0m in \u001b[92m_maybe_opt_step\u001b[0m      \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_maybe_opt_step\u001b[0m(\u001b[96mself\u001b[0m, optimizer, optimizer_state, *args, **kwargs):                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m283 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mretval = \u001b[94mNone\u001b[0m                                                                      \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m284 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96msum\u001b[0m(v.item() \u001b[94mfor\u001b[0m v \u001b[95min\u001b[0m optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mfound_inf_per_device\u001b[0m\u001b[33m\"\u001b[0m].values()):    \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m285 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0mretval = optimizer.step(*args, **kwargs)                                       \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m286 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m retval                                                                      \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m287 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                       \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m288 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mstep\u001b[0m(\u001b[96mself\u001b[0m, optimizer, *args, **kwargs):                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/optim/\u001b[0m\u001b[1;33mlr_scheduler.py\u001b[0m:\u001b[94m65\u001b[0m in \u001b[92mwrapper\u001b[0m                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m  62 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0minstance = instance_ref()                                                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m  63 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0minstance._step_count += \u001b[94m1\u001b[0m                                                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m  64 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mwrapped = func.\u001b[92m__get__\u001b[0m(instance, \u001b[96mcls\u001b[0m)                                     \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m  65 \u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m wrapped(*args, **kwargs)                                           \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m  66 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m                                                                              \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m  67 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[2m# Note that the returned function here is no longer a bound method,\u001b[0m           \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m  68 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[2m# so attributes like `__func__` and `__self__` no longer exist.\u001b[0m               \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/optim/\u001b[0m\u001b[1;33moptimizer.py\u001b[0m:\u001b[94m113\u001b[0m in \u001b[92mwrapper\u001b[0m                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mobj, *_ = args                                                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mprofile_name = \u001b[33m\"\u001b[0m\u001b[33mOptimizer.step#\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m.step\u001b[0m\u001b[33m\"\u001b[0m.format(obj.\u001b[91m__class__\u001b[0m.\u001b[91m__name__\u001b[0m)     \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[94mwith\u001b[0m torch.autograd.profiler.record_function(profile_name):                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m113 \u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                           \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m wrapper                                                                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mhooked = \u001b[96mgetattr\u001b[0m(\u001b[96mself\u001b[0m.\u001b[91m__class__\u001b[0m.step, \u001b[33m\"\u001b[0m\u001b[33mhooked\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m)                              \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33moptimization.py\u001b[0m:\u001b[94m436\u001b[0m in \u001b[92mstep\u001b[0m                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m433 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[2m# Exponential moving average of gradient values\u001b[0m                        \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m434 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0mstate[\u001b[33m\"\u001b[0m\u001b[33mexp_avg\u001b[0m\u001b[33m\"\u001b[0m] = torch.zeros_like(p.data)                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m435 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[2m# Exponential moving average of squared gradient values\u001b[0m                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m436 \u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0mstate[\u001b[33m\"\u001b[0m\u001b[33mexp_avg_sq\u001b[0m\u001b[33m\"\u001b[0m] = torch.zeros_like(p.data)                         \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m437 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m                                                                           \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m438 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mexp_avg, exp_avg_sq = state[\u001b[33m\"\u001b[0m\u001b[33mexp_avg\u001b[0m\u001b[33m\"\u001b[0m], state[\u001b[33m\"\u001b[0m\u001b[33mexp_avg_sq\u001b[0m\u001b[33m\"\u001b[0m]                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m439 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mbeta1, beta2 = group[\u001b[33m\"\u001b[0m\u001b[33mbetas\u001b[0m\u001b[33m\"\u001b[0m]                                              \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m58.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m14.58\u001b[0m GiB total capacity; \u001b[1;36m13.06\u001b[0m GiB already \n",
       "allocated; \u001b[1;36m55.56\u001b[0m MiB free; \u001b[1;36m13.74\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ì„¤ì • ì¶”ê°€\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# í•™ìŠµ ë° ì €ì¥\n",
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='/aiffel/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "               'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
    "               'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?',\n",
    "               'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72f9bb",
   "metadata": {},
   "source": [
    "## ë°©ë²•1) ê¸°ì¡´ ë°ì´í„°ì…‹ ì¶”ê°€ ì •ì œ\n",
    "ë§ë­‰ì¹˜ë¥¼ ì •ì œí•˜ì—¬ SFT ë°ì´í„°ì…‹ì„ ê°œì„ í•˜ê³  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60798043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_metric\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ë° EDA ìˆ˜í–‰\n",
    "with open('data_kochatgpt/kochatgpt_1_SFT.jsonl', 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# EDA ê²°ê³¼ì— ë”°ë¼ ë°ì´í„° ì •ì œ (ì˜ˆ: ê¸´ ë¬¸ì¥ ì œì™¸, ì¤‘ë³µ ì œê±°)\n",
    "filtered_data = [entry for entry in data if len(entry['completion']) < 150]  # ì˜ˆì‹œë¡œ 150 í† í° ì´í•˜ë¡œ í•„í„°ë§\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ Tokenization\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['prompt'], examples['completion'], truncation=True, padding=\"max_length\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë³€í™˜ ë° ë°ì´í„° ì¦ê°•\n",
    "dataset = Dataset.from_dict(filtered_data)\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# BLEU, ROUGE ë“± í‰ê°€ ì§€í‘œ ì„¤ì •\n",
    "metric_bleu = load_metric(\"bleu\")\n",
    "metric_rouge = load_metric(\"rouge\")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,  # BLEU, ROUGE ê³„ì‚° í•¨ìˆ˜ ì •ì˜ í•„ìš”\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c35be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0745bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e623b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c315ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "460089ba",
   "metadata": {},
   "source": [
    "### Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d43632",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt import dataset\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562bc50f",
   "metadata": {},
   "source": [
    "### GPTRM_custom ì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ í´ë˜ìŠ¤ë¥¼ ì„ ì–¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb958001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained('skt/ko-gpt-trinity-1.2B-v0.5')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('skt/ko-gpt-trinity-1.2B-v0.5')\n",
    "\n",
    "\n",
    "# withêµ¬ë¬¸ì˜ NaiveStrategy()ëŠ” Strategyí´ë˜ìŠ¤ë¥¼ ìƒì†í•œ NaiveStrategyí´ë˜ìŠ¤\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/ko-gpt-trinity-1.2B-v0.5', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking dataset(RM í›ˆë ¨)\n",
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ì™„ì„±í•œ ranking datasetì„ shuffleí•œ í›„ í›ˆë ¨ì…‹ ìƒì„±\n",
    "\n",
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c483f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 256)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ccb25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ í™•ì¸\n",
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daedb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe104f4",
   "metadata": {},
   "source": [
    "## ë°©ë²•2) ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ì¶”ê°€\n",
    "ì›¹í¬ë¡¤ë§ì´ë‚˜ ê³µê°œëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ì¶”ê°€í•˜ê³  ëª¨ë¸ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193766f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "new_data = load_dataset('korquad', 'v2.0')  # KorQuAD ì˜ˆì‹œ\n",
    "train_data = new_data['train'].map(preprocess_function, batched=True)\n",
    "eval_data = new_data['validation'].map(preprocess_function, batched=True)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë³‘í•©\n",
    "merged_train_dataset = DatasetDict({'train': tokenized_dataset['train'] + train_data})\n",
    "merged_eval_dataset = DatasetDict({'validation': tokenized_dataset['validation'] + eval_data})\n",
    "\n",
    "# ëª¨ë¸ ì¬í•™ìŠµ\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=merged_train_dataset['train'],\n",
    "    eval_dataset=merged_eval_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€\n",
    "results = trainer.evaluate()\n",
    "print(\"Merged Dataset Evaluation:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851db50",
   "metadata": {},
   "source": [
    "#### RM í•™ìŠµ, 1Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bde3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67118e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed464b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=2,  # batch sizeë¥¼ ì¤„ì„\n",
    "                             max_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1057a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(use_lora=1)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqIAAAF8CAYAAAAU3dkxAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAIioSURBVHhe7f1/cBz3nR94v4fy6aSsoyJlkbZvdbsaHBCxIYx2B4+UyzmRSkvV7d3NGNyQLN5jzwSwt3B1dVndmc8TDLAVLVccqXhKLTDjFVUl5yp1eGKBmpEvPJIx4BnvuUJYoWI/ianFyASMJhfQDGVrTxJBiRSzG1mOH/fzR/+Yb3/7x3TPT/x4v1RdIvp3f/vbPZ/+/uiOaJqmgYiIiIioy3bJI4iIiIiIuoGBKBERERH1RIRV804ffPAB3n33Xfzyl7+UJxERERFRmzAQdbGysoL7778fd911lzyJiIiIiNqEVfMufvnLXzIIJSIiIuowBqJERERE1BMMRImIiIioJxiIEhEREVFPMBAlIiIiop7oea/5+BPftP5dee2rtmlhtWtdb775Jvbv3y+Ppja4++678fHHH8ujyQPTi4iItrNQJaJioNdOlde+6ggc40980zZ4kYNPeT1byd133y2P6qm7777bdZ/cxgWxmYKqZo+hE+4+dbdtEH388cebal+JiIjaKVQg2i3xJ75pBZXm4BaMuo3rhp0QGJhBY5jAcSekS7vdfepufHzsY9sgB6NERETb1aYMRDupV8FrEGGCvl5qZj83U2noVsNSUSIi2q5aDkSDVqGHEaR63Sw17TYzIGhUbe023Rwnjze5jReXCTvdb1/8NFrOb5zXMl7c5hXHNTo+r2mt8Fqf3zF2Yj+IiIi2u5YDUQhtM72q0JslBrjtCjpbXY9ZqudXbX23S7W2OC5sCZfXcvI65enyPEGZ84ZZrpllGvE7Pr9prRDX67Y+t/SU96W+3H+Cf/PQXVhwHf5TrFlrICIi2pnaEoh2ileA287AtBPcAjG3cd3Qq+12SiePxwwoTW7BqNv23cbp/iP+3k9+jhHX4RMMyLMTERHtMJs6EN1u7u5A9a24znaudzMwA0Gv49pMx+6+HywRJSIi8rPlAtFWS0Pb2XQgjLul6tt2EdfZ7nVvBuYxOYO8zXPs3ueWJaJERER+NmUg2ihYlDtHNZqftj4xICUiIqLtYVMGon7k94ua44IKM2+ndDKYCrPuMPO2KmgQKc4TZH5R2Pll8j6aJZ1htLoPbprZDyIioq1gUwaiZuekTvSabwevqmI/4jJmUBFmeTfiOuV190Kr6SLvvzxNnO43rRVu56kRt2WCpoH5Antx+PhYsO0SERFtdfzWvAt+a75zwgR4zfIKAju93U7oRnoRERH1Ss8D0c2IgSgRERFR523KqnkiIiIi2v4YiBIRERFRTzAQJSIiIqKeYCBKRERERD3BQJSIiIiIeoKBKBERERH1BANRIiIiIuoJBqJERERE1BMMRImIiIioJxiIEhEREVFPMBAlIiIiop5gIEpEREREPcFAlIiIiIh6goEoEREREfUEA1EiIiIi6gkGokRERETUEwxEiYiIiKgnGIgSERERUU8wECUiIiKinmAgSkREREQ9wUCUaBO4++675VHbzuuDg/IoYrp03E64toi2MgaiPRTmBhlmXmpNp9NaXv/dd9+Njz/+2DbO1GyQ0mi51wcHPefxm+alHfO7jfNi7qO4TJjlw/LanjwEmbYTNHPs8jJyesrzytzGAcDHH3/suOaIaPNgIEpbFn9cmvP64CAeW13FY6ur8iTfaa0w1+ul0XSRuI+Pra56BiBh+K3Db3vieHGamIZ+6RnmuBvxO4ZmNbNOOb3kNPPT7HJEtHUxECWSeJVOdoJfaWgr2hXcbDbtDNyC6Pb2yJsZlDYTnLJUlGjzYiAa0t13323d0LxubOI8Mq9p5niv6c3wW6ff+EZ/ey3rxZzPaxm/6V7b81um0XS39YncxnvtR9jpYXkFQa+7VF+KGo13W7bRtEbbbNbrHsFe0G2Jy/rtp994cXojbvvabm77Iv8tjvM7hiDT3Mb5LeenG+njplfbJaLWMBBtgvl07faUbY4PM00c7za9GX7rlKcF3Za8XLPLui0jTncbJy9rzicvIwqyTrd98eK1nLxOv+lhtufldZ+q4kbMH2tz2SDT5O2F3aa5vBuvaeI2w2zLaz/lYxDXaW7fnNYt7Tz2RsfQiXUG5XWcRERgINpeZsBhEgMPt2ndZm6zl9s2/y0HZJ3YJ7d1uo3rpE6c9+32oy4HRnLgEiZ48tKONHvdpURVHCfvdzPkdbTj2CEdf7vWGYR8PI34pae532HWR0SbHwPRTeLuBlW7YZnBnte6mt1Wu/ezkU5sr93rQ4f2049bUNRJcoDQDt0OKlrd/8dcSgbNcW7Tdrpmzq+cns2eKyLaOhiIbhJm1a04tMpcjxwc3S1VJcvLmPOa88nT5aGT5G21uj2/Y2+FvI/tXLfM/IHvZvAjBwhBt+sXjJjjuxFw9CLN/Pily3aw3Y+PiNqHgegOYAZG3SipI+q2TgexFE4ng9BOrZeIeoeBaBvJwZ5Youg2zU+j6SK3ed3GuXGbz9zXIKV6bsu7kY89yLrdBN1eEO1cl8xcd6PzLk8Pq1dBWLu3K5aKyiWkYmAjT2tGmOXl7YVZthlu2/MKvsLsSyfWuZW0cs8hos5iINpmZmDhduPzmiaOd5veDL91uu1HkGDIb52NuG2zkUbbE6cH5bYfYZZ3E2Y/gx67HzNYEQOKTgYQ4vbk7foJOp8bt2N0m+Y23Y3b+sQ0E6fL3JZtRaP1eG2v0XE3cwytrNOPuM5mlieinSOiaZomj9zp3nzzTezfv18eTT68gjkzCGtHALadbcc0cgtsqPvp0onteQWWjbbT7HKt2I7XFtF2wkDUBQNRIiIios5j1TwRERER9QQDUSIiIiLqCQaiRERERNQTDESJiIiIqCcYiBIRERFRTzAQJSIiIqKeYCBKRERERD3BQNTFpz71Kfz85z+XRxMRERFRG/GF9i4++OADvPvuu/jlL38pTyIiIiKiNok888wzDESbEPmXzyEy+Ax+9bfkKZ0SQSQij9PpzxIe0zUNGgBEIpAni88gEdeF7fM0pu9DuGXapQfbNtJU0zQ9/cy0bsjtXGmw77pznuDHJqeFsS5r/4KsW55H3j8/8rLC+l3yIVrIw4B4XAEI50xmuwb03bHW7Ti/LutxXEOO/ZKOL/BxOdNFPrfy377rduyXl/p2bedHWrfntt3SxMpHXvOGyWcmeV3O9NLJ6/Y4Hw04j0nnnYfl7fpxWz7Mul3m8UoXfZWu1wJRWBMTE/KowCKnTp1iLmzCJ68cw3/87VP49JA8ZQf75N/jw//4n+LeT98pT+m8Xm6biHrn//cxPvrol7j73r8JXv1EvfHVr35VHhVY5JVXXmEg2oSP/rd/gL/626/g14flKTvYv/+/sfZXfxMDn/+b8pTO6+W2iahn/v27a3jvk3vxmw98hoEoUY+MjIzIowKLfPe732Ug2oT3c/8dPnzsu1D+S3nKDnbjCv7tB/fh7zx4nzyl83q5bSLqoo/xszd/jL+0+pPuRv/f2Q9e+US984UvfEEeFVjk3/27f8dAtAk/zf6XeO/Jf4e//Zg8ZQf7vy+h/O6vI/H/+M/kKZ3Xy20TERHtYPv375dHBRa5evUqA9EmrE09iJ/9t1dx4IA8ZQe79hqKP30AqccfkKd0Xi+3TUREtIN97nOfk0cFFvnLv/xLBqJNWD3266h98S+R/K/lKTvY+nfxz94awP/43/TLUzqvl9smIiLawT796U/LowKLfPjhhwxEm3D5H96Lt37vQxz6b+UpO9jVb+PUX+zHsZEH5Smd18ttExER7WB33HGHPMrmK1/5Cl5++WV5NAAg8td//dcMRJtQ+R9+DX9x+K/x/0zIU3aw1f8T01cewtRhRZ7Seb3cdvn38WtH/4Xxx4P4J5eX8LX/QppH9taLGH74H+MqAOC/x7m//uf4b4TJ114YxkN/pE/Fl8/hr/93cWoD5d/Hr52I4yeXvoa2N1Qo/z5+7Sgc+9sToY7z/8Lv/9o/RjzIuQnt/8Lv/9ph4Mxf45/vyPuBfvzWFfC//gRL/68AZ+R/+DUcflX/t3MZcZ0BrylLJ8/1Nbz46EP4Zlre317Q96XybLB8d+2FYTy08k/C3UsCuvbCMB4qfDXgtdhtejr941XjT+l+arvXDv6T9h6D7T4P/PfSPUK8BuT92mr8PgD0la98xfq3WzC661Of+hQ4hB/u2AVE7nCO7/hw8wf4Z9PTmD5/1TntU5/CrX/zzzA9PY2zV6VpV89ienq6PsjLm+s1BsfyQYY7IkBkl3N8N4aebftfYfzov8CXzn2CTz75BJ98chn/6EF5HpfhwX+Ey598gk9W/wT7sQt3SNP7M5fxySef4Oo/2Q/susO5vN9wx67OpcUduwCX/e3JEOo478AuRLDLMb4dwx3YBWBXL+4Hm2D4V//jYfyL9LeN/P8JLmf6HfO4Dclv6vN/Ow1EdsnTkzj9ySf45JNv40uhz1snz/WnsCvitr+9GXZFQuQ7fWbn+HYMeqI4x2+SYVcE9Xv0N5O2aea99pNzX+rAMexCBF/Ct41r4/RB+3TzGmjqPr/JBi9iEOr2NwDsuuOOO8Ah/BCJAJFdzvGdHD68+E/x/NldiP0tAJFd0vQP8fr/9jzO7IqhHy779nAKf/zHf2wM/xMe++Db+KcXP6wve3YVsf/JmH6oH299+5/i9Q+d++A7GDcjx/huDL3adu0qfowvI33QZVqQYVcEEUSwSx5vDLv0jOYY7zvsigA+62xp6OS6ww6h9mUXIog4r4u2DLsQcbvmdsTwU1x9E/jy0aTLtGDDrggQ8czjzZy3ZpYJOhjn2nN/uzmEy3dN3UsCDrsiYa7Fbg8B0ynU/STg0OD+bg6dPDfdGty4BZ1wGb8rEomAQxMDoH/irlvDxmv4P67/Dp75n38HewHon2urDze+/3/g3Seewf9yQJ/qv2/7sPc+cR378Dv/8/+C39ln/B17GP24iXevy8s1Gpz71b2hR9u+WoEK/bN/9uHPMPqpGP70rfq4P/vqpxDLX3PMq58ueXlzaOa49PN/NR+znlbt272GP/1t4Wn2t/8U16R1XBOW/dSnPoXRkrBuYX/N+WL5a4iURr3X9dU/a7ztt/4UsU+N4s8if4ZRt+nyfv39Vxvkc/sAAJGrf4pYkHXbjjlinE+P/a4niv73W8Y2xHlKo7Z1u+WDpobSKD711T+z7bu87j/7qrDftjx5DX/62zH86VvisY3iz+Rt+A5XUfmJ2/1GP89iGtrzgTA0uF/5Xx/uQ6NzLZ8P+7mONDjf4v6a843iz1yOuT5PPd3t+cz9HiHO450PBzDlmvZeg34vEfODbd1mvjUH6Vz5Xx/2/TC3UZ9Huu5D57NWhyDpFGSe8EOw/KufG+f4rTO4efnllz0HEQPRJgdEoD91u0zryPDZA/jal2OIGNtGxJ659z35NaRjEdQnu6zDGlaw/Bd7EIvtc5kWdB0ug54ozvHdGLq87Wu5If1J8PdeBfAqvmg+GX71u8Y8ehqKNzZjhLQufZznDdCa7DLNa0AEWJnEF1dm8Ktf/Qq/+osZ4A9H8HXzhzD3DHDuV/q0X72FGUxiJFcPXq7lhtD/h3F851fmPL9CYURYt7G/4nw/mexDRBmGslLBFcf+1I/Bf9sw0nISw3/xK/zqV99BamUSz3zHWM93Ru379e0U9CR2SQPHAESgYvL3Kpj5VbB1v/p7Q0aaXcPXf+uLePNP3rLtd790riOIIPLW1zH04CTi3/4VfnV5An2RCCKR72L0997EzF/U0/Mnk30u+9jEgAjwyhfRb57rb6eg/uEz+K4x/VpuCF98cwZvGdt960+AyQdHremAiskHzfR+CzNDr2JSyAuew1tfx9Add+COO76IVwG8+ntmyYi4buke4pGX3a8Lc3BeR40H+VzL+ey7GP0/09a50M91fb8jke9i9A7xfIvnUj8ufX/F+QpIRPqgxIE3r8jpJxzDW1/HM/iOtd63/gSYPPJ1K8hFBFD/sN86n2/9iYJX/8ic7pIPh6Q09hsA4JUvYjKmL29f93cxauZb8/p45YsYMtPM9/ow1m1ci9/96h1WnjPvG9dyI5iM149bTy+XfezQ4MiLboNxFI7xLQ36lhvmX2s2l2lbZGgFA9FmB4S9ObZxAHxu3EaGcBl/Y/FFPPvss3j22XNYe/AADnzWOU8kEsHKt85h7d7H8bsPO6f5D/771dmhu9vum1qFpmnQ5lMAUihpmv73XNKaRz8NwnIe58Uxn21o5rgADOVQNfdl4AjGh1T8f/6l/qPSN1VAZsCctw9HxhSoK1eMv7+LP/5DFan5ApKO9RrrBnAl9xD+iz+Mo6QJ8w0MIm4ey3f+AXY9rP/IXVlRocQGA2xbX3lqftWYJ4n0KFC5ek3/Ef6jorRf4dIGUJBbM5d3rluZPllf98ECSqNGmn3njzG5kkJuygwe+5A5n4NyumgLunD163job00iPq+hcFDedj392ztI5/pgGilUcOWtSP1cPp+xgqi+qRJyQ0UUzQDclt7O8+E5DGSwqmnQtBJSAFLzRv4X84Mjv3ucL8d89sH/+nAf7OfaOK7T542gK4mCcJ3a0yyCa7lJFIdyKFnnWxoAIHIe/2DXF1GZrmJVmG8wphjHcg1ff3gX/sF3Ioi8dQUVxDE4oKdbQZi/7/C4/eEN9vNpm/6dP8YkpP1qkHb2AcBoydpfcd3mMZ+08m0ShfmUkWYNrg9z3Yjg/Fd24YuVHKrL9TxnTReulzDDd7+yC7t2ycND9SA4yBAonTzyZ4tDsPzbmW13c2jFLnkEbV97nzyGbDaLbDaLp/YtIvviIjakeTYunMLZqwM48rUDRhMA2vqiUOLCn+t5DAo3kL4pVZimogIFw74vHygiM6UCo2nYO+sqGB6qQF0HymeAVHwJ5prj+6P6P/y2DQBIIS18sjgxp2F10li2w6x9dDM0DN8kAVCcykCV9l+XQGEtB1gBRBpleZaOaXQud4bofvECAMpj4o9oEkVhmrqsAnEFPrkB6lQGRSgYP2SfK7o/ri+/fg5L8RRwpaZPsPJPDfmYsO2BjHWNmJSxw/Vt909gVStI11mHNDhm3+sDAFYyyJyW9t8QnVxFabRoBbKDM0a6BJCYMx9yxGEVE3xl9LbBQHSH2huLYc+H7+I9YdzGhVN46XXg8T9IIyaMp62uBrVi/pCUkR7IANNV66ZenRYilX4F9p9sNymUtBJSp5PSD0oUSlzFklpGoTKMk0eBwkINasUMhhpsu8cqZtAAWGlmWakH1QAAVfrbKBUsjRaRjOXh+JntnzBKEDVUpytIdi0YVbFk33EsrYh/7wy1KxUr0KrNDCJ5WqjFMEp1TUqscZ5UpquoTgOZAek8KsNQKirK52eBoycxvHwONXUJqrHt8lgfMsiham57LdfwAadrKqot39auiBdAg+sDRkmu8cCVXpCm2QLKEuJTfYGDUftDgzkMIr8uz0lbFQPRHWr5wkXcfPBhK+CsB6HHcGCfNDM1qR4E6D9+8vQuWTiOzIq9pE4soUzaSiUTSI+qyBxyCaZs6qV8jh+dKyowdhhRZRiVM8extBKHIpReeG/bj16qWzxj/Oyv5zF4UCzHakUUh8cUqFPH60GFkWa5ySgwkkYKRWSsH84a8k/rVZVySVViroocMugb8w4z5dI5nVlS1s4f2ATSo0Dx6fq5rM1k9CpYR6ltZ1jBy0LapfS7W8o4PqUidVQ4W0IJd3nMXiIaPTQO5XTSma8lVimfy4OHuhxHeiQKBbM4fqZiD26tksca8oecJaKelGEoK7M4Z+SP8lgfMm16qIgeGoeyksFx65iNNHt+AtFG14eofwKr8ykUD/rlYwXDQ/I4bz0vETVrcVzOM7UHA9EtYxkFo1r97FUAV8/q1eyvLhuTC0a1+1msAVg7q89bMCZvXDhlVctns1mcxRFkv2yGocv43us3AdzExW/U53GruqegEijMp1A02lz1LedQGq1Prc3obSb1qjmzysosXalX3/VNqcBpoz2bT3DjsCK00ToIlKzqvQROTivWfkUGljAulUom5jSU4vY2Xq4/ytaPTn3flJiC4tQshg9Fgf7DGK8UUbR+9Btv209iTi+FNZfNtbE0KTq5apRUmmlWQW6tnmYFoxRHT48+ZOIljyYDUUwsm/tpnM+FtL00x7bu+nKHxxSXEszWyOeybyqO0vKEo+q0/aKYOC80R3h6GCXbuS4jbexT8jSgGvNZpWRWmiVRhGq0YfULbmTmMvo6MK+hYATf0ckcUsL1kYnlbCWi6J/A6loOFbGdr0cQYj14mPvWryC+kkEGerOVxNE4iqdV6+Er8UwOipmHI31YGguRh/snUJqGdVyZWNV2T2mJ45iTqExXhTTzuz4kIwWjtNg8n1JzBN/rZxPqP4zxIZdaEWqbiKZp/LJSEy6NRaAe1TDWpZKFLWG5gOzKw0KA20W93DZROyykjR/4Lpb2EG17NeRjfVh6vv4w4mohjcjTw6i6PKiVxyJIVnKu03yt5zE4sIRcg3a+tZlB9C3noM35zbW53b59Wx4VGEtEiYh6yigdZBBKtLkYJfNNBaEUGANRoq1E6nUuD0E7AGwv9Wpe1yFMk4aeSKDQ7XZvjWzifObeecUcutUJbHOxmvq4DmGaNGxfVpMg6X5gpZ1bm/ORgt4mtaUgtP62ALmJk5mXe9d+enNg1XyTWDXvopfV473cNhER0Q7GqnkiIiIi2nIYiBIRERFRTzAQJSIiIqKeYCBK1KJ0BIjE4PqeQQCozQCRSH3oRmeKNICIMeTliQJxvkF5YhNqM4OOzgDtUpsZ9HyfY7eVx0J02FlId26/F9I7s4OOrTNVyONv8XzYOgaFzestbtvXZsoLoY6zjHTHOlTpHRnlTkK0uTAQ3WquL+KU+CJ7ifnievNF9hbrhffGIC2//KowzW15as46kJwCcmuApulDN94UVwCgAfYXdUvKAIrGfBqAVXkGos3K/GRqGz9qEFR0cnXTfZ6WaCtjILqFbFw4hey3gNiD8hQA2MDii1kUEcOAPAkAYmkh0HwKj2+cxakL9e8mxb4sBKJ/8DhunD2Fxeu2NZCHggZoy3B/vYcKqEPA4c3yWh6B2iBQJSIi6jQGolvF9UUUrx9A9msHsFeeBmDjQhHv/k4Wx550myrbi71+s+3bi/vkceRgq3Ifk6fqalfkMcHlhWrziFGCKWq1Wn1JHtFG4rse7dVi0uf+XKrv5HcielWrmdtIL3hVlevbqi8vvW9UrFZdz2MwkkZZnMe2b/b9Tp6uLxqUKhyXvK/2d2PK1ZTSfrukmclMO3H9QdPT1bqUCcV8bk6DML2Nb0+T9zsinzNfjdPM73zYty2fj9Z5b7t914ctL3hUldub0rRyfUj75fZOzkZUobmFtO52XR+u342XPsEr5wVY298kzR62IQaiW8W+Azjm847MvU8eQ9p7smQZl6/uQSzmEY0uX8bavTHE9skTSBSd1Kvaq9PyFKA8pgeofVMAVoA+I2AdnJHndFcDkBGqzTXYq/TzACrCtLgRmAYxaMQMRWPwCnSbdjqJTKyqvwh6PoXiwfoNvDZzHDiv6dM0/TvdSSlo6puKo6SZ87h/ls/65J4xXYkpUJf9XwpdHisgba1X/x68/Ue8iGQkg+E1Y/pKBseN6eUx/fvY5j6F/sb3SgbJ5ZyVJurUcSFNBq1j0at8gcyAmWY15GP6d7/FNOtzCcjEtLO+472eRzJAenoaADAvZLTTAOQ8HAGwZkxfAeARGIWynjeatAjpPZRDNdAnEAOkmXg+1nLAVLIe3KzncRz1c12dBjKHfIKbsHy23a7rw5EXlGEoDb6V3sr1gYW0fb/mw9a1qMgcXELO3Law7nZdH/rnNjOIz4svqC8jfbBi5TNNvHaoezRqyo9Gob08L4/tjsvFE9qJ4mV5tOGy9sqJE9orLpOv/6sXtBMnTuiDY/nr2oVTxjSP5Ru6/IrLerukh9uuTmsaRuWxuuq0pmFI06ryhAaqxl2xJE8wKMY8JnN+WUrTtJw80pAyhnaqTisaRsW9LmkpQEt5XCv2+QPMO5TTcqPQMJSzp+l8ylpPaRSaMl011qdouTVxxrr6fJqmreU0Rdq2NX0tpylI2c6FbdlG5lPS/or75XbMVS03ZIybT2mQtm3bH2N6blpxzmfNK68/oGlN04akcfPCuDUj04nrHjWWa5F5rq00c0sHTUoLk9u8cprZzoee3p7n020bjrwbUMhth7k+/PNCSUu5HL9fPg58fYj51eQ4Tj/O69R+DcvHHOL6MJefdu6/Tp/ulQYU3EcffdT0wBLRHWTvk8esdqBP7VtE9sVF1FuJ7sWBr9XbkH7++1lbG1LqriiAEoCkR9W7CqBPKM3sk6ZvHgqGh4Q/pU9H2j5tt66iAgXDfn1AVjLInAaUscP2NrnKMJSKihrKKCCFuFU6Godits+VquCc1esppIXSpcRct0pHGhzz0HCDDjlFZKZUYDTt7AjXP4HV+VT984Z+1ZZu4vIIF2KJ3ByASeHvJkX3x4GVWZwzSgrLZ4rux+elYZqJolBsxylVjw9kfEsSWyNtu9XrwzMvKBgeqkBdB8pngFS8Xjoa32/k8W17fQDFqQxUaf91CRTWcsBUn3HcrH7vBQaiO9TeWAx7PnwX78kTAAB7EXtoD25ed59K3ZGQqt7lYLQqVd1r0vTNQcXSivlDUkZ6IANY1WhSz+N+pXHcM5RD1fjhsFUb9iuIryxBXSigEjuJNAoor6uomD9S63kMHiwiNV+vggtdvd4xKpZskY6KpRXhT7lKVZWrWFMoGVWpbu3brO9laxpK8Qz6wgSjFenvFto8h6IMQ4GKzIAZFKVQClQtb2iYZqIa1Eo9ICuP9SGDelVwZ3vmi9tuw/XhmReiUOIqltQyCpVhnDwKFBZqUCvGtbmtrw8gNa+hNFpE0i3vm29g0DRUpytIMhjtOgaiO9TyhYu4+eDDHn0LlvG9129iYMh9KnXfsPR3yigt7RizdMStrVUItZkMikPjtrcGWCUw63kkxRIfJJAeVRu3x7NK+eROC4B6BRg/FIUSq6Dw3BLUuCKUnAolKwtplxIfD/0K4iiiILZZC7psQwmkR4Hi0/Vj1tMsh5MjAEbSSKGIjBVU1JB/ughl+qRU4lUv2fHqvALobWkDO+TS5nMKwPPC3x1Sfs4ekGlaIXhpaOA0MywcR2ZFKi2z8k0N+UMdLBF12XbL14dfXriiAmOHEVWGUTlzHEsrQo1Bs9eHUapbPGPcK4ygtj3adX0AiTmf9qOG6H73UJ+dlTpMruenYLrfRlRv+2m18ZTbel5+xTlNaOtpax/qaCNqbx/KNqLB5IbMN4MKg9RWtNk2orn6L7A1yFLSdLG9p7wsXNqK+rYRnU/phawh28BVpxV7Ia3UTsw+3WjPJm2jNGov6DXbdcntBs11ie3UrOlGmzax7ZdtvUZbU3sbOLldncBMDyNNqtNK8HZljvZy7m3ixHRxtu8TpovpJbeRk86b43w41t2A2Q7UHMT2n+a0TjDOn9e+O49LPrZGaea1nHPbevtCcx4jn8nbDnqdNNh2K9dHsLxg5jvpepHXG/b6sKV3Siut5TSlLW1E6397pZnvuXa0MTXnFdrList6tCnXty9vl0Ryu88wQ0TTNE0OTqmxS2MRqEc1jDnanOxgywVkVx5G1qd3f8f0cttE1FblsQgysaqtDWJ5LIIkStDCVNETUVfcvn1bHhUYq+aJiGgT0dtN2pVROB2yaQERbQkMRIkoEPlF2vbB2V5zR5B6OcuDa+ehna5hmgET58WezBFEIvq7IrvTS7tJDY9rJ+YF6WXz8uDTXpN2DlbNN4lV8y56WT3ey20TERHtYK1UzTMQbRIDUSIiIqLWAlFWzRMRERFRTzAQJSIiIqKeYCBKRERERD3BQJSIiIiIeoKBKBERERH1BANRIiIiIuoJBqJERERE1BMMRImIiIioJxiIEhEREVFPMBAlIiIiop5gIEpEREREPcFAlIiIiIh6goEoEREREfUEA1EiIiIi6gkGokRERETUEwxEiYiIiKgnGIgSERERUU8wECUiIiKinmAgSkREREQ9wUC0lz5cxA9fK+BteXw3/LSA1364iA/k8V3w9lIWP1zdkEcHI6fZh4v44WtZvPZaFq95paU5z9KyfXzYNAg7f9M2oP4wi6WfyuO3oCbS7IPVU3jttW1y/DDSwCtvbjItXZs9sYwl6/rfavve2NY7H27KmHs2gmdfLssTuuedPE657cMbaTz7bMQaTpVq9unkqTYziEgkgvSCPCU8BqI99Pa1i8C+38VvyhOaJQdpnjagvrOGe+4/gM/IkzY5R5rdewBfeCKLJx5+HHfaZ22zDqRZE0FaO7y9VP/hbibgMwNF3+Af2FZptiN8uIi/vD2AXx/cK4zc3A9Fby+dxe17juCJJ7J44oksvmDb9y3O9Xy05p3SIJ59NoK5N+QpW1kN5VMRPPtsGpfkSbJbKt4R/36kgBMnNJxIpsSxW0ZtBohE4BoMmtPMwRaCrwODwjS35buJgWjPLOODNt9kAvtwGTfxOH7zN+QJm12TaWYGq8MxeUpwWzbNnH5zWP/RfsII4H9ePQX1Q3kuDz8tYPn6fYgay8f23UDNKzBsMs0+M3gMTzyRxXDI5ag1H7y33N4H447bwH/4OXDPfS1c15vY1jsfm9j9Ezh2QsOJYxO4X562ReVjQBKAawi9APRNASUN0DSgNAokY4BZ3ps/BIyv6dO0eaB4EMivS+toIDq5Ck3TUBiRp4QX0TRNk0dSY5fGIlCPahhr8iR8sHoKyz8/4AiO3l7Konbb/GsPPvvwMSj3wqiCuozPPPx5/OXli/gFANz5OGJfOIDPYBlLr52FtZhFXL7u7aUs/vKup5ylBz8t4LXqmvXnnfuEeT5cxA/N7QLAPUesff9g9RSuIoVf//lL1r7f0ycGEhtQf/gS3rcWltYt77+wbpFXmgHm/r2LX38ibbtxi+lp36ZxvO98HtHdy6hdv+k+j8GZZhtQf1gE9h/Ax5fNfR9A1Nj+B6unsHwrZpwfnTVuP3BVTEuLubyeXh/ffwSoOtcN+J8P/LSA1248jNhdi1hucFx62i/ibpd84qTv183d8rlzX96ZZnDJC+JxiflAyrvy8VrE5aV1e+QjVw3TTFq3de3VfbB6yloW4jXw0wJeq8KeN67ftK3fvmyQYxeO23bdOq95z/2ycZ5H+72oztpv4/qx5Wdbmvulmd/9zOB3PwKs/AK345HTTNwv617xMD4w85vtXnoZd++7gfev38Sd+45gz62zeP8XYrr65TPz2s3iMzfM9LOfk2bPB97J49RsBnseyeHmGxncAgCkkDhRwKPGtFsPlHDiKwljgRrKp/pw6VYKifFh/HDWXEZkLI/6+q15ducwbgZub6TxbKmIaFKD8pMIytekZX2XL2Pu2SRqD+Tw6K0MLt0CAAWPjq8iESAqvPRyBOVr9W3pf+vLP/zjQcy+ocqLAFY6GNs2x4vHJDKOb/cjVRxLRoUJ0vIh9vud0iBm34jj0UcquGTuo7lfrtsztmXbxzLSkSSKoyVoc+Z51Us7j+8HCiNAOgJgXv+3KR8DZseA1UljxDowOKAHnxP99flMbuvwZuwTAEBBbm0VE/3A7dsuN4ugNGrKj0ahvTwvjw3qsvbn339BW/3APvbGT17Qvv+DC9oN8e/vv6Jd0zRjmRPC39e11R+c0H7wk+v1FXxwQfuBNd2D1zxvv6J932WfdPq2//xt+9/mtvX9lP4WjuPan5/Qvv/nl82FtWt/bt/va38u7o+8LXG81/75HJdB3qammccs7NsHF7QfuG3Ddd16+otpZjtOx7pcztfbr9jSqU5et7ysnEb28+E4rrdfEfKNxHMf3Oj7YT83buO80szIG0JecOdzrn32157P5DRrQE4z6fzd+MkrvufSfq1KhPR3ne+DC9qfy+uyjlHalnydyvlMPtce50HmfV48zq8mpJm5/lBpJt/P3PK0Rx744IL2g++fMLYtDn7rErZtLW+uX5zf2K8/v2wd35+/bb9/+Ocz89r1mL+V8/GznPZCFlo2q2iln2mappW0l7PQst8saZpW1UovQMtmU9qPpPlf+E61vorvKFo2C+3lS9YoY4Kx7m+W3P++lNKyWWgvvKDoy5p/m+uW57cx9tParrjfjf3om/bj0v8200BzP3YHY5sv5DRrMZF8PJpWX6/XMg2YaW0tb2zDlgbium3TTSUtBWgY9U6rFDQtJcUiKWhabs3+N1zmM7mto7GSloJibeejjz5qemDVfC/89DJu3xnDPlsJ0jLevn7T1p7uM4MpfPbONXxgtc/ag88+bJb+7MW+3Xvwi5+/Z04M5IP3lvGLex6Wqnv0tnx37ks5SrUA4IPVRdy+U6xijWG4bwC/uLVcr5K954hVWvGZz8Vw5y/exV+h3s4p6lMy9ZvDYilmDJ+5B/j5X0kN9F3TrA3ufBwxc9/ujWHPnTdx8z37tt3TTHdPX73E4jfvGwB+vqGnyb0H8Ov3COv6cBk3fxGuWUF93fZzHeh8iMf1Gw/jHtzAf7Cq3/V2f6+9lsVr1TBtOPX9uP1OvSr+g9WiraTb5JdmuH3Zp11ps/RmG/V8thfK/VKaNOKSFz7+K/3PzwymhWtDvvaMa7fPXhov+w+rp4xmDdJ89x7AsJAv7NfPMm7+Yg/2fM6Y/hsP4x7U90uvvhWu29/4XemeAQDy37INXL91s8kqbuGeFCrNIN3P3K5757UICE1tnjiCe4wSRb2piVniHOD6sF1f8rb34LMPGGlhWw+C5zPhfmi7LwAtn4/dj5SMEjkF9+0GcK2AS4gi8XdTAIpQjfafly5kcAspfMFWwufunR/P4hYUPPqkUep2/wS+8IC57rpbu3MYewTAI2lEAdy6oZf06dsSlnfzQElf1iS319ysbmWw2HSbWgWPHjFKNz83jN0Aaj8pA0jgwCMKcGsWl9+BXnr9gyKwO4cDYhohgYKm2UpDw8jH9DagmNer5ytX5DmA8hhQHAJOBioN7QwGol3n14FjD+7+tDyuAdsNrpFlvH0d9Zus5T18/Avgrk/7BEl37XXZ3zb5acHWecZZHeiXZu20F3/jLnmcV5p5MAMI4wfI/IFqe3uvls7HXihfMH+8n8Ld7wTvmfuZwRQ+i4tYNs7VVRzAZ++U8613mn1m8Bii96yh1u5ezh9u4Oeor9cMsm2kfBak05MVnNjezpC1Va3q25bTQLaG96/fBFyDc+HB4LUsXhOrlO/di7vEgOynl3EbA/iMERz91c9v4hfXXxKOy94EBvcewBf6BnC76nPMP/0e3m+iLS8AwPZwqOcrq5rZL80aimH44ccB69j8OsW5aHh91NMQRrtp96YrkiD5DFK71d9I4wmzyUFbz0cUe3cLfxrBoR7olKFeA/BAul517uNdI6BsZPd9ivGvBMZOaEIzAACIY2+AKuutI4rEkZwePJbMnvWDKDcbPd+vYI/452+NYzdUrP24BrxzDmu3gN39h53NBpqUGQCWntfbgRZGALUCxPfb56nNAMnTQGkZaPy40jkMRLvNKBkTb4J19dIEnR4g+mp4wxV4lip+Dnc36nIuBbwf/NUN4a8WfLiIH1bXhFKNLKL3yPP4pVk76Z0fbAG5Z5p5uPPzsOKR33gY9/xiGdc/3MD1W6iXarVD286HW0mVHzGIzeILg8DHv7gPf0NMnwZpVu8sdQR3XX+pfcEoBqxOVNYgtjn8jbT3NA96XljG0uWLwL6nrGVj+4SflHv3wvH84jCA6BNHcM/ts47jfXvpJbyPxxEz98v2Bgj92rSCzeqao+T1TmG/zMHW5lA47uhdF7EsBT9v31jDnbtjDdMinAZpFoRV8ml0igsTjLbt+nDTIJ810rbzUcPGLQC7h/F5oF7Kdq2AS28UUGtUQunLWHcoFWw0G6RtVmYnpxMaxh9RAKi4dMHjFVDyq6Fk76i4KQbzRqnzrfVzuPTj2cCl10EMDwHKtL3N59IKMGw+RxhBaN8UkFsDms0l7cJAtMscrx+y6NVD9mpPuYpJ8OEirl6Xqm/u3Yu7PKt9/EoVjSrXqvuNXq8qvIi3rfU6mxF4kvbpg9VTLiWeQmnSTwuO6d5p1mY//R7etwW8fmkmW8ZSVf4BieE39wHvXyni5l0HnM0ePv153PmLZVwP2mPd0NL5cDCWlasBzdJD+d2rNmZnETEwCpNmAR6AZF5pdm8Me+5cQ813f4PTmxzYH36sBxTj2quL4TP33MT7V1xKt2zqpXyO1yFZD5QbUK8IJaJG6ZgVpEpB5m/eN4BfXC8GfuvBp++SgsGGrwjSawlu32guXb3TLJzPfPo+eZSn9l4fkjbns/DnQ/DGcVySStH0UrYiyqUisHscD0vFa/d/Ng4AuPm+/X2Zjz6Zw24xyDJL6B45GahE9dGHUu5BWtsYQe4baaOjlMgsGe5sIKynrdOln+jdduRmDDKz+cLAb9WDzUcfSgG3Mii/oXqUXpeRjkQQGQuXrofHAHWq3hO+NqNXvx82OiqJQahb56VuY6/5JjXXa96lJ6TE3lPVq0exzrW3pVcPWo9e5SK5N6dfr3lHj1+xN7u8LXGf7jmC2F2LuIqUtbztmO98HJ+96yJuWr2t/dNM3medmW7ONIPc69dWrebWM90rzeTe3269euvpdpfbuXLsv9xrvr6Mexq7nw+rN7Oth7KZhs799s1Hcs9zr/xlCplmjt7+jmpO5zbc0wyu63c9J24c27bnBXmbn913A+9Lb3CQe5l79Zp3pK10Lu/Z9zh+ft1MQ+cx2dYN/313Xh8ux+X1JgqL/Tpy9Jr3KA30TzPndW17y4LjmJz5oP4g5JJ//a4P3zwq7Ndf1Y/vr2xvgHCek/r6ndeuqKXzIfdKB1x6eZs95YFoUrO3yTTovc7Nv4Se70ZPbpNt3a69vCXS8vV1m73mpZ7sXj3YHcSe6ykkkkC5VJF6r0u9281tuaSZOF3v2e5slqAfpyr1mPfodW8et+2NBWaveXHdbj3uzf12m6ZPd+s1jwUgclCcT5cSer6bwSYAYAioCtXv6QiMXu8CaR5PC2lEDspLK/joo38rjQuOgWiTmglEfW8yDTlv3GG4v0pn82stzVrjn2b+PzgW3x+97cc/zTaxBkFVz7jtlxzYNq21ewq1W4PzYQRV8AsGzUDzlkvARF2lB6LwCDBNxoPDbnsQuxW18vomVs130WcGj/UkoEKYxvibzNZOM72atSvNCjaJ1tOMRG5tG9++sWZvi9y0GIaf8Ah6qAfacD6Mquvo32UQuhW8U0ri0q1W2vJuDwxEidrO7AX9Et6/q/4aF6Kw5LcUvPZaFrWfO1+mTzuc+S11o/rcrUp+MzI/Oeo+tNBDfbMzOjjNvqEimvQrMd0ZWDXfpGaq5omIiIi2G1bNExEREdGWw0CUiIiIiHqCgSgRERER9QQDUSIiIiLqCQaiRERERNQTDESJiIiIqCcYiBIRERFRTzAQJSIiIqKeYCBKRERERD3BQJSIiIiIeoKBKBERERH1BANRIiIiIuoJBqJERERE1BMMRImIiIioJxiIEhEREVFPMBAlIiIiop5gIEpEREREPcFAlELYwOKLp7B4XR5PREREFB4DUSIiIiLqCQaiRERERNQTDESJiIiIqCcimqZp8khq7NJYBOpRDWMj8pTe2bhwCi+9ftP4awBHsmnEhOnLr2Zx9qrxx72P46mvHcBeYXpjG1h8sQh86RgO7JOnERER0U50+/ZteVRgLBHdLpYLeOn1+3Akm0U2m8VTj93A2RcXsWFM3rhwCmc3HsdTxvQjey/ipVeXpZUQERERdQ8D0W1hA4vfX8Oex37XKgHd++QBDHy4jOXrALCM771+EwO/Uy8BjT35OPZcvQyGokRERNQrDES3kfv2iRXtn8Pn772Jd983/96Dz39WmLxvL+7DDWzwVUxERETUIwxEt4W9iD20B2vfF6vii7j4oTk9hocfvImLF+rln8uvnsWa9RcRERFR9zEQ3Sb2PpnC47iIl4w2oEUcwOP31ktBY18+goGrZ5E1pl8eOoIB3Ie97HREREREPcJe803ajL3m7ZZRyF7Gw1LPecv1RZz6FpAK1XOeveaJiIjIjr3mSbKMQvYscMQnCP3GMmJfChOEEhEREbUXS0SbtOlKRJcLyJ41W33uweN/YC+1bPSO0WBYIkpERER2rZSIMhBt0qYLRLuCgSgRERHZtRKIsmqeiIiIiHqCgSgRERER9QQDUSIiIiLqCQaiRERERNQTDESJiIiIqCcYiBIRERFRTzAQpa1lHRiMAGV5fAO1GSASqQ/hli8jjYj13yBq8gybXB6DSHsccxnpyCDy6/L45g0CiBiD+zZ15jwRAHl5Yies5zEYy2+5s0dEtJ0xEN1qri/iVDaL7KvL8hT9pfbGt+Sz2VNYvG6fvHHhlDC9AJc1bE/rQHIKyK0BmqYPCXkeH2UkUUQJmvHfKqLyLFtUDflYEpXpEib65WnNWwWgAVDkCYI0gJQxnwZgQp6hE/onkItn0DfmFx4TEVE3MRDdQjYunEL2W0DsQXmKEaCevYHH/0APNJ96DLj4DSHYXC7gpdfvwxEjED3y4BrOvriIDftaticVUIeAw00FWzWoAFKhQtetoTaTRAY5lCa7H1hXjGC02xJzVeQqSaQX5ClERNQLDES3iuuLKF4/gOzX3L8Pv3zhIm4+eMD64tHeJw9gAGu4vAz9i0jfX8Oex37X+qxn7MnHsefDZSxLpaab0oJQrT4AqNLkRtXutSvSiFBULMmjLPYq+wjEat8a8oggDaAsVOrrVdBpaV5ziUFErL3Xlw+/bpO4ZxlHmgFlHJ8CcucnXMt301LVubyvrVarO/enRet5DEYiGJyp72l5LIJIRG6SEMXE8ykUn3amPwDUZgYRaXNTBSIi8sZAdKvYdwDHvuz1dfgNbGwAA0Pm9A0svngWawBuXN8A8B7e/XAPYjEzhF1G4RsXcRM38e77wmo2o3Vg8KBQrb4mVfkuAH1TQMmocq9OA8mYHjiVx/TAtG8KwArQZwSqgzPiCrzkMYgIIkiiCKBoBXVmYFNDHklUUDUq7KvIIYM+KewpIoKMMU8JKjIoAxiGgiXfYKyG44DVGEBfd1IKndzXDWPfK8hZy+ec1eQLBRSHxl1LictGiaVZba4BtmB1EEBOmJZp0BbUVBOCVwBISn+3pH8Cq/MpqFNJPYhcSCN5OoWSVnCWZY+kkVqZxTkGm0REPcdAdJtZfjWLbPYlLD/0FJ56bA9uXn9PmLqBxRezyGbPAkeyOPKgGahuXuXnAEzDsw1j+QyQmq+3+YxOAqkV4Nw6kJirB6cYAqpGsLo6Ka3E1QRWjRAvBSBlBXVmYHMcGaSQs0K0KCaQg4KCFJSVrDalCaQAqKhBQdyaXi8dVaFCMULGKApCu8koDkOB6ghd3dYNlJEBUPJtd1m7UgHiimtpKIwSS9cSQ+P/4rpzAArC316iQvAKACXp75aNFFCdBjLP5ZF/uojUvEsQCgBQMDykYklOTgDRyVVo2qpnfiMiovZiILqNrJ3NYnHfU8hmszj25F68d/0m9uz7nDH1Ji5+4yW8+zt6G9F0TC9FvW+fW0X/1qFWgOJBe9V8UZ6pY4adJY0Se9vSAjRMIAoFw6hANUoSU0LpaNwKDc0SWf2/PkcQ6rXuYNRl5/pMCSO47DNKK8W2nKoxiFXzGWF6r0UnS8hVMsjESyiMyFNNUShxoHLFLdQmIqJuYiC6LezF3r0AHjyCY0+agaUYaH4On78X2PPYU0hbtft6df3nP1tfy1aVmq/3hjeH7pRoydXr8t9eolCgYgllFDCMkwAKqEGFgmHAaHuaAaxqfw3VhiFvOErMf30TQmml3LFIkarttYAlot1QHuvD7FgJuUrS1l7Urga1AsT3Bw3biYioUxiIbhOxoQHg6lkUzG7yy9/DxQ8H8HAMAPYi9tAe3Hy9aL3SaePCItbujSFmdG7arJQYoM7Vq4TTUmelw2N6iWiQNortlUYKRWSsPashjyIUnPSoDnajAjiMKIZRwXEsIW4LN8XS0WTAEBcAFChQcc7YMz2olZeO7o8DFb0qv5F6MwK9tFRtsoNSYAtpRCIRRMK+ZsloF5qbTGDifA4w24s6qFhaUTDsEouzsxIRUXcxEN0yllEwXr109iqAq2f194Ga7xONpZE9MoC1s8Z7Qs8CR7Jpq5f83iePGa900qe/9JMYnvLogb+ZRCeNamKj2n14TX//pDi9Og0khar5iNFZqbMSKKCEOPqsyvOM0GazEQUKipjFMKIADmMcRRStqv4ETkIROkgtYTxEiWgUJaPjlN5jftho52ozkkZqJYPjLq8xknvMV6QST7ODkjhPva9/fZwqdEgKFVKOpPX9DRgoA0bwelBoF9o/gdyoisyAS1Dp01GLiIi6K6JpWtv6Cuwkl8YiUI9qGPNsh7YdbWDxxSLwpWPWa6Jo66rNDKJvbhzV5eBtS7ujhnysD5l4Cdpc8PLlYPR1Lz2v+bQhJSKiMG7fvi2PCowlokQ7VHRSLzlNeral7D69arxTQajehtS/IxMREXUTA1GiHSuKieUS4p5tKbtPf32S1pEgFOt5ZCo5VDuxbiIiagoDUaIdLYHCTnlvZv8EVjddMwQiop2NgShtDdcXcerFRWzu1+8TERFRGAxEiYiIiKgnGIgSERERUU8wECVqSRnpSASRmP69eOq1LpyPhbRz/et5DMrjROt5DEYi+ov6IxGkXd7f2inlsfp2Q38koF3c0qxFgy7vsZWJ77X1m68p5ocXIpFQH0Eoj3X3/BNtdgxEt5rrizglvshetFzQX2afzSKbPWV9Rck2y6ve02jnKI9FfD6BSZ7W8xg8WEHuvNTpqX8CuXgGfb6BXgolTYOmtf8dprUZ/WMObgFOYk7fZnU6+EcR2sorzVq0anxcwe+oktA/iGF+irZ970soI32wiNS8nrZaiA5/iWdyqBwMHrgSbXcMRLeQjQunkP0WEHtQnmIEqGdv4PE/0ANR/StKBdTDVf3LTJf3PY49tgWpNQkUNA0ae2NvEp08HzXkD2WA6ZJr0JGYqyJXSboGg52Uj+kBl+PrWZuCf5p1mv4R3Q5YV1FBCulmHij6J7A6H0fmUHtLiIm2KgaiW8X1RRSvH0DW47Ocyxcu4uaDB6wvHu198gAGsIbLRiS6/OoiPv8HWaTNb35SIPoL1t2rVG3THCVhRhWxbbCXgtjXnQ5XbShV9dq276gGLSNtbNvcZvI0oE71uS/fKcY+iyWxerVx8GPv5PloWNW6cBwZ5FCa9Apxo5h4PoXi080FGGHTAkZJ6NLzwOqkPKWd9LRrKsBukGby52TldBOn5aVpjcjrCqo2M4jBmZqtSYPj2NUlqNKoUEZOIgf3T+wS7TQMRLeKfQdw7MteUeQGNjaAgSFz+gYWXzyLNQA3rusvPIp9mZ/lDG09j+RU3KpOlatUzZevu1d5GiVzxlCdVqCIpUILafQJ665OV5AM3IaujPRABnGrWrCE1OlkoKp2c59Lo4AyXbX2ryMvkJf1T2B1PgXVfIH+QhrJ0ymUNOP78I108nyY1cdrxjzzcWQG7EFh+UwRythh/5LWkTRSK7M4JwexHRKdRNur+dvJL83KACpCtbkG2OYblKrVMwHbeJaNwLXP+LvP+HtQms+POtWHTEy/PqrTivVwYT3sHCwCKCLp+eDTSBSHxxQUz4Rdjmj7YSC6zehtQF/C8kNP4anH9uDm9ffkWSiUIgqtllqs55GcG7eVCpXPFJGarwdg0clc4ACmNpNBcSiHk1YAkkBhPgV17lzAQLZ5comkZ4mRl5ECqtNA5rk88k/b0yCYzpyP2vlZe/XxyEnkhsRt1aBWgPh+t5BKpGB4SMVSE8VlelvOsOmxmTVOM9Wj5NIcNyGMywEoCH97SRiBa9X4u2r8vSrN52u0hFUjf0QPjUNZ0UtArS9/zadsbX6beZCL7o8DFdX1+Il2Egai28ja2SwW9z2FbDaLY0/uxXvXb2LPvs/Js1FQRgle8aARcAUusRTVkD80i3FbRw39B9pabySCSCSJom25BuKKaylTp1k/xNIQplQuOllCrpIJ/833jp0PQF1W7U0VIn3IrAgzQMWS7W8vUShxoHIl/J55kpthhKy+b1a9alrPm/X8GnT7/mmWMIJLs8QyLUxTjUGsms8I07cFZdgKcIl2Mgai28Je7N0L4MEjOPak2YJUr66/b59bi1IKbKRgBVuleAZ9IYOf2kwSs2PuHTXqPW7NIXjPW7kkpXalIvzVOS2XiAIoj/VhdqyEXCVYcwKbDp4PW1MFR4CtYHjIPr+7xqWAofVPYNW2X90pNTV722taCSlbfg26/cZpNiFUvVekYFSRqu21gCWiW4a6BHVo2LfXP9FOwEB0m4gNDQBXz6JgdpNf/h4ufjiAh72alVJoSizkT8ZCGn1SFbDOaB92MGjJkp1eVSh2dCjj+JSK1PNCKZ9V0lJDPuYsbVViSlNV+S2XiBrtQnOTCUyczwFme1GBWRLXKLht3/kAEkeFtquugpZ0qlhaUTAcctfQZGelzS1omuniwr8TRolo2A5KYQTNZ00xO775tB2tXan0rGaDaDNhILpl6K9fymazOHsVwNWz+vtCzfeJxtLIHhnA2lnjPaJngSPZNKzuSxdO6eO/cRE3cRMXv8H3iTYil/71TcVRsl4LVEM+Zo5XgdNJ6YdHf88gVjLoE9ZhlgBGJ1f1DkpiyWLQ0r3+Cayu5VCxqkqTqExX68HgSAGlUbMjRR+WntdLtETRyRJyEPbN5wezbRbSiBwU2oX2TyA3qiIz4NJD3UUnzwdGCkYHJeF8SEGhHqwe9w8UFwooDo3jsEuJa0cs6O8PjURgVJ97v0+0F/zSTO4xX5FKPM0OSuI85npqwjjVeF+oOL3nRtL6NefZBrSGc3MqUkeDlS0TbWcRTdM0eSQ1dmksAvWohrGgJUHbwgYWXywCX+pBD/zrizj1LSDl8foqos6rIR/rw+xY1erI4jZ96XmXEuL1PAYHlpALXK3dfrWZQfQt55rqWNO8Rmm2XenHnYmX3NN7IY3I08OoduR9t0Tdd/v2bXlUYCwRJSIKJOrZnABG29fQHbC2Pf802470knufILRDX5oi2qoYiBJtKm4vXheGblShkzevr+Ks55Gp5FB1Czws9fdOdrPq3GwL2TfVo/7ZXmm2TVntqD3yQvm5DOLzITomEm1zrJpvEqvm5Wkdxqp5IiKiTamVqnkGokRERETUtFYCUVbNExEREVFPMBBt0qWxCOa62M6r15ZfzdbfUdpVG1h8ka+ZIiIi2o4YiBIRERFRTzAQJSIiIqKeYCDaLb9cRPHuCJ6NRPDsf/USbsjTiXqsNjPYvddDLaR9vyRVHhO+etRODbbbjEGXL/+4Eb8QFOTTlcHOh/G6rzYf02ZnfmWrk6/BCpb+24f+mq9gXzlrThnpNq+/NjPoe58oj3U2jzSrLNwLBuWJAvHrY37ztVvn84IdA9Fu+dQB/Ffml0X+7XEs/ht5hsa8M0f984au75o0v3sc8VqeNpX1PAa31TfHN4kOvUh81fgcpd/n5dMAUsZ8GoAJeQbaXBbS+idkPd4F6icdASIx/TOkZKohH0uiMl0K//7UdWAwAkTG5AlAdDKH+FSfZ7CZeCaHysHN95uXMO4DJXmCoAz9s73mPWNVnmEbYSDaRdGpk7gfAHAL6teL+Lk8gye91KMQy7n+2JlfdNE0DZpWQup0UnhKLCN9EChpGjRNQ3UayAwwyKGdpob8oQzQzA9hG1SMYLT9EihoGrQd9qlI86XxnfmKVRnpg0Wk5kN+jnUBiESA4Wl5wtaQmNOgaZ150X5tJokMciiF/MxrbQaIHALGR+UppgQKazlUDnr8pm3hjymoxsNrL3QyL7hhINpNn07hb//Du/R/nz+JxYBPaeWxDIbXNBQOyVOML7qcVpB7xrxlJnByWoE6d8648BIoCN+3jh4ah4IK1IDb3pGMEuT6U7ZR4tzx6k+jmnUgA1X4Co9cim1+Kcc5Td/P9II4j1waIJWeu5S8iuv3KmlwZSt5dy5rVqdGIhFEDhbtE6X9Sp4WpswMYnAmb3xxKo2yuZ2xsrHcIPIz9XHm/tu2v3Dc94dQrAKLwFmaFbZaXdbKN428zoctPeVaEKCen7zmkc6XXxVnGPr+CvlqPY/BgOuvzQw6rjP7OPGY5Lyts6WLsB/uzT3q14w1ZiaD4mj4T7WmnwaqGnBYntAyl/uP4x7VgN+5Ns6PnF46+X6hD/bt2vOZM43LOD6F8DUR60ByGdCW/Wsb0D+B3GgRGcd2DSMnkUMGx4OmVUA16b4gP2jmpelhLckjGnLmZZ3UJMKvhtQ3L8jTfeYLS6Om/GgU2svz8tgAlv9IywFaFtBO/sML8lR/azlNgaLl1oRx8ykNQzmtKv4NaEBKKwmzWdzWEcDl4gntlcvy2G64rl049YJ24X15fGdVpxUrDavTij2NO20tpyke50/eF3E/Na2q5Yb0mhxlWp+jNAoNo+aajOnW33b6uurLytvyV9JS4nrnU/Y86Pa3sG77fup/2/YDipZbM/Z/KKdVrTSSx+nLVacVz/XJSpqmKfJIgaJpWk74G8YyMkUaXzXmdRuCkM+HIw3F+VzOaWlUnLekpQAtZd2zSlqqiftAMGI+0//tlfYOLvcn93Pnsf9SGol52D2d9P2rp4v8d3jVaU3DkH7+20c/f3o6yOeyEY+0kvncdyzzKSkN5fPrsm/yb1QTSqOahlF5rKDBNtzPfWvk611Ukq7znMc9xu3eo7jcL7zuObKG14p8fXncU9zzgnSu51PGfVn/86OPPmp6YIlot90/iE8b//zle++FqJ5vwHxSOQiUtBJSrqWeevWkOprrWpH7VhWdXEVptIjMTD7k07zHt+LbUppaxvEpFann6/sSnSwhN1REQXwKHi1h1Sj5SxxNARVV3/bCcWRWUv7t3oRlo4fGoawsBSzNS6AgrnckLeTBGvJP+1R1rueROd1ov+p5Vjx+U31cCjlHqWcNagWI75fH16kupaAQxoltOnMACsLfXqJC+y4Y7cHEvwMRzoeeptK59pGYE9M7gfQoULkiHqWK2fNuR92qKCaWS0idziA/418S7dA/gdyouF9lFE67nVN35TP2fBadzCG1Motz60B0f7w+n1U6qmJpRcGwVeQm/71Z6FXQmDqOfFMltu0410YzL/E6XT+HWdv51WvkimfqZWS1KxUgrjiu2bZShn3vVdH98fp9sI287gMFqf3nhM89Rma2OU9J7cp97o4WJWZmXKF0dF1FBXEo/UDt/Ky9edLISefvh5f1c5hdUTB+SLwfqVjySvQQGIh2We3rx/EeAGA3lH+UglFR35qVDPoGlpDTNGhaAQkh44nKY33IIIeq3w8+WRJzJcSnMiEb2Btt9uShbW34Gv9Ipo6KAWHB2nbtSgUYGvav5mqBvclAEnLle++oWFqRx9UljOCyz6WKTTUGsYotI0zvLgXDQ/I4H1J1rNjcoR7Y9LWves0mgcJ8HJmp8J3DEkdT9aZFCwUUR9OBfoTNB47iQY98qAxDqaiooYwCUogvm7+gwr1yXUXFnN/BWU3trAbtoP4JlKYryITuRNWec10eSwLyw6S6BHUlgz4hTfqm7JGJaqWzC0dVb3P7hn4FcdfCF0ODQLUZq0ZnIrfmPBUAyRar5psR3R/X03v9HJbiKcB88DTu++qyCtXKBxFEIn3I+NwbbfoVxMUHmoUCikghHeqByB0D0W765SL+v2Y7lr9zEgf+njxDE5RhKFCQWxNuEOoSVCngKI9FkDydQqltAdF2p/fyxHzJt1emUydLRKEHVba7qX+QJRJLhNqtNjOo5y8r+C71rKG9U+MAbkIoeZA7FinCNHPwKgnprBCldet5DB4sIjVffxgqyR0++iewakyrTleQbDYIcLOex+BBoDQfD985ciRtlWKWzxTtD1YBiMesD0ani34F8ZUlqAsFVGInkUYB5XUVFfFe2a/A+yqJYmLZvu5wpZItWkijb24cpekKkmHvJ62e64U0kvAohR3KoSo/eAuBcr2UzoWwX/rgUWvSiEfhi8XlN7EdzPtByXiQFc+JWANiDh3/7TUetsrnZ4GjJzG8fA41dQmqUCKtTFel6yNoPtbvo1Yg20yHPg8MRLvoxj//X7H2cwC4C9Gnx3GfPEMz+g9jfEgVegXq1aDK2GEr41lBaLMX+Q5k9vI8OZJAYT6FolevTIc2lIj2K4i7VsHq1avFp+s/QrWZDIpDOZwMciMZSSO1kkHSq1F/EGYJhtsPoXCjL4+JJaJRKHHUq+uMIMkiHa8e1NYnt07fvr1a2psYiCSMEtFmOigF5dq5yoV+rsdx2OvH1kEIWhfSvmnq+pDid659mW8oOInESAGl0SKSckcpq3TRrcNRAulRFUtqGYVKwLwNAIji8JjS8FpVrwDjh6JQYhUUnrP/SOs/tvLDXhvZOtqFYfTkf34CickScnC5jgOu2/Vc+zEfKtxKYQPcUzpVLW7TIND0bB4QMM0akbc7bpSIdkyDa1NdjiM9EoWCWRw/U7EeBhJHU1Cnki7XXABGh0/xoSNYABuArS0qBRa+s9Kq9u0H9E5K2Qf+yLNRtRuz04J9EBuf1zupQOzcoJmNjuVlvTuseNlJnZXqnWOsMUb6yo23O8jqdCafa6NjjzVN3KcgHS30zgRuyzsa9Ls1WDfzk6NjgH29ynRO6iAhTk9ppbWcprh2stPzZnVasXdWEjq+pObFfXMb53IsXo3yNU1LSU8NcucBzfFUUe84UHWZJk4Xl5fHmczzKZ83x3VvS3P7NS+mncmWT4ZyWk7syGDLX3DkMV0z+V7oPGaOMvOMSycX9+3Wl3F0vHDst3Md3ukm7ZvHNsJ10qvLDWkaIA2OTkvGdRBq/cYyjvws7buZNvK93ZFmDdIL4jlvnM/kax+OvOzSgSmglJyecO+05N5Jx+R3X2zmfLhf9/I1kpOmi/cV+Z4jTzfnSUnjLI3uw+b5Mc69o/OY67luLi+Y65Y7IIUZIpqmaXJwSo1dGotAPaphLOgTwS9/jls3bun/vms3du9uS+vQrll+NYvLQ1mkY/KUTtvA4otF4EvHcGCfPI0oqBrysT7MjlXrnX+oofJYBMlKDtUwJfpbXhnpSBKYb2OJj0XPh5l4yVZ9vd3VZgbRNzfemXy0nsfgwBJyXjV+C2lEnh722PbOPB9NcUvHhbTRQbqAv3f7tn3+EFg13y2fugu7P/c5fdhiQSjR1hfFxPkc0Gy11E5jVFnuvCAURkerMM1xgtHfb7ozg56oV3OClpWRHsgg7tVW0edrajv5fDSjdsXZja98ptiWDrAsEW1S6BLRLY4lorQtuD3VE7mozQyibznHIKVtykhHMhhea98Xe2ozg0hCeMWZpDwWQeFoJ0q2dyKj9FjsHDtUf1C93UKJKAPRJjEQ7RYGokRERJtZK4Eoq+YpkNiXs0jHNrD44iksXpendtJeHPjaMRzAIk69uIgNeTIRERFtWQxEiYiIiKgnGIgSERERUU8wECXakYwvQHm8ELk9yki7vrDcn96btfEL3reS2syg8W1zIiISMRDdaq4v4lQ2i+yry/IUYLmAbDZrDFJbTts0j+WJ2kb/RGplutS2HrKbm/6lIK/gOTqZC/mpWCKinYGB6BayceEUst8CYg/KU4wA9ewNPP4HeqD51GPAxW8UYIWbsbQQiD6FxzfO4tQFdv3ZuYxPkXboNUbmJ1JLHq9V8ROdXIXWzs/HbQoJFNZyqLT53ZRERFsdA9Gt4voiitcPIPu1A9grTwOwfOEibj54wHrF0d4nD2AAa7jsWvC5F3vdVkIC41vYYtW18ZLvjpdqGd8RFqty9e+RBwli9P22VwPbx5lV397fWDaq7Y3BWtd6HoNu+7CQlqr4yzg+BdeXSNe/MW4O4vrE7Tqr9G377br/0rpdj82bvP76eXaWdtZmBq316+dGf79e8aBLupn6J5AbLSIjjyci2sEYiG4V+w7g2Je9XuK5gY0NYGDInL6BxRfPYg3AjetupZ7LuHx1D2IxRqPeophYLiG1Yn4NpIz0wSJSHfnsn6R/AqvzKajmV4AW0kieTqHk9Qk7mygOjylQ584JgaGKpRUF44f0sNAscaxOu30Pw6xSrxqfQC7Vq5T7FcRRgdqozedCAcWhcRx2qZKvzST1L5lYn1gWj8kopdVKSNmW0pn7be5XCimUhJeNl8eMr6RoGjStilwl6QwGvSyk0TcVR8lc/3wKxYPOYNhNYs7Y3hCQmq9/PtrtJduJoynp3BAR7WwMRLeZ5VezyGZfwvJDT+Gpx/bg5vX3rGkbF04ZVfNnsSaUnpIXvToVU8eRn8mgOFoKHoQapZr+JXg+RgqoTgOZ5/LIP11EyusTdi6ikzmkVmZxzgyiFgoojuaCtdVcP4dZW5V6AienFRTPlAEoGB4y56uXjtauVIC4YpV+yn87nC44S1VDKo8lAVualFE4LQamUUw8HzToqyH/dBHK9Mn6+kYKKI2qmD3feOlQlGEoK0tQ5fFERDsUA9FtZO1sFov7nkI2m8WxJ/fives3sWff56zpe588ZrUTfWrfIrJ8QXxj/RMoTVeQmYrbSt8a6p/AqlV6Jwwh1hGdLCFXySATDxEAAwASSAtBVPlMEamjAberLkFdyaBPCJ77psywKQolrmJJBWrnlxAfhVU6qsTqpavqsneYFZ1cRWm0iKRX9XUQC2kkIaXJuooK6uuNRCKIHCwKMzQW3+8ZOrdP0FJlIqIdgoHotmC0+XzwCI49aVa369X19+1zr37fG4thz4fvol5eSq4W0uibG0dpuoJkmFcdtVoialQ1z46VwlUxG+pVwHpJYTpMIDuUQ9UjgFZiCipXyjg3B6SfGcbS+RrUZdUWxIlBqRu9KluvXo9P9YU8tjLSB+HxUJCqV62bQ4jOWJUr4n7UoFaEP9tlXUUFcShBSqeJiHYABqLbRGxoALh6FgWzc9Ly93DxwwE87NGsVO/c9DA8JhNQbxf6/AQSkyXkYLYXDaDVElGjXWhuMoGJ8znAbC8qzeMZ3I6kkVpZgrpQQEWscm5kJC20i/WiYimeRqJfAeaOo1BRMCzEntH9caCiBgjahar+QPT2q/YqeUP/YYwPFZF0S4uGjHa1U8frTQYWjiOzkkJOaOdpBaoLaaGU2BSFEofRhMGHugR1aBj+oToR0Q6iUVN+NArt5Xl5bCdd1l45cUI7IQ/Fy8IsrwjTXtGEKdr1f/WC93KBXdcunHpBu/C+PL4L3r+gvXDqgnZdHt8xJS0FaBgt1UfNpzQAmjJdFWdsP2M7KSF/lUahAYqWW3POZ9tHQXVacS6jVbXcEPTlxMG2DuPYhcHaF2nf9P1KafY90JcX91/nsm2X9LUP9f3Xj0eeLm7buf4w58q+find1nKaYk4bymmlacUl3e3p5rbt0qj7eCKireyjjz5qeohomqbJwSk1dmksAvWohrEwVZ5b3gYWXywCXzrW/Y5O1xdx6ltAyuP1VbS51GYG0Tc3jmqIqvFtbz2PwYEl5AK9/YCIaOu4ffu2PCowVs0TUdtFwzZl2PbKSA9kEHdrVkBEtIMxECWiDtDfwxp3a9vabV4dx1rpuR9SbSaDynQ15NsPiIi2PwaiRNQhCRS01WDvL+0kr45jxuD24vl2i06udmU7RERbDQNRIiIiIuoJBqJERERE1BMMRImIiIioJxiIEhEREVFPMBDdaq4v4lQ2i+yr5ieUBMsF61vy2ewpLF6XZ4DxLlC/6dRO+RiQXpDHEhERERiIbi0bF04h+y0g9qA8xQhQz97A43+gB6JPPQZc/EYBcri6caGIi3sHMCCNJyIiIuo2BqJbxfVFFK8fQNbjy0L6t+MPWF882vvkAQxgDZfFSPT6IoqvA48/+bAwkjwtAJEYUFsHBiNAJAJExuyz5GPGeGlaeUwfl1kBigfr8wzO6NNrM9K6jG2YXyovj+nz1mbqy4olq+kIUBb3K4YA33avv1NTfHdmeSyCSCRd/846ERFRlzAQ3Sr2HcCxL8fksYYNbGwAA0Pm9A0svngWawBuXN+oj/vWReCxVPc/z7mVrQB9A0BOA7Q1QDkN6wXt5TEgEwc0TR9ylXqgmZgzxg0Bqfn6PKuTtrX7UqeAvmV9ueo0UHzaHmwmzf3SgNQKcDxIE4D+CazOp6CaL5pfSCN5OoUSPztJREQ9wEB0m1l+NYts9iUsP/QUnnpsD25ef8+Y8D1cxONIPelWnkp+cmvQg7R+YHwIWFL18YXTQGmuPt/E84A6F7BkMoghoGqsP3oIUFYAY9OAuF8A0qNA5Yow0c9IAdVpIPNcHvmni0jxs5NERNQjDES3kbWzWSzuewrZbBbHntyL967fxJ59nwOwjMLZG3j8S+7V+uRjCDgsfBloYhn6ZxrXgQqApFk1HgEiB8UFW6eMAda3ePqBVa0eeLYqOllCrpJBJl7iZyeJiKhnGIhuC3uxdy+AB4/gmFXiqVfX37dvL7B8GWu4iYvfMHvUn63/7db7ngIrGVXj1rAsBI+bWHmsD7NjJeQqya58a52IiMgNA9FtIjY0AFw9i4IZVy5/Dxc/HMDDMQCxtPBapyyy2SMYwB69h71nu1PyZVTTJ6XOSzIlDhTPyGMNlXo1fnrAXu3eUUa70NxkAhPnc4DZXpSIiKjLGIhuGcsoGIHk2asArp7Vg0qzRDOWRvbIANbOGsHmWeBINg2GmZ0zsax3ULKq5oVe8abEHJA67ZwenQRyAPqM8cPzgGJftDMW0ogcFNqF9k8gN6oiMzDIYJSIiLouommaJo+kxi6NRaAe1TC2o9rXbWDxxSLwpWPd73l/fRGnvgWkPF5fRURERL1x+/ZteVRgLBElIiIiop5gIEpEREREPcFAlIiIiIh6gm1Em7Qz24gSERER2bGNKBERERFtOQxEiYiIiKgnGIgSERERUU+wjWiTetZG9PoiTn3jIm4+eMT5VaTlArJn14w/9uDxPxDe92kuJ8wOt3UQERERhcA2ojvExoVTyH4LiD0oTzECzbM39M92ZrN46jHg4jcKsH1J/t7H8ZT4qU8GoURERNRDDES3iuuLKF4/gKzHl4WWL1zEzQcPWCWge588gAGs4bItEiUiIiLaPBiIbhX7DuCYZwnmBjY2gIEhc/oGFl88izUAN65vSPMSERERbQ4MRLeZ5VezyGZfwvJDT+Gpx/bg5vX36hM/vIiXrKp5qdqeiIiIqMsYiG4ja2ezWNz3FLLZLI49uRfvXb+JPfs+p0/cdwDHhPahTz12A2cZjBIREVEPMRDdFvZi7169F/yxJ80WpHp1/X373FqUAntjMeyRRxIRERF1EQPRbSI2NABcPYuCWcS5/D1c/HAAD7s2K93A4rcu4uaDD8N1MhEREVEX8D2iTer+e0SXUcjqHZBsxHeB2t4jOoAj2bQVaG5cOIWXXq+/RXTPY08JpadEREREzWnlPaIMRJvU/UCUiIiIaPNpJRBl1TwRERER9QQDUSIiIiLqCQaiRERERNQTDERbcPNKTR5FRERERAExECUiIiKinmAgSkREREQ9wUCUiIiIiHqCgSgRERER9QQD0SZ9PqbIo4iIiIgoBAaiTbr/0DgwdRyX5AlEREREFAgD0Wb1T+DIdAXlSJrBKBEREVET+K35Vi2k8ezBIvD3x7H7X87iljzd8DeT/fj3pXV5tEHBb/59FW//S3m8KY67UcHH8mhDa+t+Ap/Ba/hAHm1qcFx/Yxj4D0vyWFNr627tuHqYZsP/ObD0M3msocV1t5Rm/znuwc/g+UXgltbd4nExzVz0MA83OC5e9y6Yh100OC6mmYse5uEGx+V33f+/P/pIHhUYA1EiIiIiatrt255he0OsmiciIiKinmAgSkREREQ9wUCUiIiIiHqCgSgRERER9QQDUSIiIiLqCQaiRERERNQTDESJiIiIqCcYiBIRERFRTzAQJSIiIqKeYCBKRERERD3BQHSrWc8DkQgwVpanELVVbWYQkUjEGNJgjuus8lgEkcgg8l6fkQ5iPY/BLXKu9OPVh8GZmjwVaSvvtZgm1DVtycO+9HyRXpDH01bGQDQMMwgUh1henqs1M4P6ehtdaBVVHtOCGhCTjmsbBbvuAVUN+Zj8AyiOE38IjSGWh/xzqf/we0zrKH1fO3ZDXs8jOQXk1jRomgZNKyAhz0OAI381yC+bUHlMvg66IzGn563SqDwFABIoaBo0rYSUPKkbFtKdPX/vlDH47LOIPPss0m94T4s8+ywGS+9IM/h4Y85azrnud5A/VZ8WOVUOfHx6gCk84Bj3vWD5xuVe2tRvSxnpSBJFpJAesU+xX4NCIGzen4Whvs+NfgOCuIS0kN6Rly/JM7TGygtzng+X5ZfDnk8pH7jlFSkfBV93cxiINmO0BGgaMJ8CVjLdDdj6J/RtL0/IU9ogpa/bHOa2fuhRHougbyqOkmYGVGkUQpyv1Ly5XBU5ZNAn/TjVzs8C0yXkMItzHSsF6AF1CerQOA73yxNIFp1cNfKIHjRZeWZ5AlF5Zh96YLaKCab5NvcO8mcvwa0ooVY6hcjsJcSTJ6Cd0IfV5P3ybO7emEOkVEPKWLb0AFAsmQHGO8ifmkUGj6J64gS08Ueh3LqEvkABRg1qRYEyVIFq3ONq52eBIUWe0TcP1++lJaROJ0M/RJfH9CC0JD0UO+/xJeA58T6tCA/UJcSn+kIEmn4uIf1sGcUHEvq5SkaBa+W2BaNmXnDLJ5Y35pC8Jo8MaLeRF4yh8Igx/p0yBoV8pJ1IIHXrEvradFxuGIi2YiRt/9ssMTUzuVyNPjMIRNL1Uk9xmjnvlJHtDpolk+Y2yt4lset5IDIIjBnrHcvXSzjFC07cbmQQCBw4GdseKwML6fo6zBvJzKCxPqFk1VZSLO27dUzCNK91m8Y8xvtZzyNzWkFuTbxxJVBoKsCOYmK5hNRKBsetfajh3JyK+P4EDo8Bs+fD3dzEqkl7dZaztLM2M2iVIujL9SGzAhQPuj3pNyKVULiU/NSuVKQxQZSRjqSRN0onBmfKyMfcj81ZMlJDPjaI/EzaGm+mj5gO3mlWL9kTS0fMZWszg47jdBvXEVZ1uZDu4nZtpTZu1er28+U4zwtGmkUiiAxk/H+4BGY6JU8D6lSfyzmR9y1sSZZ0rtuc1n55AcLxmYMt8BHTTJxmHu/BIrCSQZ81j3hevPJwQG8sInNrN3LJR2EP5S7h+Bu3gAcS9aAgsHeQ/0EN2P0oThrLJp7U11/8ySVjm0Dq7yb0h6P7E8g9AODWGs4FLHAdH4sb97gazi2PIzcGqMtGbmuYh0UJpEeBypUQuWE9j8xpQJk+aa+Zcb3HRzEx5/UQmMDJaQXq3LmW82Kt9EMUsRu5Jx/VRzxyALndAK6ptuPX86Ezf/p6p4zkG0BuXH+gcGec8wcSPvM04b0b0j3k8xjebRvRdgxEW7FQ0P8fcz4ZeisCc+N6ieO0ApzO6AGhWdI5baxr3iyZNLaBhPF3SVyZQAWQ00tpT2eA5zVgFMDcOX3yzKAe5JrrHVWBASmQbqSSAc6k6/vwtNQs4VASOG8cw0rGCBrLQCQJDOWE/S86mzQ0WncTaudn21yqp2B4SLiBrp/D7IpeTRTdHw91c6vNDCJZyaFqPMVXp4HMQKMbuE4vdagiNySWMmhYnXS/9drVkI8lUZmu2kt6bUFuBH1Tqu2H2BH8eCois5yDNp+COpXE0vMaSqOqFaSXx/qQiZfq264khXWryMwNo7qWg3I6iUysiuq0guIZfd+CpJk61Ye+5ZwxXUHxaT34iR4ah7IillrrDxHK2GGPH6x2KyIZyWB4zSgREh9o+iewqmnQ1nJSYAKX86WX6NgCp4OVeomP6zrcmSW5pVFAsdavQbMe1MpID2QQl0qyguaF2sxx4Ly5rJ7PkgGXbaRRXqjNDEqlZBoKVnVuGekz6frxzqdQPGgsa56L+RQwVF+/2DTFPw83cgnpUg144AuY+Jw06Z0NVAAo920I1b2nkA8UKL6LpVsAdu818rNQ6nprAxffvwlgN4bNbVolabew9J6wGj+H0ojPnUNt/RyWYoft+cw3D8vKKJxWMH4o+JVXOz8LFSnkpHtc++/xwak3bgHYA8UosK6VziJzCwBuQg10znzcn8DqiWOY8CkM17cnBMJh3bqEPrcmBUZAXSw9i8jLZb0kvZXtBMBAtBmnk3rp3MEKsKYBgQIAkwKcF6vVVT2GbIej5q0yBdja0NSAOVUPBs3xR1N6QGgrYSzaSy7lm+tK3KiuT+hB7sqSMFEFxkpAP4BD4/qoK7V6sP68ecwJI1CdtZfI+q4bwJwRQEttg7orCiVe/6t2fhbqaFr/gRpJI2ULdPyUcXxKRer5+lN7dLKE3FARhTAlvs1YOI7MinhDj2LifA7K6QLKQru96rRi+yEOFuRCrwZ7xsiHQzmctJ2vMgqnUyhZgU4UE8+nbAF8PU3kH52AaTaUQ9VYvx58LumXV/8EckJAbD5EyD9snZSaN6ssQ5QIrZ/DLHIoWfupl+iYwXn5uQwwXXKtCm1VbSaDou0cJlCYt58vP9HJgrBfURweU+olaC1plBeM6fNe7ZqlWpGRNFKoVzv7a5yH/ThK0URGSZT6xg2kzSpR3EJm1rt9oMUIYgGjSvfZWSz93XG9hO7WDXz/xi1jqtGm8Qf3oZrUU6/yfqOoScXSCvR8G5/F8eeWMBwiiDTVa2+SqITMs+qyCgwN+we5YqmsV+n7eh5JKe805x2oZpIa7Tj7bnwB1Ud2O4J7/YHPvblC88zS8y/4Bqvu7sfEsXqVvLNJwf2YOGbknWuX9OC6qe0Ex0C0GaMlo+ROBQ61XnLXeSqwAr2U0gwyDxblmZxtROUf6aHh+r/nxNJaw35jfrN0dzIKBK3ibbTuTaEG1TocvUQtZQX/CaTFQKchBcO+d9UOanRD75R1FRUUkRSqRCOu+dBL4zSzlXD2T2BVKMlKHK0HDHrbXqmar6PsHSwScwGDe3UJqq2K2Cit7pa40vwPtlSt39799skL6yoqftMd1fpJBM6FreThd8pIvnELyiNHfH/UlUcOGPnyUaQfQLAStvv3Ig4A18rou/EFe5u/3ffhd+7TA6TM7A8xPH4C2jGjih5A/LM+OyNJHI2jWBnG4X69FigMsb39+FxfiCYN4n3Xh1iabaMiM2CcK6OEv1463qz7oewGgBqSszeQO3EC2lfMhwuh5LlDyi+XUUQUJWubLTCbFNzaMIL3S0g/W29PXHpAz1ftavvqhoFo08ySPbMKejNTgCG9tMgWaPaqhLEtpSKNRffHAc9SSnvppihuBtSy9XOYXTGmG/8W22gmTyNwyQigYsmWDGapQxeYpYQmVfq7o1K26lJNC9Opp8U0s0qtazg3h1BVgz1lqyI2hqbaOTehotrb1QZ9sDSq9SFU+VfNZkdt4ZMX+hU9KPNQmxlE8rSYD8P2zG8uD9d+vGaUeM7q1aFGRxS9CvQS8Ln7gj0gmj2abYGB2Y5PDE7q1fWPf3YPANiC4JpcXe8rDqUfwEjBfqxS/ghGL0UOvqz3vdr/Hm8SOyuJQaj3ej1/AwTKfXrDyVRyzHqglavrO+MSCtegB8FG1XrymlnVLjblMHvHN2reYeQTg15qX29PnPiKWbJuBqrtx0C0FWYVtFFNZllW9erwQxn7+CDMp8wg1XaBRYGxHgXNkzn9/1abzzJwGsBoTq/GD6qZzkojJ5EbUpE5JFbTlJE2nsSVmNRo3ai2ll8NotN/WNXRkn4jU5egygHCWk5qh+hFr5o12y/CtRpUqLpdSLuUJuk3UbOKNrCRNFIoIiO+wuTporMTQCf0H8b4UBHJwCUhomBp5i+Bk9NA5lASs/Gca1WZWwepnhpJI7Xi3bbSnoeNPCrP1IDjOjDoTRvEznnOKvFGrB90o0q0PRrlBb12wn7dS4RaAb03tkQZdr+WW8jD0eSxenWo2XMd0Hsnf+VR4P6HMb4bUN9YNKrijYBj9wAOC4FN+SfGUdk6xdyPw/27AdSQMV73ZGsG8IiCFOzrbr5qt1XGPSdE+2wlpjfncpwP13t8cI687/sbYBf9rQG9M9gPjDcPvFNG5ppYoq0zO821757yKApiPjJLLXc/iqrYrvSdy5i9BQC3MPtj70hUDjyjxkOL1WTDXI/V/rgDNApuLaeXI46WjBFVTRuCpkHRtDVj1LRSL2+cLunTzfmnFfd5542/TaNimWVKH2duWx5GS/Vp85qmzafqy4xC04ZyHusV1m0dh8u6NU3TtJL+t7gukddxmOR9t9arNV63ydx3r214qmq5IWiAOaQ029ZHxWmKljPPjVbSUtZ4fVCmq7blxL91+rac493Zt23fL20tpynmtKGcVppWNNjSTXPsY9Dtyss516tp1WlFw1BOC7pGXUlLmWk4n7KWt6eVfD7Mafr41Lx57Hp6VKXj9ksz93MiMdI15ZGPzPV7Tfenp6tjWeF43FSnFVt6OI/NmRfr2xDTU9FyayUt5bMtd9I5EfODmA9D5TH5uFJaznYuncdkW/98yjHNfn365wXndDHN5OsmV8+3Ann/6+v3ysMh/aykKdmslrokjvyRlspmNZjDCyXnNXjpZX3aN38kT9Gq33mhvmz2BS33M3GqtG6X5V155V/hGvfPwy7n2uWe48vIh17pLJ9r6961ltMUl3Mr8v4NCMA4h2aaKt+xJbjOzMuhjlk6V9bwsvM8aJpW+qZbXvmZlnshQD5wTJfzkdu6nT766KOmh4imaZocnBIRbUvreQwOLCHHF/QTbSnlsQiSpxXk1trd8afD1vMYbFvb1M3r9u3b8qjAWDVPRDtEDflDmS53UiKidkjMlZCC6nhl2+ZlvG92BwShrWKJKBFtczXkY/oHADBa6l5HHyJqM/0zn2Bgt+m0UiLKQJSIiIiImtZKIMqqeSIiIiLqCQaiRERERNQTDESJiIiIqCcYiLaZ+Om4QcdLqMtIW5+GG0RefjnvlqH3BmzfC3q3MfMzh028ADsI82XJ+rBVepNuUx0+19tbc/cU/X67le+ldrbrOWw+Wkh7f2N9xyoj7Zs/jJ7tYdM6FGMbzZ7XHYCBaJsl5vSv7JRG5SkAkEChqU/KtclmvlGZn607ZXylQh5vDXMhgq1LSNuWdf/cWfnlJtZvBB3iw4b+oxgsGLR/6zr4cjbreSSnIHy+bou8G/OdMgbd0rqVc+1Y1mUd0jyDxhdogrIH/X4/bnb25cItSz3S0r2yuaAaAKKTqx34HGrrWsrDC2lp2RbueW1nfG70dNKl4KgRsWCpPsjnvTaTRAbCV/j41g4HBqLUc+WXn0Wk5HETeGRM+JRZAinUkJSD1UYeSAjrED6BZgSqyVuPompNr3832Je6BHVIMT7nCgBlFCqK81vR/RNY9br5jJasz4OWRpv4bKC6BHVoHIe30Muda6VT1je2HVo517Zl659PxAOKfj7fmEOkVIPyyDi0EydQfWQ31DdmkX5DXpG78lgEfVNx2/fJ8ZwUqPida+FzsNVpNP1JQrLTH/y32AvOtyrxk8bzcWQGAgajIwXjmqkiNwQo09XN9eA8UkB1WoE61ecIIv3ZC5ZS83rayK+VUpfVUJ8z3YkYiIYmFbM3/dTszl5S5rzQ5SdT24UjPXla08wqw4NFYCWDPtcn0kbVB+LTn/FORnmOECWCljfmkLwWRenEOHK75Ymyz2N4N4BbN9wDmZCsbzEf0b+xG1p8HOOVgn68CwXg+RziqEA1zpl/Mw27xNEUUFFD5aXalYo8Sreex2AkjbJ4zmz5NMy5dt93PR8686evd8pIvgHkxo1vI/tq7VyXL1yCan5nG+b3uXdj/Lf0p5Bo8gtIASj+5JK0pIv1PDKnFeTWxB/OKCbm6t9cD3Ou9W+4LwU+rvKYvk7x2ve+7p3nxPee4ZsX6iV79eMz169Psx+rPM5v3Qh0T/Fk3tMiEZd7ThnpSBplcZ6g9+lA90qpJExYt55O+rEUD9bnEdOp2ZL1oFRh/fXt6vtsP/du4wIyvvE+e77msR63cT5s51PKK2bptDiPnJek3z7HNaj654XoZE6/HzztnNaaGlSP23QjW+26bwUD0ZBqM8eB82apSBU5ZJCUM32TajODSFakkhPhKxK1mUGpVEZ8+iojfSZtjdfmUygeNJY1S2rmU/anWuGJtDzWh0zcLKGrIlcRqypqyMeSqFhPsvqTbVs8Mha8FPKdy5i9JZRyBXWt7Fodq964BWAP8ONT9SrblwMEJhYFh8cqKCwA5TNAWnoS9m+mYVc+Uwz81GzeHPqmVNuPpf3mUEQyksHwmvHEvpLBceNGFO5clxAPXVLg4f4EVm0l0j6aPdfQA97MNQC7B3C40bZubTT84amdn21Y8hzmXNfOz0IdTYc6LnWqD33LOeO+oNR/MNfzGDxYqTfPmI/bvzyznkfS857RKC/oigcjyMT0/FAaVZF5rgwgisNjCtS5c0L6qVhaUTB+SM/F/uuW81nIe4p5T1vLOWshAD3/DywhZ+RhMf/7anivdNlvZNBn/JDr+UA/FrOETNM0rE4aV/Z6HsdRrwlpe+n4SgZJI59oazlgKmkEEAmkR4HiGTHAK6CIlOO+FV6r6y4jbXx9SE+XElJyVflKBn3m+VzLQTmdqQdeC2lExGtATG8AgIrMwUZ5IYGT0wqwMotzbXkwMB9W9IcSdarPM2D0s5Wu+5bIH66ncKrTiobRkjxaK41CU6ar8mhDSUtB0XJr8jhoqXlxXFXLDZnj3Kb7cdnGfErDUE5z7lVJSyGl2Y5CnNexnLhf7fIzLfdCVsMLJcf+Vb/zgoZs1hhetu9nCOZ6lO/8rL496295uj/rvK/lNGU0paVGS+5p7pEXSqPQAGFwPS/+qtOK+3JrOU2R8kp9Hxqc67Wcpkjr9MrjrSh90/1ctuNcm+tIXXKOs87tpZf1bbjkN5ktnY209Tpnbue6Oq3Yz7Wc/g2URqVtreU0xVhHdVqRtiddmy55oa5BXjDWZTv3tulSfp9PCfM2WHe77ilCWtQ5r0O38+LLsX/CeHl7jn0IcSyOZXVNXXOOfdb3wzpuaVth0kS+1+h5WtjvQOuW9scc63Yfk/OK7XyK6eu+zroQeWE+pcHzWvHj99vcaP+8bbXr/qOPPmp6YIloWFIVQt9U0Aq2IBQMez1YrKuo+E0XSsr0IYmiPIOXdRUVFJEUjityMPDSHRdNHrPa/pUeqCH57LOB2/aJzOpY9ca74ljkkkZ17W8NQHFMb6D/MMYrReBoQj9/YUp1hDai1bFZ9DmqGFthL41IzBmlBI3OtboE1VYl2e487q/1c30Jx9+4Bex+FCcfqY+NJo+h9ACgvjGrl3z/BHqHwd17A5VCW8QSszBs7euAZMiSEVtpef8EVo0SOnVZFUpb6iUwlv4JrM6n6tXEYrVko7xgSB0Vym5HCtCWzSYJCaRHzepZvVTfmjfgurekoWGPUtggpCrRgUzgJhrhRaHEhT/7D2N8qIjCAgCUUTidQs5WctiAcF/Qa+aEpiqtrjuu+F+HthqJKCaWzRI+FUsrQHy/79LBKK2c187Yctd9kxiIhqJXIcCqlml370YVS7a7kn6RAQD6FYj3FFltZhDJ0ylbZ4pwP5XissZgZbzNI/GQvkeV98Uez+8gf+pZ1x7xNu9soN5c534oDduk+lNiinRThMs5DEZvo1RvX9pZDc61rUrSGNw64HRYU+f6DRVFAEr/w468m/iK0JnpyftQAaDc93lpLqfo/ngbq+zq7euaySdu6p0/6oOtw4TVWURDKZ5Bn62NXIO80EDiaMqoptODD3tVbGvr3rTk9r1qmPa+ffYe1J5NC9pBb59YD9L0atXimbJedR6yeYh7UwVTi+uW2sd7tn93CPnw7yfEedwMNu91Hx4D0SZYF/Z6Hsm2lRYZ7WyExtK1mQyKQzmcHDGnq/7tiYQn9fKYS4moMgzF7QfVeJr17LUtLVcec+9Y0Pl3+r2D/A9qtk4n+mijPSFuYfbHbtGJTu7Aogc6NWSMdqO1H69BBZB6SJ/eTfq59m+H2BaNzvVIGqmVxu2ezYbwbWk76qqZc20uUy/ldvcO8mcvQW04n8EIHH2vvTAWjiOz0vrNG+YPgtUOsDH94cnQKC8EMZJGamUJ6kIBlemTjhIyz3UHvKf0jNe9ciSNFIrIiG1dny5CEY/dKIm0tZkUWaV/NeQPdbBE1CWfRSdzSJ3OYPDpCnLPhAoVG2p23XrnPbHdZhnHp1Skng8SGBkBsNkfogV68Otf67hZbNrrvln1RgAUhL29V0rL2dry6G1F7O3BhPYhRhsU++Bsv+LXlkxuW1hvA2LftjKdc7SN0Vz2X27X5Lrf0nLKdFUrjTrbn5jzhGoP87OSplhtAoXBaLtnbzPo1W7QbO/5gpYTm3c61u1cVl6/2K7Qj3s7I2ebXjE9xbSRz6PbuW7EtW2V5myv5eR/rt32XT7XVl4O1Y7tR1pKPs/CeZHPhdv58jzXmtDu85s/kia45AW3eRpwnDNbmyn5fNbT1NlG1Hld+nHPawLHfaV+7p3blvOFX14I1tZR34bbMfmtO9g9xYvzuMRjC9Eu0If3vVI6367XgHw/NrYttjEGtNS0eK0608t7/S588oHI0fYwAM97jcR/3T7tJaV0sc3jaPvqJOeH+vJB84JLu8iG3K97ex72OeYG3PdT4HO+5fRw5gVnXmv1upfbfYYZIpqmaXJwStQUq/ci3+u37a3nMWj0dJXfm0dEm1d5TO8Nbe9Z3h6dXHcn6W+kAX+7WnD79m15VGAMRKkNykhHkihC4YW87dWQj+nVqCkGoURby0IakYOwdzRql06uu5MW0ogcLEKZ3noB9GbCQJSIiIjcGcEWOlFY0Ml1d5zxYB0v9aRD5nbCQJSIiIiIeoKBKBERERH1hBmIvvnmm/Kkhvj6JiIiIiLqCQaiRERERNQTDES3GvMTo628jHYHET97OujxkvbOv5zdW+c/AhBWGelNtT/dFTQvlMe885M36fOOvIYbCno+yMV6HoNt/Wzw9tXbfFZGWvjcZvj7ytbHQLTbGt0cFtL278JSSxJzxifORuUpAfF8hMc0c1WbSdo/77gdeum2dK71wLw3P/6dYwY11tB0+pBlG+ez8lgSxdGS9anNjr9CyijMEgNevUDEJy7pMAaiW03/BFa3y4/YJhGdXHV+p7dL9EB5q73yZPvqZF5Ql1UoY4cDfLaQTJ08Hx2xkEbfVNz+He8Q3/Cm3uhdPqtBrQCpo138PVeXoA4pwLL5cdkyChXF+jx4LzAQDU2qXnM8RXgVsxvjBzJQUUTSmseoBjWr3A8WgZUM+lzW71vNvJBGZKxsexqX5xGX95rHTW1mEIMzeeO40igvpPXlhapFeymAULXbjqcvc3su+6zvW812bMGffMVzJVVHBzgfDfnst7V+13XKecztuLzyWQMLaUTG8sb6B5FfMPZDLm1Qhf2Tp3kJlGb2/Q68boNnPltIO9NRHmdLc7lq3CcvAI5zkjwtT29E/8FxZZb2iPvnuW/yNDlNxHnKSEfSyBvTB2fK9fNuHZ9fcwF9+bK4X+b5CnSupXUL51q/XvUPIxQP1udx3C8j8v7W2e9n4jw++91h5TNFYDTt+0J3+/lqcO2Hbb4h3nMG5O/YN7725Lxk3XMcJZJiE54W85nfb1fP85lfmrUjn6lYWpHHGfzuV1bNqrB/YbYdH8d4paCn4UIBeD6HOCpQbdeQV3q5n+uWCJ8OpQCqtm/Ly+TvyurforV9s7XRd8ADfFfX9Ru08re/51P278va/g73Xd36N2WN5YZyWlU8jrWclpK/IS0ew3yq/k1aeb8acn4rWOT4vr3H+l3TzOKzjQDnw53POkWN8oNm7IPtXAXIZ16MfJKaN7+dntJKtn01v59szyveaefCM83kdYXLh/75zJkG9mtVnq7/7Twu9/NWGrXvp39+Erl/j9r2bXbrm9H160kRppdGnd85t47DMa/4rW9j26Mlx3k3991+XPL5kfOCnIZ+51rTqtMpIR3lddfH+edb9/Mh32P0+4B9P+W//bfTJvJ9WCbdn+Tj8D8fDUh5wX5vkdflvPbsaShxnGeXe0az+UxOM7d7uGP7dZ3LZ/K65DRrIZ+t5TTFcU/wW5d0v7KWt5+DQNs2fk+q04pxnuTfAPm4xXV7n2vzu/H/+l//69ADS0Sbcdp4kpCtn8MscihZbTwSODmtoHjGde72G8qhalbZj6SREp5w7E/qURweU4CKGvwJajRnVR+nnpeqmvonUBDatUQPjUNZWao/jY8UUJ0GMs/lkX+6iNR82E/AqZg977Ono6V6uxrpuHurwX4HUkb6IFASm2K0ms+GcjhpVEEp0yddzoWC3Jp5jvS8os6dC55XvCwcR2YlhZy131FMnM9B8bqeZL75TE6DGs7NAbln9KOozWRQFI4bSKAwnwp2XOt5ZE6n7OcgsAQKmgZNqyI3BCjTVaPKVm6OIaR5/2GMD6lYMi6gxJx4vSSQHgUqV/S9rp2fhTo0jsPGuhJHU4B47UGx0kA877oyCrbjimLieTlNxLxg33Yj0cmCcIxGPrKqA1tRxvEp1XYfik6WkBsqomDVGjS/3y0ZKUDTSkidTrqU3On3YfH+F53MIbUyi3NGyWLj8+Gt/FwGmC65N/NpeO0ZaRr63mxqMZ/5/HY10rF81jDN0Hw+M5vYaSWkoH8uWb8v6OsKer9KzZv3keDbrl3Rq2aih8ZROZNGAVIJfsPfF79z3RwGoiFFJ1dRGq1XrduqRNUlqLbqgwj6ptpwQbSBElOEALqGc3PtbK8mVY04qoSMH4pKBpl4KWQ7nAQKazlgqs/1xu6u/iPeO83st1N5LAnIPw5dzmfR/XF5VPOGhltoi+Sfz6KHxus/EuvnMIt6gAYAiCttyu8dIASTQBQTy0J7NamJh9gsILo/DliBjPzA2cC6ioqtmZBRBdouUtVie/OoguHmM1KHmQ8fmvFbYVb56s0zxCriSCQJK8U7fT78rr11FZVOpWmnj6uT+cwvzTqt4f0qhbTwW5qYC9nRqf8wxitF4GhCv56GjPFd/n0BA9HmmD2xNa2E+FSfPRgdEnrFbqLesXowYd4M+pCxPfG0pjymr8867rWc4+Itj/VhdqyEXCUZvD2jyXp61FCdriDZMKjr0A01rND7LVlIIwmPwL2L+ax2pRLgphiQrbTOuOmJf/tomM/6D2PcKBWrnZ8F5ActqQbALBnY1NbzGDxYFEpMpDdAKMNQoCIzoP9gJEOX3KbsHWu0dnWuKSM9kAGsEmAN1el2XpTyw6ZPW7seSsyVkJL2VTyX+iCWjnfqfDS49voVtPFx00WnjqvD+cwvzTqtg/crJaY4H3jFfNrF3xcwEG2V8BQBo0phJYOkX6DVryAOsQpJogxDEUo42qNmVIm3+yYgsAKVGvKHpBLRhTSSp1PITSYwcT4HTCVdGoUH06h0rjyWRNFWutSiNp2PRvvtsJ7HoFwlbwqSz9rGqLIL06vTK81G0kihiIy133q+dG8e4MEvnxnVcpUrZZybiwtVamY1fgbHrevOWb3rSbpmazODTXRWaoXwYLWQtm1br46t/wibVXuBGIF7MmyHGJHXuTbE9xupu55H0lGqEoUSR/AmJRa9GrL4dL1zhrMqszGzs1Pwzo1NWCigaJ0/PX8WD3o8kLZ4PpSY2IRGD9CsFG947SWQHlWROeTT4cUKymrIx4SS3EZaPC6gR/msYZp1Tkv3q1Z19ffFYGvESg0YjZXFxsWORulmA+b64GhAbHVOEBsb15kdcNwaL8uNm20Nvj0bk8vbNAaPxt+yeqcPodG31FlJbHidmhamCQ2aTXonGedxu3Lst305e1rJxxQgzRzTnfvlfj4acKy7wX7b1u2SzyDnNeexOfKZGyGf1DsTuHQ8CLteiXeaSet3XD8+/PKZRWhML5OWt3VocJwveHQo0tddnVaCdyLRNJcOAALHtWunXy/GMJTTcmJHKddOD+I9wzgG1/OuueY1eycFe75166Tlda7l8Tlb5zGTPT+EuTZt6eLIY8555f02l28mf7tzpqW8z5ojXeR7lnMd8n57E5dVtNxaSUs50kVYt+NcyGnqdt82x8v3jBbymSP/O8+f5ki3buUzvzRz7qdbPvOnr981D/rdr4J0cvXgvo9yhy6v3wHvc91KZ6WIpmmaHJzSdlNDPtaHpefFYnh93OxYNVy7kk2mNjOIvuVcR6sNiDar8lgEmZj9Gi6PRZBEidcEEXXN7du3AQBvvvmmPKkhVs3vCC7tp9bPYXZFqNIgoi3G7d2kZRROm23AiIg2PwaiO4L+6gdbb82BDOLzvfiShEB+Ya80hO7U1DXSS47loZW2UC2SX0htH9xe2Exbl/46mfqbGSKIRJKoTG/tWg4i2llYNU9ERERETWPVPBERERFtOQxEiYiIiKgnGIgSERERUU8wECUiIiKinmAgGopLb2mrh7Q+zdHT2+gZnl6QvpMtDOIy9l7PYi9n57Yd25K/xS3tm/0LIm7j/Nn3zfw6iNt6hHHSd7Ld91uYr6ke5+ZxO3uF12YGfdapL+eXBt7nw2A7Ppfpvuzn1J4u0rmMCV88MbbpyDfS+banu8fXXIiIiHqIgWgT6p/KrCJXSRoBgP6KJHXquO0Hv/xcBupoCYUR/buumqZBm0/Zvr1rvmqlNjOIvrlx6xuv1WkgMyAGEApya8Y61vTXttSDqDLSkT5k4qX65/6OFnyDrDDKYxH0TcWF7wWnUfAM8GTCd4aN/ZaD0fKZIlLzJaROF8IHTOvnMIscStPA7HmXILdJjc5HbWYQkYOV+jnRSsBzPp/IE63nMWi8asc8XyUcNwLZ+scGrHyGDPrE9B5SAOtzfm6EvKJp4T79SERE1CUMRFsSxcTzKcAMnkZOIjckfJt2PY/MaQW5Z4KEAGUcnwJy5+vfko1OlpAb8vgufb8C8evl5reWq+LXVEYK7XlPqHkca2Iwk0ChmS+39E9g1RGwl1E4rWBYSSA96nG8PmrnZ6HGFSQOjTcIzsJodD6Mb//Or2LC+q59FBNzwb4FbD6giO97jE4W9HUtHEcGOZSsafr7IhVbkD6O8bj4LWIiIqKth4FoW+mBqRlk6cFGTghUfKyrqCAOxTZvFEocKJ5xKSNcKKAIBcMKANRwbk6FMnY4UBAUVu38LNShcRwOchxBKMNQUIFqVmMvFFA01q/EFPfj9aQfe+poQg/OV2ZxLlT1uIdG52OhgCJSSDcV6Otfv0kddQ/ka1cqQFyxn8t+BXHYg/TDz+RQCZVWREREmwsD0ZaUkT5YhDJ9sl5SOFJAabSIzFg6RGkoAHUJ6tAw5A/z2T/VpyIzYLT5OwiUNLE0rjHbl5UiSRTlGbpFKs0tnylaQXT00LhU8tfA+jnMrpgBYQLpUbU91fOBzkdnqMuqy3YUDA9Jo/oPY7yS8WiXKuQVuY0pERHRJsFAtAn1gM79c3qJZ3LA6SIwXQoeKCrDUFaWoEqj7UGJ2e6vhJRUOhZEvW2ruY4eWVdR/0S2Xi0/fshIw/7DGPdqjuCidn4W6mjaehBIHE1BbUf1fKDz0RlKTIG67NgyllbMEnBTFBPPxz0Cb6mN6HKwJgNERETdxEC0CWJAJwehQL3EL77fZZqXfgVxsboaAFCDWnFbTwInpxUUnzZLufQq47YEYC6i++OAZ5W3S0kdoI/3iNdq52ehmtXeCwUUbaV3fciseDRHcNCr5XE6WS/5O1j02dcQGp0PZRhKEw8DOj3NvI4xuj8OVFT7uXRtKgBgJI341HGck0YTERFtBQxENw29WjlzqF6FWptJIoMcTrq0Q4xO5pBaqXdWSTyTg7KSQVLsjb6Qbk+v+ZGTyA3Z9w0oIz1Wdm3Hqnec8mhTupBG35SK1Lze8al2pQKMCj39NeOtAoGq5/VSQnvv8CpyQ+2onm9wPvonkBsFigfFtxrUkB8LUgVudnJL2s5PbSatV7OPpJGyncsa8ocygNgExJLAyekKZufk8URERJsfA9FNJDGnoRTPoM8o3eubG0fVs0rVKBU1A6H+Cawar0aySgfPpNvTax5RTCwbrxCy2pgWkDZ6zSfmjNdYmfs9FUfJtt9FJK0SS/11R/p+CR2NRCPpYE0PhE5OdVEcHlPspcNiiWnE+a5Se9vZ+ntFG52PxJyG0qhwbJEk8IzX+ZKMFKDNp2zbTuKk0ZQjgYJWQtw6l/qrnFxL3412tViRq/KJiIg2v4imaZo8koiIiIgoiNu3bwMA3nzzTXlSQywRJSIiIqKeYCC647l9DtK7Grub7J/XlIewn9Psnq2630RERN3GqnkiIiIiahqr5omIiIhoy2GJaAhmxE9EREREdiwRJSIiIqItgyWiIbSjRLSZpwUiItqafvu3f1seRbRtNRPjsESUiIiIiHqCgSgRERER9QSr5kMIUjU/Pz+PgwcPyqMtzRRbExHR1sSqeSJ/LBFto/n5eXkUEREREXlgINomDEKJiIiIwmEg2gYMQomIiIjCYyDaBn5tQomIiIjIHQPRNmEwSkRERBQOe82HwF7zREQUBnvN007l12xRjJNYItpmfkEoERER0U7gFQ/J4xmIEhEREVHbyUGn/DcYiBIRERFRp5jBp1sQCrYRDSdIG1EiIiIiCub/D8obEB+vOXs3AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "3687ab0a",
   "metadata": {},
   "source": [
    "### ì˜¤ë¥˜!!\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "í•™ìŠµ í›„ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ì„ ì €ì¥í•˜ë ¤ë‹¤ ë©”ëª¨ë¦¬ ë¶€ì¡± í˜„ìƒìœ¼ë¡œ ì¸í•´ ì˜¤ë¥˜.\n",
    "```\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "```\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "```\n",
    "\n",
    "ë°©ì‹ ë“± ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ì„œ ë°œìƒí•œ ë¬¸ì œë¥¼ í•´ê²°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2285188c",
   "metadata": {},
   "source": [
    "#### RM í•™ìŠµ í™•ì¸ ë° ì ì ˆí•œ reward score ì¶œë ¥ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = 'ì¸ê³µì§€ëŠ¥ì€ ë˜¥ë©ì²­ì´ ì…ë‹ˆë‹¤'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11547a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264538ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤. AIëŠ” í˜„ëŒ€ì ì¸ ì»´í“¨íŒ… í˜ì‹ ì—ì„œ ì¤‘ì¶”ì ì¸ ì—­í• ì„ í•˜ë©° ê°œì¸ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ê°€ì¹˜ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê´‘í•™ ë¬¸ì ì¸ì‹(OCR)ì€ AIë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ë° ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ë° ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì½˜í…ì¸ ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ê³ , ìœ ìš©í•œ ì •ë³´ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf659f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"ì¸ê³µì§€ëŠ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ ì¸ê°„ì˜ ì§€ëŠ¥ì´ í•„ìš”í•˜ê±°ë‚˜ ì¸ê°„ì´ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²ƒë³´ë‹¤ ê·œëª¨ê°€ í° ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ , í•™ìŠµ ë° í–‰ë™í•  ìˆ˜ ìˆëŠ” ì»´í“¨í„° ë° ê¸°ê³„ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒê³¼ ê´€ë ¨ëœ ê³¼í•™ ë¶„ì•¼ì…ë‹ˆë‹¤. AIëŠ” ì»´í“¨í„° ê³µí•™, ë°ì´í„° ë¶„ì„ ë° í†µê³„, í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§, ì–¸ì–´í•™, ì‹ ê²½ ê³¼í•™ì€ ë¬¼ë¡  ì² í•™ê³¼ ì‹¬ë¦¬í•™ì„ í¬í•¨í•˜ì—¬ ì—¬ëŸ¬ í•™ë¬¸ì„ í¬ê´„í•˜ëŠ” ê´‘ë²”ìœ„í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ìš´ì˜ ìˆ˜ì¤€ì—ì„œ AIëŠ” ì£¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ ëŸ¬ë‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ê¸°ìˆ  ëª¨ìŒìœ¼ë¡œ, ë°ì´í„° ë¶„ì„, ì˜ˆìƒ ë° ì˜ˆì¸¡, ê°ì²´ ë¶„ë¥˜, ìì—°ì–´ ì²˜ë¦¬, ì¶”ì²œ, ì§€ëŠ¥í˜• ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1340e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ê´€ë¦¬(ìºì‹œ ë¹„ìš°ê¸°)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670d89a",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6114b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='/aiffel/KoChatGPT/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='aiffel/KoChatGPT/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7212c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ì˜µí‹°ë§ˆì´ì €, ëª¨ë¸ ì¤€ë¹„\n",
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138bcfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO í•™ìŠµì— ì“¸ ë°ì´í„° ë¶ˆëŸ¬ì™€ í† í¬ë‚˜ì´ì§•\n",
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c53e9",
   "metadata": {},
   "source": [
    "### PPOTrainer í´ë˜ìŠ¤ ì„¤ê³„ ë° í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2500c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO í•™ìŠµ\n",
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLHF ì ìš©ëœ koGPT-2 ìƒì„±ëŠ¥ë ¥ í™•ì¸\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?', \n",
    "    'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?', \n",
    "    'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
    "    'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9a206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ca751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c52e90b4",
   "metadata": {},
   "source": [
    "## íšŒê³ \n",
    "í•´ê²°ëª»í•´ì„œ ë„ˆë¬´ ì•„ì‰¬ì› ë‹¤ã…œ CUDA memory ë¬¸ì œê°€ ë„ˆë¬´ë„ˆë¬´ ë‚  í˜ë“œê²Œ ë§Œë“¤ì–´ì„œ\n",
    "ëª¨ë“  ì‹œê°„ì„ ë‹¤ ìŸì•„ë¶€ì—ˆëŠ”ë°ë„ í•´ê²°í•˜ì§€ ëª»í•œê²Œ ì•„ì‰½ë‹¤.\n",
    "\n",
    "ê·¸ë¦¬ê³  ë…¸ë“œë¥¼ ì¬ì‹œì‘í•˜ê³  ë‹¤ì‹œ restartí•´ë„ ì²˜ìŒì—ëŠ” ì§„í–‰ì´ ì˜ ë˜ì—ˆë˜ ê³³ì—ì„œ\n",
    "\n",
    "ë‹¤ì‹œ ì‹¤í–‰í•´ë´¤ì„ ë•Œ, memory ë¬¸ì œë¡œ ì‹¤í–‰ì´ ì•ˆë˜ëŠ”ê²Œ ë„ˆë¬´ ì–µìš¸í–ˆë‹¤ ã…œ\n",
    "\n",
    "ê·¸ëƒ¥ ëª¨ë¸ ë°”ê¾¸ì§€ë§ê³  ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•´ë³¼ ê±¸ ìƒê°ë„ ë“¤ì–´ì„œ ì˜¤ëŠ˜ í•™ìŠµ ëë‚˜ê³  ë‹¤ì‹œ í•œë²ˆ ì§„í–‰í•´ ë³¼ ì˜ˆì •ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0b041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
