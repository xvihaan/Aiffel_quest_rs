{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5c10c9",
   "metadata": {},
   "source": [
    "# ğŸ‘» [í”„ë¡œì íŠ¸] ì»¤ìŠ¤í…€ í”„ë¡œì íŠ¸ ì§ì ‘ ë§Œë“¤ê¸°\n",
    "KLUE/BERT-base ëª¨ë¸ í™œìš©, NSMC Task\n",
    "\n",
    "## STEP 1. NSMC ë°ì´í„° ë¶„ì„ ë° Huggingface dataset êµ¬ì„±\n",
    "## STEP 2. klue/bert-base model ë° tokenizer ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "## STEP 3. ìœ„ì—ì„œ ë¶ˆëŸ¬ì˜¨ tokenizerìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ì „ì²˜ë¦¬í•˜ê³ , model í•™ìŠµ ì§„í–‰í•´ ë³´ê¸°\n",
    "## STEP 4. Fine-tuningì„ í†µí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥(accuarcy) í–¥ìƒì‹œí‚¤ê¸°\n",
    "## STEP 5. Bucketingì„ ì ìš©í•˜ì—¬ í•™ìŠµì‹œí‚¤ê³ , STEP 4ì˜ ê²°ê³¼ì™€ì˜ ë¹„êµ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e32f836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2015927",
   "metadata": {},
   "source": [
    "## STEP 1. NSMC ë°ì´í„° ë¶„ì„ ë° Huggingface dataset êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a8aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3958165fa91f4170ba3424a0c594d4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Huggingface datasetì—ì„œ NSMC ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_nsmc_dataset = load_dataset('nsmc')\n",
    "print(huggingface_nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825f1209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'document', 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train datasetsì˜ ê° ì»¬ëŸ¼ì— í•´ë‹¹í•˜ëŠ” ìš”ì†Œ\n",
    "train = huggingface_nsmc_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c49f8",
   "metadata": {},
   "source": [
    "### ë¹„ìƒë¹„ìƒ ğŸš¨ğŸš¨ğŸš¨\n",
    "\n",
    "ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ì„œ í›ˆë ¨í•  ë•Œ 10ì‹œê°„ì´ë‚˜ ê¸°ë‹¤ë ¸ì–´ì•¼ í•´ì„œ ë°ì´í„°ë¥¼ ì¤„ì´ëŠ” ë°©ì‹ì„ íƒí–ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4a02a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a71d75b8e6d4225847dee1a3cc787ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-9d879241da83f708.arrow and /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-0fdc790e2ba4f643.arrow\n",
      "Loading cached split indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-5dcc18b03997bc21.arrow and /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-d0620966fea985b9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 15000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "huggingface_nsmc_dataset = load_dataset('nsmc')\n",
    "\n",
    "# ë°ì´í„°ì…‹ í¬ê¸°ë¥¼ ì¤„ì´ê¸° ìœ„í•´ 10%ë§Œ ìƒ˜í”Œë§\n",
    "hf_train_dataset = huggingface_nsmc_dataset['train'].train_test_split(test_size=0.1, seed=42)['test']\n",
    "hf_test_dataset = huggingface_nsmc_dataset['test'].train_test_split(test_size=0.1, seed=42)['test']\n",
    "\n",
    "# ìƒ˜í”Œë§ëœ ë°ì´í„°ì…‹ í™•ì¸\n",
    "print(hf_train_dataset)\n",
    "print(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8aef73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ë˜ìŠ¤íŠ¸ê°€ ë„ˆë¬´ë‚˜ë„ ì´ë»ë³´ì˜€ë‹¤\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e964b1e",
   "metadata": {},
   "source": [
    "## STEP 2. klue/bert-base model ë° tokenizer ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b396b8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# klue/bert-base ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b644c1",
   "metadata": {},
   "source": [
    "Huggingfaceì—ì„œëŠ” AutoTokenizerì™€ AutoModel ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬, pretrained ëª¨ë¸ì˜ ê²½ë¡œ ë˜ëŠ” ì´ë¦„ë§Œ ì•Œë©´ ìë™ìœ¼ë¡œ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, BERTì™€ RoBERTa ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ ê°ê°ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹ì • ì‘ì—…ì— ë§ëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ AutoModelForSequenceClassificationì„ ê¶Œì¥í•˜ë©°, ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì‹¤í—˜í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "í† í¬ë‚˜ì´ì§•ì€ transform í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì˜ í˜•íƒœì— ë§ì¶° ì§„í–‰í•˜ë©°, ë¬¸ì¥ì´ ê¸¸ ê²½ìš° truncationì„ í†µí•´ ì§§ê²Œ ìë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c455202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['document'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed29a68b",
   "metadata": {},
   "source": [
    "ì´ë ‡ê²Œ í•˜ë©´ document ì—´ì„ ê¸°ë°˜ìœ¼ë¡œ í† í°í™”ê°€ ì§„í–‰.\n",
    "\n",
    "ë°ì´í„°ì…‹ì„ í•œë²ˆì— í† í¬ë‚˜ì´ì§•í• ë•Œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì€ mapì…ë‹ˆë‹¤.\n",
    "\n",
    "mapì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ Data dictionaryì— ìˆëŠ” ëª¨ë“  ë°ì´í„°ë“¤ì´ ë¹ ë¥´ê²Œ ì ìš©ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ëŠ” mapì„ ì‚¬ìš©í•´ í† í¬ë‚˜ì´ì§•ì„ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì— batchë¥¼ ì ìš©í•´ì•¼ ë˜ë¯€ë¡œ batched=Trueë¡œ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "029dca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503f5a33da0241e98de5be1410be8c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e08ffcace44af5a9faffe0e0215747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
      "    num_rows: 15000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ìƒ˜í”Œë§ëœ ë°ì´í„°ì…‹ì— transform í•¨ìˆ˜ ì ìš©\n",
    "hf_train_dataset = hf_train_dataset.map(transform, batched=True)\n",
    "hf_test_dataset = hf_test_dataset.map(transform, batched=True)\n",
    "\n",
    "# ì ìš©ëœ ë°ì´í„°ì…‹ í™•ì¸\n",
    "print(hf_train_dataset)\n",
    "print(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf8e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6250083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae6965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0f792d7",
   "metadata": {},
   "source": [
    "## STEP 3. ìœ„ì—ì„œ ë¶ˆëŸ¬ì˜¨ tokenizerìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ì „ì²˜ë¦¬í•˜ê³ , model í•™ìŠµ ì§„í–‰í•´ ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b977013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # outputì´ ì €ì¥ë  ê²½ë¡œ\n",
    "    evaluation_strategy=\"epoch\",           #evaluationí•˜ëŠ” ë¹ˆë„\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # ê° device ë‹¹ batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation ì‹œì— batch size\n",
    "    num_train_epochs = 3,                     # train ì‹œí‚¬ ì´ epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f59bd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485cd2adb17b4b7e82485bb26fcc1310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a129c015758242f2bd36b5eed3fdeffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 1:21:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.340800</td>\n",
       "      <td>0.311782</td>\n",
       "      <td>0.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.434459</td>\n",
       "      <td>0.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.596637</td>\n",
       "      <td>0.878800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5625, training_loss=0.2486114040798611, metrics={'train_runtime': 4903.6732, 'train_samples_per_second': 9.177, 'train_steps_per_second': 1.147, 'total_flos': 1.18399974912e+16, 'train_loss': 0.2486114040798611, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_metric, load_dataset\n",
    "\n",
    "# ì „ì²´ ë°ì´í„°ì…‹ì— í† í¬ë‚˜ì´ì§• ì ìš©\n",
    "hf_train_dataset = hf_train_dataset.map(transform, batched=True)\n",
    "hf_test_dataset = hf_test_dataset.map(transform, batched=True)\n",
    "\n",
    "# í•„ìš” ì—†ëŠ” ì—´ ì œê±°\n",
    "hf_train_dataset = hf_train_dataset.remove_columns(['id', 'document'])\n",
    "hf_test_dataset = hf_test_dataset.remove_columns(['id', 'document'])\n",
    "\n",
    "# ë¶„ë¥˜ë¥¼ ìœ„í•œ accuracy metric ì„¤ì •\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Trainer ì„¤ì •\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model,                          # í•™ìŠµí•  ëª¨ë¸\n",
    "    args=training_arguments,              # ì„¤ì •ëœ TrainingArguments\n",
    "    train_dataset=hf_train_dataset,       # í•™ìŠµ ë°ì´í„°ì…‹\n",
    "    eval_dataset=hf_test_dataset,         # í‰ê°€ ë°ì´í„°ì…‹\n",
    "    compute_metrics=compute_metrics       # ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜\n",
    ")\n",
    "\n",
    "# í•™ìŠµ ìˆ˜í–‰\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a46e03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 02:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5966372489929199,\n",
       " 'eval_accuracy': 0.8788,\n",
       " 'eval_runtime': 170.9164,\n",
       " 'eval_samples_per_second': 29.254,\n",
       " 'eval_steps_per_second': 3.657,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a99801ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52c082",
   "metadata": {},
   "source": [
    "## STEP 4. Fine-tuningì„ í†µí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥(accuarcy) í–¥ìƒì‹œí‚¤ê¸°\n",
    "- ë°ì´í„° ì „ì²˜ë¦¬, TrainingArguments ë“±ì„ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ 90% ì´ìƒìœ¼ë¡œ ëŒì–´ì˜¬ë ¤ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2771e4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 4685\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4685' max='4685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4685/4685 1:17:11, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.527048</td>\n",
       "      <td>0.870200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.635770</td>\n",
       "      <td>0.876600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.817771</td>\n",
       "      <td>0.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.887424</td>\n",
       "      <td>0.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.925307</td>\n",
       "      <td>0.878400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-937\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-937/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-937/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-1000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-1500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-2000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-2500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-3000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-3500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-4000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-4500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1874\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1874/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1874/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-5500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2811\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2811/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2811/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-1874] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3748\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3748/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3748/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-2811] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4685\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4685/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4685/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-3748] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /aiffel/aiffel/transformers/checkpoint-937 (score: 0.5270480513572693).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 06:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í‰ê°€ ê²°ê³¼: {'eval_loss': 0.5270480513572693, 'eval_accuracy': 0.8702, 'eval_runtime': 190.5614, 'eval_samples_per_second': 26.238, 'eval_steps_per_second': 13.119, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                       # ëª¨ë¸ì´ ì €ì¥ë  ê²½ë¡œ\n",
    "    evaluation_strategy=\"epoch\",                 # ë§¤ ì—í¬í¬ë§ˆë‹¤ í‰ê°€\n",
    "    save_strategy=\"epoch\",                       # ë§¤ ì—í¬í¬ë§ˆë‹¤ ëª¨ë¸ ì €ì¥\n",
    "    learning_rate=2e-5,                          # ë‚®ì€ í•™ìŠµë¥ ì„ ì‚¬ìš©í•´ ì„¸ë°€í•˜ê²Œ ì¡°ì •\n",
    "    per_device_train_batch_size=2,    A          # í•™ìŠµ ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ 16ìœ¼ë¡œ ì„¤ì •\n",
    "    per_device_eval_batch_size=2,               # í‰ê°€ ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ 16ìœ¼ë¡œ ì„¤ì •\n",
    "    num_train_epochs=5,                          # ì´ í•™ìŠµ ì—í¬í¬ ìˆ˜\n",
    "    weight_decay=0.01,                           # ê°€ì¤‘ì¹˜ ê°ì†Œ ì„¤ì •\n",
    "    logging_dir='./logs',                        # ë¡œê¹…ì„ ìœ„í•œ ë””ë ‰í† ë¦¬\n",
    "    logging_steps=50,                            # ë¡œê·¸ë¥¼ ì°ì„ ìŠ¤í… ìˆ˜\n",
    "    save_total_limit=2,                          # ì €ì¥ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜ ì œí•œ\n",
    "    load_best_model_at_end=True,                 # ìµœê³ ì˜ ëª¨ë¸ë§Œì„ ì €ì¥\n",
    "    gradient_accumulation_steps=8,   # ê°€ìƒ ë°°ì¹˜ í¬ê¸° ì¦ê°€\n",
    "    fp16=True,   # í˜¼í•© ì •ë°€ë„ ì‚¬ìš©\n",
    ")\n",
    "\n",
    "# Trainer ì„¤ì •ì€ ì´ì „ê³¼ ë™ì¼í•©ë‹ˆë‹¤.\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model,                     # í•™ìŠµí•  ëª¨ë¸\n",
    "    args=training_arguments,                     # ì„¤ì •í•œ TrainingArguments\n",
    "    train_dataset=hf_train_dataset,           # í•™ìŠµ ë°ì´í„°ì…‹\n",
    "    eval_dataset=hf_test_dataset,             # í‰ê°€ ë°ì´í„°ì…‹\n",
    "    compute_metrics=compute_metrics,             # ì •í™•ë„ ë° ê¸°íƒ€ í‰ê°€ ì§€í‘œ\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()\n",
    "\n",
    "# í‰ê°€\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"í‰ê°€ ê²°ê³¼:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec64d822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5270480513572693,\n",
       " 'eval_accuracy': 0.8702,\n",
       " 'eval_runtime': 190.6608,\n",
       " 'eval_samples_per_second': 26.225,\n",
       " 'eval_steps_per_second': 13.112,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753f807",
   "metadata": {},
   "source": [
    "fine-tuningì„ í–ˆëŠ”ë° accuracyê°€ ì¡°ì˜¤ê¸ˆ ë” ë–¨ì–´ì¡Œë‹¤!ã….. ì•„ë¬´ë˜ë„ ë°ì´í„°ë¥¼ ë„ˆë¬´ ì¤„ì–´ì„œ ê·¸ëŸ°ê°€ë³´ë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a62b3",
   "metadata": {},
   "source": [
    "### ì˜¤ë¥˜ !! \n",
    "ëŸ°íƒ€ì„ ì˜¤ë¥˜: CUDA ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. 20.00 MiBë¥¼ í• ë‹¹í•˜ë ¤ê³  ì‹œë„í–ˆìŠµë‹ˆë‹¤(GPU 0, ì´ ìš©ëŸ‰ 14.58 GiB, ì´ë¯¸ í• ë‹¹ëœ 13.28 GiB, ì‚¬ìš© ê°€ëŠ¥í•œ 1.56 MiB, PyTorchì—ì„œ ì´ 13.37 GiB ì˜ˆì•½).\n",
    "\n",
    "ê°€ìš©í•  ìˆ˜ ìˆëŠ” memoryê°€ ë¶€ì¡±í•´ì„œ ìƒê¸´ ì˜¤ë¥˜ì¸ ê²ƒ ê°™ë‹¤ ã…œã…œ ë‚¨ì•„ìˆëŠ” ê³µê°„ì´ ì–´ëŠì •ë„ ì¸ì§€ í™•ì¸í•´ë³´ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1245a6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (5.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10d1955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 17.57 GB\n",
      "Available RAM: 12.19 GB\n",
      "Used RAM: 4.98 GB\n",
      "RAM Usage: 30.6%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def get_ram_info():\n",
    "    ram_info = psutil.virtual_memory()\n",
    "    total_ram = ram_info.total / (1024 ** 3)  # GB ë‹¨ìœ„\n",
    "    available_ram = ram_info.available / (1024 ** 3)  # GB ë‹¨ìœ„\n",
    "    used_ram = ram_info.used / (1024 ** 3)  # GB ë‹¨ìœ„\n",
    "    ram_usage_percent = ram_info.percent\n",
    "    \n",
    "    print(f\"Total RAM: {total_ram:.2f} GB\")\n",
    "    print(f\"Available RAM: {available_ram:.2f} GB\")\n",
    "    print(f\"Used RAM: {used_ram:.2f} GB\")\n",
    "    print(f\"RAM Usage: {ram_usage_percent}%\")\n",
    "\n",
    "get_ram_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e669b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 17.57 GiB\n",
      "Available Memory: 12.17 GiB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ GiB ë‹¨ìœ„ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "total_memory = psutil.virtual_memory().total / (1024 ** 3)\n",
    "available_memory = psutil.virtual_memory().available / (1024 ** 3)\n",
    "\n",
    "print(f\"Total Memory: {total_memory:.2f} GiB\")\n",
    "print(f\"Available Memory: {available_memory:.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b1a08",
   "metadata": {},
   "source": [
    "ë©”ëª¨ë¦¬ í™•ì¸ ê²°ê³¼ ì‚¬ìš©ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ê°€ ì‚´ì§ ëª¨ìë¼ì„œ torch.cuda.empty_cache()ë¥¼ ì„ ì–¸í•´ì„œ í•´ê²°í–ˆë‹¤.\n",
    "ë˜í•œ, batch_sizeë„ 8ì—ì„œ 2ë¡œ ë‚®ì¶°ì„œ ìµœëŒ€ë¡œ ë‚®ê²Œ ëŒë ¤ë´¤ë‹¤.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b50014",
   "metadata": {},
   "source": [
    "## STEP 5. Bucketingì„ ì ìš©í•˜ì—¬ í•™ìŠµì‹œí‚¤ê³ , STEP 4ì˜ ê²°ê³¼ì™€ì˜ ë¹„êµ\n",
    "- STEP 4ì— í•™ìŠµí•œ ê²°ê³¼ì™€ bucketingì„ ì ìš©í•˜ì—¬ í•™ìŠµì‹œí‚¨ ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ê³ , ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒê³¼ í›ˆë ¨ ì‹œê°„ ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ê°ê° ì–´ë–¤ ì´ì ì´ ìˆëŠ”ì§€ ë¹„êµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b2b01",
   "metadata": {},
   "source": [
    "1. Bucketingê³¼ Dynamic Paddingì´ë€?\n",
    "- Bucketing: ë°ì´í„°ë¥¼ ìœ ì‚¬í•œ ê¸¸ì´ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë°°ì¹˜ë¥¼ êµ¬ì„±í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ëª¨ë¸ì´ íŒ¨ë”© í† í°ì„ ëœ ì²˜ë¦¬í•˜ë„ë¡ í•˜ì—¬ ê³„ì‚°ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.\n",
    "- Dynamic Padding: ë°°ì¹˜ë§ˆë‹¤ íŒ¨ë”©ì„ ë™ì ìœ¼ë¡œ ì„¤ì •í•˜ì—¬, ê° ë°°ì¹˜ ë‚´ì—ì„œ ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ë§Œí¼ë§Œ íŒ¨ë”©ì„ ì ìš©í•´ ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¤„ì…ë‹ˆë‹¤.\n",
    "\n",
    "2. group_by_lengthì™€ DataCollatorWithPadding ì‚¬ìš©\n",
    "- Trainerì—ì„œ bucketingê³¼ dynamic paddingì„ êµ¬í˜„í•˜ë ¤ë©´ group_by_length=Trueì™€ DataCollatorWithPaddingë¥¼ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "016cb88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 1:20:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.903095</td>\n",
       "      <td>0.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>1.007563</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1875\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1875/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1875/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-1875/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-1875/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-937] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3750\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3750/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3750/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-3750/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-3750/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-4685] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5625\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5625/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5625/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-5625/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-5625/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-1875] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /aiffel/aiffel/transformers/checkpoint-3750 (score: 0.8705695271492004).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 02:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketing ë° Dynamic Padding ì ìš© í›„ í‰ê°€ ê²°ê³¼: {'eval_loss': 0.8705695271492004, 'eval_accuracy': 0.8712, 'eval_runtime': 176.8874, 'eval_samples_per_second': 28.267, 'eval_steps_per_second': 3.533, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_metric\n",
    "\n",
    "# ë°ì´í„° Collator - dynamic paddingì„ ì ìš©\n",
    "data_collator = DataCollatorWithPadding(tokenizer=huggingface_tokenizer)\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ metric ì„¤ì •\n",
    "metric = load_metric(\"accuracy\")  # ê¸°ì¡´ì˜ `compute_metrics` í•¨ìˆ˜ë¡œ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ëŠ” metricì…ë‹ˆë‹¤.\n",
    "\n",
    "# Fine-tuningì„ ìœ„í•œ TrainingArguments ì„¤ì • (Bucketing ë° Dynamic Padding ì ìš©)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # í•„ìš” ì‹œ batch í¬ê¸° ì¡°ì •\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    group_by_length=True  # ê¸¸ì´ë³„ë¡œ ë°°ì¹˜ êµ¬ì„± (Bucketing)\n",
    ")\n",
    "\n",
    "# Trainer ì„¤ì •\n",
    "trainer_with_bucketing = Trainer(\n",
    "    model=huggingface_model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train_dataset,\n",
    "    eval_dataset=hf_test_dataset,\n",
    "    tokenizer=huggingface_tokenizer,  # dynamic paddingì„ ìœ„í•œ tokenizer ì „ë‹¬\n",
    "    data_collator=data_collator,      # dynamic paddingì„ ì ìš©í•˜ëŠ” collator\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer_with_bucketing.train()\n",
    "\n",
    "# í‰ê°€\n",
    "eval_results_bucketing = trainer_with_bucketing.evaluate()\n",
    "print(\"Bucketing ë° Dynamic Padding ì ìš© í›„ í‰ê°€ ê²°ê³¼:\", eval_results_bucketing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8c4aa",
   "metadata": {},
   "source": [
    "### ì²« í›ˆë ¨, íŒŒì¸íŠœë‹, Bucketing ê²°ê³¼ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec6229",
   "metadata": {},
   "source": [
    "| |First Learning|Fine-tuning|Bucketing|\n",
    "|---|---|---|---|\n",
    "|ì‹œê°„|4903.6732s(81ë¶„)|4680s(78ë¶„)|4854s(80ë¶„)|\n",
    "|acuuracy|0.8788|0.8702|0.8712|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46287d12",
   "metadata": {},
   "source": [
    "ë™ì¼í•œ ì¡°ê±´ì€ ì›ë˜ ë°ì´í„°ì˜ 10%ë§Œ ë‚¨ê¸´ train data 15000ê°œ, test data 5000ê°œë¡œ ëŒë ¸ë‹¤.\n",
    "\n",
    "ì²« í›ˆë ¨ê³¼ íŒŒì¸íŠœë‹, Bucketing ì„ ì‹œë„í–ˆì„ ë•Œ, ë‹¤ì´ë‚˜ë¯¹í•˜ê²Œ ì„±ëŠ¥ì´ ë°”ë€Œì§€ëŠ” ì•Šì•˜ë‹¤.\n",
    "\n",
    "ë¬¼ë¡ , ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ë„ˆë¬´ ì‘ê²Œì¡ì•„ì„œ ê·¸ëŸ° ê²ƒ ê°™ì§€ë§Œ. ì‹œê°„ì´ ì—†ëŠ” ê´€ê³„ë¡œ ë” ì‹œë„ í•´ë³´ì§€ëŠ” ëª»í•˜ì˜€ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40776c32",
   "metadata": {},
   "source": [
    "## íšŒê³ \n",
    "ğŸ¤—Hugging faceë¡œ ëª¨ë¸ì„ ë°›ì•„ì™€ì„œ ì‚¬ìš©í•´ë³´ëŠ” ê²½í—˜ì€ ì‚¬ì‹¤ ê²½í—˜ì´ ë§ì•˜ë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ, ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ìœ¼ë¡œ í—ˆê¹…í˜ì´ìŠ¤ì— ëŒ€í•´ì„œëŠ” ê¹Šê²Œ ì•Œê³  ìˆì§€ëŠ” ì•Šì•˜ëŠ”ë°\n",
    "\n",
    "ì´ë²ˆ ê³ ì‰ë””í¼ í•™ìŠµì—ì„œ ëª°ëë˜ ì‚¬ì‹¤ê³¼ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ê°€ì ¸ì™€ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆë‹¤ëŠ” ê³¼ì •ì€ ì •ë§ ì¢‹ì•˜ë˜ ê²ƒ ê°™ë‹¤.\n",
    "\n",
    "ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë‚˜ë„ ë‚´ê°€ ì§€ì€ ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ë°°í¬ í•´ë³´ê³  ì‹¶ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤:)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
